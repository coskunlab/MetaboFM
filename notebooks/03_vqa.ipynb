{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9debf04d",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e0fa9ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FILTER] Dropped 0 rows with all tasks == 'unknown'.\n",
      "\n",
      "[SUMMARY] Class counts after filtering (unknown shown but ignored during loss):\n",
      " - organism: total=3938, classes=2 (+unknown)\n",
      "organism\n",
      "homo sapiens    2122\n",
      "mus musculus    1799\n",
      "unknown           17\n",
      " - polarity: total=3938, classes=2 (+unknown)\n",
      "polarity\n",
      "negative    2397\n",
      "positive    1541\n",
      " - organ: total=3938, classes=6 (+unknown)\n",
      "Organism_Part\n",
      "kidney           1353\n",
      "unknown          1020\n",
      "brain             643\n",
      "kidney cortex     368\n",
      "liver             225\n",
      "lung              212\n",
      "breast            117\n",
      " - condition: total=3938, classes=8 (+unknown)\n",
      "Condition\n",
      "n/a             1259\n",
      "unknown          781\n",
      "biopsy           598\n",
      "wildtype         587\n",
      "frozen           194\n",
      "tumor            164\n",
      "diseased         141\n",
      "cancer           109\n",
      "fresh frozen     105\n",
      " - analyzerType: total=3938, classes=4 (+unknown)\n",
      "analyzerType\n",
      "orbitrap        1965\n",
      "timstof flex     627\n",
      "12t fticr        593\n",
      "unknown          443\n",
      "fticr            310\n",
      " - ionisationSource: total=3938, classes=4 (+unknown)\n",
      "ionisationSource\n",
      "maldi            2983\n",
      "unknown           408\n",
      "ap-smaldi5 af     227\n",
      "desi              183\n",
      "ap-smaldi 5       137\n",
      "[STATS] Using cache dir: \\\\bme-data2.ad.gatech.edu\\labs6\\coskun-lab\\Efe\\MSI Foundation Model\\vqa\\_stats_cache\n",
      "[CACHE] Image embeddings dir: \\\\bme-data2.ad.gatech.edu\\labs6\\coskun-lab\\Efe\\MSI Foundation Model\\vqa\\_img_cache\n",
      "[STATS] Loaded cached mu/std from: vqa\\_stats_cache\\mu_std_c64_in256_8f39161ffd68646249a4caf6a1d88d7330ab27e1.npz\n",
      "[CACHE] All embeddings already computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VQA(train cached + heads + text) -> 20251015_171627: 100%|██████████| 10000/10000 [2:35:29<00:00,  1.07it/s, ema=2.0028, loss=2.2207, lr_max=0.00e+00, lr_min=0.00e+00] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoints & logs to: vqa\\20251015_171627\n"
     ]
    }
   ],
   "source": [
    "# ---------- train_vqa_fast.py (cached image embeddings + robust multitask heads) ----------\n",
    "import os, csv, json, hashlib, math\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "from itertools import chain\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "\n",
    "import timm\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Pull safe utils you already have\n",
    "from fm_utils import CFG, NPZDataset, collate_simple, nullcontext\n",
    "\n",
    "# =====================================================\n",
    "# I/O (parquets)\n",
    "# =====================================================\n",
    "IDX_PARQUET = \"metaspace_images_dump/msi_fm_samples3.parquet\"\n",
    "MAN_PARQUET = \"metaspace_images_dump/manifest_expanded.parquet\"\n",
    "\n",
    "df_idx = pd.read_parquet(IDX_PARQUET)\n",
    "df_man = pd.read_parquet(MAN_PARQUET)\n",
    "\n",
    "need_cols = [\n",
    "    \"dataset_id\", \"organism\", \"polarity\", \"Organism_Part\", \"Condition\",\n",
    "    \"analyzerType\", \"ionisationSource\"\n",
    "]\n",
    "man_sub = df_man[[c for c in need_cols if c in df_man.columns]].drop_duplicates(\"dataset_id\")\n",
    "\n",
    "df = df_idx.merge(man_sub, on=\"dataset_id\", how=\"left\", suffixes=(\"\", \"_man\"))\n",
    "df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "if \"sample_path\" not in df.columns:\n",
    "    raise ValueError(\"Index parquet must include 'sample_path' pointing to .npz files.\")\n",
    "\n",
    "# =====================================================\n",
    "# Label normalization\n",
    "# =====================================================\n",
    "def _canon_base(x):\n",
    "    if x is None:\n",
    "        return \"unknown\"\n",
    "    s = str(x).strip().lower()\n",
    "    return \"unknown\" if s in (\"\", \"none\", \"nan\", \"na\", \"null\") else s\n",
    "\n",
    "def normalize_organism(x):\n",
    "    s = _canon_base(x)\n",
    "    if s == \"unknown\": return s\n",
    "    if \"mus musculus\" in s or s in {\"mouse\", \"mouse brain\", \"mus\", \"m. musculus\"}:\n",
    "        return \"mus musculus\"\n",
    "    if \"homo sapiens\" in s or s in {\"human\", \"h. sapiens\"}:\n",
    "        return \"homo sapiens\"\n",
    "    return \"unknown\"\n",
    "\n",
    "def normalize_polarity(x):\n",
    "    s = _canon_base(x)\n",
    "    if s in {\"pos\", \"+\", \"positive\", \"positive ion mode\", \"positive mode\"}:  return \"positive\"\n",
    "    if s in {\"neg\", \"-\", \"negative\", \"negative ion mode\", \"negative mode\"}:  return \"negative\"\n",
    "    return \"unknown\"\n",
    "\n",
    "def normalize_text(x):\n",
    "    return _canon_base(x)\n",
    "\n",
    "for col, fn in [\n",
    "    (\"organism\", normalize_organism),\n",
    "    (\"polarity\", normalize_polarity),\n",
    "    (\"Organism_Part\", normalize_text),\n",
    "    (\"Condition\", normalize_text),\n",
    "    (\"analyzerType\", normalize_text),\n",
    "    (\"ionisationSource\", normalize_text),\n",
    "]:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].map(fn)\n",
    "    else:\n",
    "        df[col] = \"unknown\"\n",
    "\n",
    "# =====================================================\n",
    "# RARE-CLASS FILTERING / NA REMOVAL\n",
    "# =====================================================\n",
    "# Any label mapped to \"unknown\" is **ignored** in loss. We also downmap rare classes to \"unknown\".\n",
    "MIN_SAMPLES_PER_CLASS = 100      # <- change as needed\n",
    "DROP_ROWS_ALL_UNKNOWN = True    # drop rows where ALL tasks are unknown (saves compute)\n",
    "\n",
    "TASK_COLS = {\n",
    "    \"organism\": \"organism\",\n",
    "    \"polarity\": \"polarity\",\n",
    "    \"organ\": \"Organism_Part\",\n",
    "    \"condition\": \"Condition\",\n",
    "    \"analyzerType\": \"analyzerType\",\n",
    "    \"ionisationSource\": \"ionisationSource\",\n",
    "}\n",
    "\n",
    "def _apply_min_count_filter(df: pd.DataFrame, col: str, min_count: int) -> pd.Series:\n",
    "    \"\"\"Return a filtered label series: classes with freq < min_count -> 'unknown'.\"\"\"\n",
    "    s = df[col].astype(str)\n",
    "    vc = s[s != \"unknown\"].value_counts()\n",
    "    keep = set(vc[vc >= min_count].index.tolist())\n",
    "    return s.where(s.isin(keep), other=\"unknown\")\n",
    "\n",
    "for tname, col in TASK_COLS.items():\n",
    "    df[col] = _apply_min_count_filter(df, col, MIN_SAMPLES_PER_CLASS)\n",
    "\n",
    "if DROP_ROWS_ALL_UNKNOWN:\n",
    "    mask_known_any = False\n",
    "    for col in TASK_COLS.values():\n",
    "        mask_known_any |= (df[col] != \"unknown\")\n",
    "    before = len(df)\n",
    "    df = df[mask_known_any].reset_index(drop=True)\n",
    "    print(f\"[FILTER] Dropped {before - len(df)} rows with all tasks == 'unknown'.\")\n",
    "\n",
    "def _print_task_summary(df, task_cols):\n",
    "    print(\"\\n[SUMMARY] Class counts after filtering (unknown shown but ignored during loss):\")\n",
    "    for tname, col in task_cols.items():\n",
    "        vc = df[col].value_counts().sort_values(ascending=False)\n",
    "        print(f\" - {tname}: total={int(vc.sum())}, classes={int((vc.index!='unknown').sum())} (+unknown)\")\n",
    "        print(vc.head(20).to_string())  # top-20 preview\n",
    "\n",
    "_print_task_summary(df, TASK_COLS)\n",
    "\n",
    "# Build class vocabularies **excluding 'unknown'** (we ignore unknown in loss)\n",
    "def _build_vocab(series):\n",
    "    vals = sorted([v for v in set(series.dropna().astype(str).tolist()) if v != \"unknown\"])\n",
    "    # Can be empty if everything was filtered; handle later when building heads\n",
    "    return vals\n",
    "\n",
    "cls_spaces = {\n",
    "    k: _build_vocab(df[col]) for k, col in TASK_COLS.items()\n",
    "}\n",
    "\n",
    "# =====================================================\n",
    "# Config / knobs\n",
    "# =====================================================\n",
    "SEED = 6740\n",
    "torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "# ---- Switches ----\n",
    "USE_TEXT   = True     # set False for image-only (often a strong baseline)\n",
    "REBALANCE  = False    # simple sampler example (organism only)\n",
    "LABEL_SMOOTH = 0.05   # cross-entropy label smoothing\n",
    "\n",
    "# ---- Frozen image backbone (timm ViT) ----\n",
    "TIMM_ID         = \"vit_small_patch14_reg4_dinov2.lvd142m\"   # e.g., \"deit_small_distilled_patch16_224\"\n",
    "PATCH_MULTIPLE  = 14                                        # 16 for DeiT/MAE, 14 for DINOv2\n",
    "TARGET_SIZE     = 518                                       # 224 for DeiT/MAE; 518 for DINOv2 reg4\n",
    "PRETRAINED      = True\n",
    "\n",
    "# ---- Trainable text tower (HF) ----\n",
    "HF_TEXT_MODEL      = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "TEXT_MAX_LEN       = 64\n",
    "TEXT_OUT_DIM       = 384\n",
    "\n",
    "# Partial tuning strategy for speed\n",
    "# options: \"none\" (freeze all, only heads), \"proj\" (only projection), \"last_k\" (unfreeze last K blocks + proj)\n",
    "TEXT_TRAIN_STRATEGY = \"last_k\"\n",
    "TEXT_TRAIN_LAST_K   = 2\n",
    "TEXT_FREEZE_EMBED   = True\n",
    "\n",
    "# Optimizer specifics (separate LR for base vs projection)\n",
    "LR_TXT_BASE   = 5e-5\n",
    "LR_TXT_PROJ   = 1e-4\n",
    "USE_FUSED_ADAMW = torch.cuda.is_available()\n",
    "\n",
    "# ---- Data / train ----\n",
    "AMP = True\n",
    "CHANNELS_PER_VIEW  = 64\n",
    "CHANNELS_PER_STEP  = 16\n",
    "BATCH_SIZE         = 128\n",
    "NUM_WORKERS        = 0\n",
    "\n",
    "TARGET_H = 256\n",
    "TARGET_W = 256\n",
    "\n",
    "TOTAL_STEPS = 10000\n",
    "LR_FUSION   = 5e-4\n",
    "LR_HEADS    = 8e-4\n",
    "WD          = 0.05\n",
    "SAVE_EVERY  = 200\n",
    "KEEP_EVERY  = 1000\n",
    "EMA_BETA    = 0.98\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# =====================================================\n",
    "# Output / stats cache\n",
    "# =====================================================\n",
    "RUN_ROOT = Path(\"vqa\"); RUN_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "RUN_DIR  = RUN_ROOT / datetime.now().strftime(\"%Y%m%d_%H%M%S\"); RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- PERSISTENT cache across runs (fix) ---\n",
    "STATS_DIR = RUN_ROOT / \"_stats_cache\"\n",
    "STATS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"[STATS] Using cache dir: {STATS_DIR.resolve()}\")\n",
    "\n",
    "# --- Image embedding cache across runs ---\n",
    "IMG_CACHE_DIR = RUN_ROOT / \"_img_cache\"\n",
    "IMG_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"[CACHE] Image embeddings dir: {IMG_CACHE_DIR.resolve()}\")\n",
    "\n",
    "cfg = CFG(\n",
    "    channels_per_view=CHANNELS_PER_VIEW,\n",
    "    input_size=TARGET_H,\n",
    "    crop_size=TARGET_H,\n",
    "    patch_size=PATCH_MULTIPLE,\n",
    "    batch_size=max(8, min(64, BATCH_SIZE)),  # for the raw image pass during caching\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "# =====================================================\n",
    "# DataLoader (tiles) for CACHING only\n",
    "# =====================================================\n",
    "def make_loader(paths, cfg, shuffle=False):\n",
    "    ds = NPZDataset(\n",
    "        paths,\n",
    "        target_h=int(cfg.input_size),\n",
    "        target_w=int(cfg.input_size),\n",
    "        k_target=int(cfg.channels_per_view),\n",
    "        scale_u16=True,             # -> float [0,1]\n",
    "        pad_mode=\"repeat\",\n",
    "        sort_by_mz=False\n",
    "    )\n",
    "    ld = DataLoader(\n",
    "        ds,\n",
    "        batch_size=int(cfg.batch_size),\n",
    "        shuffle=shuffle,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=0,\n",
    "        persistent_workers=(NUM_WORKERS > 0),\n",
    "        collate_fn=collate_simple,\n",
    "        drop_last=False\n",
    "    )\n",
    "    return ds, ld\n",
    "\n",
    "all_paths = df[\"sample_path\"].tolist()\n",
    "\n",
    "# =====================================================\n",
    "# Frozen image backbone (returns [B, D_img] CLS-like)\n",
    "# =====================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class FrozenBackbone(nn.Module):\n",
    "    def __init__(self, timm_name: str, pretrained: bool = True):\n",
    "        super().__init__()\n",
    "        self.m = timm.create_model(timm_name, pretrained=pretrained, num_classes=0)\n",
    "        for p in self.m.parameters():\n",
    "            p.requires_grad_(False)\n",
    "        self.m.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x3):  # [N,3,H,W]\n",
    "        feats = self.m.forward_features(x3)\n",
    "        if isinstance(feats, dict):\n",
    "            if 'x_norm_clstoken' in feats:  return feats['x_norm_clstoken']\n",
    "            if 'cls_token' in feats:        return feats['cls_token']\n",
    "            if 'avgpool' in feats:          return feats['avgpool']\n",
    "            for k in ('last_hidden_state', 'tokens', 'x'):\n",
    "                if k in feats and torch.is_tensor(feats[k]):\n",
    "                    t = feats[k]\n",
    "                    return t[:,0] if t.dim() == 3 else t\n",
    "        if torch.is_tensor(feats):\n",
    "            return feats[:,0] if feats.dim() == 3 else feats\n",
    "        return feats.mean(dim=-2)  # fallback\n",
    "\n",
    "def crop_resize_to_target(x3, target=224, patch_multiple=16):\n",
    "    _, _, H, W = x3.shape\n",
    "    Hc = (H // patch_multiple) * patch_multiple\n",
    "    Wc = (W // patch_multiple) * patch_multiple\n",
    "    dh = (H - Hc) // 2; dw = (W - Wc) // 2\n",
    "    if Hc > 0 and Wc > 0:\n",
    "        x3 = x3[:, :, dh:dh+Hc, dw:dw+Wc]\n",
    "    if (Hc, Wc) != (target, target):\n",
    "        x3 = F.interpolate(x3, size=(target, target), mode=\"bilinear\", align_corners=False)\n",
    "    return x3\n",
    "\n",
    "bb = FrozenBackbone(TIMM_ID, pretrained=PRETRAINED).to(device)\n",
    "bb.eval()\n",
    "autocast_ctx = (torch.autocast(device_type=\"cuda\", dtype=torch.float16)\n",
    "                if (AMP and torch.cuda.is_available()) else nullcontext())\n",
    "\n",
    "# probe image dim\n",
    "with torch.no_grad():\n",
    "    dummy = torch.zeros(2, 3, TARGET_H, TARGET_W, device=device)\n",
    "    d_img = bb(crop_resize_to_target(dummy, target=TARGET_SIZE, patch_multiple=PATCH_MULTIPLE)).shape[-1]\n",
    "\n",
    "# =====================================================\n",
    "# Per-channel mean/std (cached)\n",
    "# =====================================================\n",
    "def _paths_signature(paths):\n",
    "    n = len(paths)\n",
    "    head = paths[:100]; tail = paths[-100:]\n",
    "    sig_src = json.dumps([n, head, tail], separators=(',', ':')).encode('utf-8')\n",
    "    return hashlib.sha1(sig_src).hexdigest()\n",
    "\n",
    "def _stats_cache_path(paths, cfg):\n",
    "    sig = _paths_signature(paths)\n",
    "    fname = f\"mu_std_c{int(cfg.channels_per_view)}_in{int(cfg.input_size)}_{sig}.npz\"\n",
    "    return STATS_DIR / fname\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_channel_stats(paths, cfg):\n",
    "    _, ld = make_loader(paths, cfg, shuffle=False)\n",
    "    sum_c, sumsq_c, total = None, None, 0\n",
    "    for batch in tqdm(ld, desc=\"Compute per-channel mean/std\"):\n",
    "        x = batch[\"patch\"].float()   # [B,C,H,W]\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.view(B, C, -1)\n",
    "        if sum_c is None:\n",
    "            sum_c   = x.sum(dim=(0,2))\n",
    "            sumsq_c = (x**2).sum(dim=(0,2))\n",
    "        else:\n",
    "            sum_c   += x.sum(dim=(0,2))\n",
    "            sumsq_c += (x**2).sum(dim=(0,2))\n",
    "        total += B * H * W\n",
    "    mu  = (sum_c / (total + 1e-8)).cpu().numpy()\n",
    "    var = (sumsq_c / (total + 1e-8)).cpu().numpy() - mu**2\n",
    "    std = np.sqrt(np.maximum(var, 1e-12))\n",
    "    return mu, std\n",
    "\n",
    "def load_or_compute_stats(paths, cfg, force_recompute=False):\n",
    "    cache_path = _stats_cache_path(paths, cfg)\n",
    "    if (not force_recompute) and cache_path.exists():\n",
    "        z = np.load(cache_path, allow_pickle=True)\n",
    "        mu, std = z[\"mu\"], z[\"std\"]\n",
    "        print(f\"[STATS] Loaded cached mu/std from: {cache_path}\")\n",
    "        return mu, std\n",
    "    print(\"[STATS] Computing mu/std (once) ...\")\n",
    "    mu, std = compute_channel_stats(paths, cfg)\n",
    "    np.savez_compressed(cache_path, mu=mu, std=std, meta=dict(\n",
    "        channels_per_view=int(cfg.channels_per_view),\n",
    "        input_size=int(cfg.input_size),\n",
    "        n_paths=len(paths)\n",
    "    ))\n",
    "    print(f\"[STATS] Saved mu/std to: {cache_path}\")\n",
    "    return mu, std\n",
    "\n",
    "# stats once (now persists across runs)\n",
    "mu_c, std_c = load_or_compute_stats(all_paths, cfg, force_recompute=False)\n",
    "mu_t = torch.tensor(mu_c, device=device).view(1, -1, 1, 1)\n",
    "sd_t = torch.tensor(std_c, device=device).view(1, -1, 1, 1).clamp_min(1e-6)\n",
    "STEP = CHANNELS_PER_STEP if TARGET_SIZE == 224 else max(4, CHANNELS_PER_STEP // 2)\n",
    "\n",
    "# =====================================================\n",
    "# IMAGE EMBEDDING CACHE (mean-CLS per tile)\n",
    "# =====================================================\n",
    "def _embed_key(path):\n",
    "    h = hashlib.sha1(f\"{TIMM_ID}|{PATCH_MULTIPLE}|{TARGET_SIZE}|{path}\".encode()).hexdigest()\n",
    "    return IMG_CACHE_DIR / f\"{h}.npy\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_image_mean_cls_batch(patch_bchw):  # vectorized encode for a batch\n",
    "    x = patch_bchw.to(device=device).float()   # [B,C,H,W] in [0,1]\n",
    "    B, C, H, W = x.shape\n",
    "    x = (x - mu_t[:, :C]) / sd_t[:, :C]\n",
    "    x_flat = x.permute(0, 2, 3, 1).contiguous().view(B * C, 1, H, W)\n",
    "    x_rgb  = x_flat.repeat(1, 3, 1, 1)\n",
    "\n",
    "    outs = []\n",
    "    with autocast_ctx, torch.inference_mode():\n",
    "        for s in range(0, B * C, STEP):\n",
    "            e = min(s + STEP, B * C)\n",
    "            xr = crop_resize_to_target(x_rgb[s:e], target=TARGET_SIZE, patch_multiple=PATCH_MULTIPLE)\n",
    "            cls = bb(xr).float()  # [N, D_img]\n",
    "            outs.append(cls)\n",
    "    cls_all = torch.cat(outs, dim=0)           # [B*C, D_img]\n",
    "    D = cls_all.shape[1]\n",
    "    z = cls_all.view(B, C, D).mean(dim=1)      # [B, D_img]\n",
    "    return F.normalize(z, dim=-1)\n",
    "\n",
    "def build_img_cache(paths, cfg):\n",
    "    ds, ld = make_loader(paths, cfg, shuffle=False)\n",
    "    done = 0\n",
    "    for batch in tqdm(ld, desc=\"[CACHE] image embeddings\"):\n",
    "        z = encode_image_mean_cls_batch(batch[\"patch\"])\n",
    "        for pth, zi in zip(batch[\"path\"], z):  # NPZDataset must return \"path\"\n",
    "            out_p = _embed_key(pth)\n",
    "            if not out_p.exists():\n",
    "                np.save(out_p, zi.detach().cpu().numpy())\n",
    "                done += 1\n",
    "    print(f\"[CACHE] Wrote {done} new embeddings.\")\n",
    "\n",
    "# Build cache once (skips existing files)\n",
    "missing = [p for p in all_paths if not _embed_key(p).exists()]\n",
    "if missing:\n",
    "    print(f\"[CACHE] {len(missing)} embeddings to compute...\")\n",
    "    build_img_cache(all_paths, cfg)\n",
    "else:\n",
    "    print(\"[CACHE] All embeddings already computed.\")\n",
    "\n",
    "# =====================================================\n",
    "# Trainable HF text encoder + projection (partial tuning)\n",
    "# =====================================================\n",
    "class HFTextEnc(nn.Module):\n",
    "    def __init__(self, name: str, out_dim: int,\n",
    "                 train_strategy: str = \"last_k\",\n",
    "                 last_k: int = 2,\n",
    "                 freeze_embed: bool = True):\n",
    "        super().__init__()\n",
    "        self.model = AutoModel.from_pretrained(name)\n",
    "        self.name  = name\n",
    "        hid = self.model.config.hidden_size\n",
    "        self.proj = nn.Linear(hid, out_dim)\n",
    "\n",
    "        # default: freeze everything\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "        # optionally unfreeze embeddings\n",
    "        if not freeze_embed:\n",
    "            for p in self.model.get_input_embeddings().parameters():\n",
    "                p.requires_grad_(True)\n",
    "\n",
    "        if hasattr(self.model.config, \"hidden_dropout_prob\"):\n",
    "            self.model.config.hidden_dropout_prob = 0.0\n",
    "        if hasattr(self.model.config, \"attention_probs_dropout_prob\"):\n",
    "            self.model.config.attention_probs_dropout_prob = 0.0\n",
    "\n",
    "        # unfreeze per strategy\n",
    "        if train_strategy == \"none\":\n",
    "            pass\n",
    "        elif train_strategy == \"proj\":\n",
    "            for p in self.proj.parameters():\n",
    "                p.requires_grad_(True)\n",
    "        elif train_strategy == \"last_k\":\n",
    "            enc = getattr(self.model, \"encoder\", None)\n",
    "            if enc is None:\n",
    "                enc = getattr(self.model, \"transformer\", None)\n",
    "            layers = None\n",
    "            for cand in (\"layer\", \"layers\", \"block\", \"h\"):\n",
    "                if hasattr(enc, cand):\n",
    "                    layers = getattr(enc, cand)\n",
    "                    break\n",
    "            if layers is None:\n",
    "                raise RuntimeError(\"Cannot locate transformer blocks for last_k tuning.\")\n",
    "            k = max(1, min(last_k, len(layers)))\n",
    "            for layer in layers[-k:]:\n",
    "                for p in layer.parameters():\n",
    "                    p.requires_grad_(True)\n",
    "            for p in self.proj.parameters():\n",
    "                p.requires_grad_(True)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown TEXT_TRAIN_STRATEGY={train_strategy}\")\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        out = self.model(input_ids=ids, attention_mask=mask, return_dict=True)\n",
    "        x = (out.last_hidden_state * mask.unsqueeze(-1)).sum(1) / (mask.sum(1, keepdim=True) + 1e-6)\n",
    "        return self.proj(x)\n",
    "\n",
    "    def param_groups(self, lr_base: float, lr_proj: float):\n",
    "        base_params, proj_params = [], []\n",
    "        for n, p in self.named_parameters():\n",
    "            if not p.requires_grad:\n",
    "                continue\n",
    "            if n.startswith(\"proj.\"):\n",
    "                proj_params.append(p)\n",
    "            else:\n",
    "                base_params.append(p)\n",
    "        groups = []\n",
    "        if base_params:\n",
    "            groups.append({\"params\": base_params, \"lr\": lr_base})\n",
    "        if proj_params:\n",
    "            groups.append({\"params\": proj_params, \"lr\": lr_proj})\n",
    "        return groups\n",
    "\n",
    "# =====================================================\n",
    "# Fusion + Heads\n",
    "# =====================================================\n",
    "class Fusion(nn.Module):\n",
    "    def __init__(self, d_img, d_txt, d_out):\n",
    "        super().__init__()\n",
    "        self.ln_img = nn.LayerNorm(d_img)\n",
    "        self.ln_txt = nn.LayerNorm(d_txt)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_img + d_txt, d_out),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(d_out, d_out),\n",
    "        )\n",
    "    def forward(self, zi, zt):\n",
    "        x = torch.cat([self.ln_img(zi), self.ln_txt(zt)], dim=-1)\n",
    "        return self.mlp(x)\n",
    "\n",
    "class VQAHeads(nn.Module):\n",
    "    def __init__(self, embed_dim: int, cls_spaces: dict):\n",
    "        super().__init__()\n",
    "        # Heads have ONLY the kept classes (unknown excluded).\n",
    "        self.cls_spaces = {k: v for k, v in cls_spaces.items()}\n",
    "        self.heads = nn.ModuleDict({k: nn.Linear(embed_dim, max(1, len(v))) for k, v in self.cls_spaces.items()})\n",
    "        for lin in self.heads.values():\n",
    "            nn.init.trunc_normal_(lin.weight, std=0.02); nn.init.zeros_(lin.bias)\n",
    "    def forward(self, z):  # [B, D_fused]\n",
    "        return {\"cls\": {k: h(z) for k, h in self.heads.items()}}\n",
    "\n",
    "# =====================================================\n",
    "# Text utils (pre-tokenize once)\n",
    "# =====================================================\n",
    "tok = AutoTokenizer.from_pretrained(HF_TEXT_MODEL)\n",
    "FIXED_QUESTION = \"what is the organism?\"\n",
    "_tok_once = tok(FIXED_QUESTION, padding=False, truncation=True, max_length=TEXT_MAX_LEN, return_tensors=\"pt\")\n",
    "_ids_base  = _tok_once[\"input_ids\"]\n",
    "_mask_base = _tok_once[\"attention_mask\"]\n",
    "\n",
    "def encode_text_fixed(batch_size: int, text_enc):\n",
    "    ids  = _ids_base.to(device).expand(batch_size, -1).contiguous()\n",
    "    mask = _mask_base.to(device).expand(batch_size, -1).contiguous()\n",
    "    zt = text_enc(ids, mask)  # [B, d_txt]\n",
    "    return F.normalize(zt, dim=-1)\n",
    "\n",
    "# =====================================================\n",
    "# Dataset over cached embeddings\n",
    "# =====================================================\n",
    "class EmbedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, cls_spaces, task_cols):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.task_cols = task_cols\n",
    "        self.cls_spaces = {k: list(v) for k, v in cls_spaces.items()}\n",
    "        # maps without 'unknown'\n",
    "        self.label_maps = {k: {c:i for i,c in enumerate(v)} for k,v in self.cls_spaces.items()}\n",
    "\n",
    "    def __len__(self): return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        emb_path = _embed_key(row[\"sample_path\"])\n",
    "        if not emb_path.exists():\n",
    "            raise FileNotFoundError(f\"Missing cached embedding for {row['sample_path']}, run cache first.\")\n",
    "        z = np.load(emb_path, mmap_mode=\"r\")\n",
    "        out = {\"z_img\": torch.from_numpy(np.array(z, dtype=np.float32))}\n",
    "        # encode labels; set -100 if unknown or filtered out\n",
    "        for head, col in self.task_cols.items():\n",
    "            val = str(row.get(col, \"unknown\"))\n",
    "            if (val == \"unknown\") or (val not in self.label_maps[head]):\n",
    "                y = -100\n",
    "            else:\n",
    "                y = self.label_maps[head][val]\n",
    "            out[f\"y_cls_{head}\"] = torch.tensor(y, dtype=torch.long)\n",
    "        out[\"question\"] = FIXED_QUESTION\n",
    "        return out\n",
    "\n",
    "embed_ds = EmbedDataset(df, cls_spaces, TASK_COLS)\n",
    "\n",
    "# Optional simple rebalancing example (organism): downweight unknown heavily\n",
    "if REBALANCE:\n",
    "    org_vals = df[\"organism\"].astype(str).tolist()\n",
    "    weights = np.array([0.05 if v==\"unknown\" else 1.0 for v in org_vals], dtype=np.float32)\n",
    "    sampler = WeightedRandomSampler(weights=weights, num_samples=len(weights), replacement=True)\n",
    "    train_loader = DataLoader(embed_ds, batch_size=BATCH_SIZE, sampler=sampler,\n",
    "                              num_workers=NUM_WORKERS, pin_memory=False, drop_last=False)\n",
    "else:\n",
    "    train_loader = DataLoader(embed_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                              num_workers=NUM_WORKERS, pin_memory=False, drop_last=False)\n",
    "\n",
    "# =====================================================\n",
    "# Build trainable modules\n",
    "# =====================================================\n",
    "if USE_TEXT:\n",
    "    text_enc = HFTextEnc(\n",
    "        HF_TEXT_MODEL, TEXT_OUT_DIM,\n",
    "        train_strategy=TEXT_TRAIN_STRATEGY,\n",
    "        last_k=TEXT_TRAIN_LAST_K,\n",
    "        freeze_embed=TEXT_FREEZE_EMBED\n",
    "    ).to(device)\n",
    "    d_txt = TEXT_OUT_DIM\n",
    "    FUSED_DIM = d_img + d_txt\n",
    "    fusion = Fusion(d_img, d_txt, FUSED_DIM).to(device)\n",
    "else:\n",
    "    text_enc = None\n",
    "    d_txt = 0\n",
    "    FUSED_DIM = d_img\n",
    "    fusion = None\n",
    "\n",
    "heads = VQAHeads(embed_dim=FUSED_DIM, cls_spaces=cls_spaces).to(device)\n",
    "\n",
    "# Optimizer with per-group LRs (fused if available)\n",
    "opt_groups = []\n",
    "if USE_TEXT:\n",
    "    opt_groups.extend(text_enc.param_groups(lr_base=LR_TXT_BASE, lr_proj=LR_TXT_PROJ))\n",
    "    opt_groups.append({\"params\": fusion.parameters(),   \"lr\": LR_FUSION})\n",
    "opt_groups.append({\"params\": heads.parameters(),          \"lr\": LR_HEADS})\n",
    "\n",
    "try:\n",
    "    opt = AdamW(opt_groups, weight_decay=WD, fused=USE_FUSED_ADAMW)\n",
    "except TypeError:\n",
    "    opt = AdamW(opt_groups, weight_decay=WD)\n",
    "\n",
    "# Cosine scheduler (per-step)\n",
    "sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=TOTAL_STEPS)\n",
    "\n",
    "# =====================================================\n",
    "# Logging / checkpoints\n",
    "# =====================================================\n",
    "config = {\n",
    "    \"timm_id\": TIMM_ID,\n",
    "    \"patch_multiple\": PATCH_MULTIPLE,\n",
    "    \"target_size\": TARGET_SIZE,\n",
    "    \"pretrained\": PRETRAINED,\n",
    "    \"hf_text_model\": HF_TEXT_MODEL if USE_TEXT else None,\n",
    "    \"use_text\": USE_TEXT,\n",
    "    \"text_max_len\": TEXT_MAX_LEN,\n",
    "    \"text_out_dim\": TEXT_OUT_DIM if USE_TEXT else 0,\n",
    "    \"text_tune\": {\n",
    "        \"strategy\": TEXT_TRAIN_STRATEGY,\n",
    "        \"last_k\": TEXT_TRAIN_LAST_K,\n",
    "        \"freeze_embed\": TEXT_FREEZE_EMBED,\n",
    "        \"lr_base\": LR_TXT_BASE,\n",
    "        \"lr_proj\": LR_TXT_PROJ,\n",
    "        \"fused_adamw\": USE_FUSED_ADAMW\n",
    "    },\n",
    "    \"channels_per_view\": CHANNELS_PER_VIEW,\n",
    "    \"channels_per_step\": CHANNELS_PER_STEP,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"input_size\": TARGET_H,\n",
    "    \"total_steps\": TOTAL_STEPS,\n",
    "    \"lrs\": {\"fusion\": LR_FUSION, \"heads\": LR_HEADS},\n",
    "    \"weight_decay\": WD,\n",
    "    \"ema_beta\": EMA_BETA,\n",
    "    \"embed_dim_image\": int(d_img),\n",
    "    \"embed_dim_text\": int(d_txt),\n",
    "    \"embed_dim_fused\": int(FUSED_DIM),\n",
    "    \"cls_spaces\": {k: list(v) for k, v in cls_spaces.items()},\n",
    "    \"rebalance_sampler\": REBALANCE,\n",
    "    \"min_samples_per_class\": MIN_SAMPLES_PER_CLASS,\n",
    "    \"drop_rows_all_unknown\": DROP_ROWS_ALL_UNKNOWN,\n",
    "}\n",
    "RUN_DIR.mkdir(exist_ok=True, parents=True)\n",
    "(RUN_DIR / \"config.json\").write_text(json.dumps(config, indent=2))\n",
    "\n",
    "log_f = open(RUN_DIR / \"train_log.csv\", \"w\", newline=\"\")\n",
    "log_w = csv.writer(log_f); log_w.writerow([\"step\", \"loss\", \"ema_loss\", \"lr_min\", \"lr_max\"]); log_f.flush()\n",
    "\n",
    "def _cpu_state_dict(m):\n",
    "    return {k: v.detach().cpu() for k, v in m.state_dict().items()}\n",
    "\n",
    "def save_ckpt(tag, step):\n",
    "    ckpt = {\n",
    "        \"step\": step,\n",
    "        \"heads\": _cpu_state_dict(heads),\n",
    "        \"optimizer\": opt.state_dict(),\n",
    "        \"scheduler\": sched.state_dict(),\n",
    "        \"cls_spaces\": cls_spaces,\n",
    "        \"embed_dim_image\": int(d_img),\n",
    "        \"embed_dim_text\": int(d_txt),\n",
    "        \"embed_dim_fused\": int(FUSED_DIM),\n",
    "        \"backbone\": dict(timm_id=TIMM_ID, patch_multiple=PATCH_MULTIPLE,\n",
    "                         target_size=TARGET_SIZE, pretrained=PRETRAINED),\n",
    "        \"text_model\": dict(id=HF_TEXT_MODEL, out_dim=TEXT_OUT_DIM, used=USE_TEXT),\n",
    "        \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "        \"min_samples_per_class\": MIN_SAMPLES_PER_CLASS,\n",
    "        \"drop_rows_all_unknown\": DROP_ROWS_ALL_UNKNOWN,\n",
    "    }\n",
    "    if USE_TEXT:\n",
    "        ckpt[\"text_enc\"] = _cpu_state_dict(text_enc)\n",
    "        ckpt[\"fusion\"]   = _cpu_state_dict(fusion)\n",
    "    path = RUN_DIR / f\"{tag}.pt\"\n",
    "    torch.save(ckpt, path)\n",
    "    return path\n",
    "\n",
    "# =====================================================\n",
    "# Training loop (on cached embeddings)\n",
    "# =====================================================\n",
    "ema_loss = None\n",
    "best_ema = float(\"inf\")\n",
    "\n",
    "if USE_TEXT:\n",
    "    text_enc.train()\n",
    "    fusion.train()\n",
    "heads.train()\n",
    "\n",
    "pbar = tqdm(range(TOTAL_STEPS), desc=f\"VQA(train cached + heads {'+ text' if USE_TEXT else ''}) -> {RUN_DIR.name}\")\n",
    "loader_it = iter(train_loader)\n",
    "\n",
    "for step in pbar:\n",
    "    try:\n",
    "        batch = next(loader_it)\n",
    "    except StopIteration:\n",
    "        loader_it = iter(train_loader)\n",
    "        batch = next(loader_it)\n",
    "\n",
    "    # image emb (already cached and normalized)\n",
    "    z_img = batch[\"z_img\"].to(device)  # [B, D_img]\n",
    "\n",
    "    if USE_TEXT:\n",
    "        # pre-tokenized fixed question path (fast)\n",
    "        z_txt = encode_text_fixed(z_img.shape[0], text_enc)  # [B, d_txt]\n",
    "        z = fusion(z_img, z_txt)\n",
    "    else:\n",
    "        z = z_img\n",
    "\n",
    "    # heads\n",
    "    out = heads(z)[\"cls\"]\n",
    "\n",
    "    # per-head masked loss (ignore unknown=-100)\n",
    "    loss_list = []\n",
    "    for field in cls_spaces.keys():\n",
    "        key = f\"y_cls_{field}\"\n",
    "        if key not in batch:\n",
    "            continue\n",
    "        y = batch[key].to(device)  # [B]\n",
    "        logits = out[field]        # [B, num_classes]\n",
    "        mask = (y != -100)\n",
    "        if mask.any():\n",
    "            loss_list.append(F.cross_entropy(logits[mask], y[mask], label_smoothing=LABEL_SMOOTH))\n",
    "\n",
    "    if not loss_list:\n",
    "        # No usable labels present for this batch; skip optimization/logging but keep the loop moving.\n",
    "        continue\n",
    "\n",
    "    loss = torch.stack(loss_list).sum()\n",
    "\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    if USE_TEXT:\n",
    "        torch.nn.utils.clip_grad_norm_(list(chain(text_enc.parameters(), fusion.parameters(), heads.parameters())), 1.0)\n",
    "    else:\n",
    "        torch.nn.utils.clip_grad_norm_(list(heads.parameters()), 1.0)\n",
    "    opt.step()\n",
    "    sched.step()\n",
    "\n",
    "    # logs / EMA / ckpts\n",
    "    cur_loss = float(loss.detach().item())\n",
    "    ema_loss = cur_loss if ema_loss is None else (EMA_BETA * ema_loss + (1 - EMA_BETA) * cur_loss)\n",
    "    lrs = [pg[\"lr\"] for pg in opt.param_groups]\n",
    "    pbar.set_postfix(loss=f\"{cur_loss:.4f}\", ema=f\"{ema_loss:.4f}\", lr_min=f\"{min(lrs):.2e}\", lr_max=f\"{max(lrs):.2e}\")\n",
    "\n",
    "    log_w.writerow([step, f\"{cur_loss:.6f}\", f\"{ema_loss:.6f}\", f\"{min(lrs):.6e}\", f\"{max(lrs):.6e}\"])\n",
    "    if step % 20 == 0:\n",
    "        log_f.flush()\n",
    "\n",
    "    if (step + 1) % SAVE_EVERY == 0:\n",
    "        save_ckpt(\"last\", step + 1)\n",
    "    if KEEP_EVERY and (step + 1) % KEEP_EVERY == 0:\n",
    "        save_ckpt(f\"step_{step + 1:06d}\", step + 1)\n",
    "    if ema_loss < best_ema:\n",
    "        best_ema = ema_loss\n",
    "        save_ckpt(\"best\", step + 1)\n",
    "\n",
    "# final save\n",
    "save_ckpt(\"last\", TOTAL_STEPS)\n",
    "log_f.close()\n",
    "print(f\"Saved checkpoints & logs to: {RUN_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0031b8",
   "metadata": {},
   "source": [
    "### 5-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "20d20d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_64364\\1351817744.py:96: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  autocast_ctx = torch.cuda.amp.autocast(enabled=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 1/5 =====  train=3150  val=788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1/25 :: EMA loss ~ 9.1557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 2/25 :: EMA loss ~ 6.6921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 3/25 :: EMA loss ~ 6.1200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 4/25 :: EMA loss ~ 5.7258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 5/25 :: EMA loss ~ 5.7220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 6/25 :: EMA loss ~ 5.4128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 7/25 :: EMA loss ~ 5.3807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 8/25 :: EMA loss ~ 5.3318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 9/25 :: EMA loss ~ 5.3659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 10/25 :: EMA loss ~ 5.2217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 11/25 :: EMA loss ~ 5.0434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 12/25 :: EMA loss ~ 5.1704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 13/25 :: EMA loss ~ 5.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 14/25 :: EMA loss ~ 4.9523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 15/25 :: EMA loss ~ 5.2142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 16/25 :: EMA loss ~ 5.0696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 17/25 :: EMA loss ~ 4.9953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 18/25 :: EMA loss ~ 4.9397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 19/25 :: EMA loss ~ 4.8209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 20/25 :: EMA loss ~ 4.7671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 21/25 :: EMA loss ~ 4.8943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 22/25 :: EMA loss ~ 4.9604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 23/25 :: EMA loss ~ 4.5940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 24/25 :: EMA loss ~ 4.5197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 25/25 :: EMA loss ~ 4.7664\n",
      "\n",
      "===== Fold 2/5 =====  train=3150  val=788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1/25 :: EMA loss ~ 9.0485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 2/25 :: EMA loss ~ 6.4557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 3/25 :: EMA loss ~ 6.1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 4/25 :: EMA loss ~ 5.8416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 5/25 :: EMA loss ~ 5.6462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 6/25 :: EMA loss ~ 5.4164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 7/25 :: EMA loss ~ 5.5657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 8/25 :: EMA loss ~ 5.4107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 9/25 :: EMA loss ~ 5.3277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 10/25 :: EMA loss ~ 5.2410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 11/25 :: EMA loss ~ 5.1456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 12/25 :: EMA loss ~ 5.0761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 13/25 :: EMA loss ~ 5.1057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 14/25 :: EMA loss ~ 4.9724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 15/25 :: EMA loss ~ 5.0786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 16/25 :: EMA loss ~ 4.9341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 17/25 :: EMA loss ~ 5.0561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 18/25 :: EMA loss ~ 4.9017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 19/25 :: EMA loss ~ 4.8092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 20/25 :: EMA loss ~ 4.6349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 21/25 :: EMA loss ~ 4.6489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 22/25 :: EMA loss ~ 4.8116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 23/25 :: EMA loss ~ 4.6917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 24/25 :: EMA loss ~ 4.5260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 25/25 :: EMA loss ~ 4.7181\n",
      "\n",
      "===== Fold 3/5 =====  train=3150  val=788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1/25 :: EMA loss ~ 9.2823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 2/25 :: EMA loss ~ 6.6623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 3/25 :: EMA loss ~ 6.1251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 4/25 :: EMA loss ~ 6.0144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 5/25 :: EMA loss ~ 5.7137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 6/25 :: EMA loss ~ 5.5601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 7/25 :: EMA loss ~ 5.5490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 8/25 :: EMA loss ~ 5.3803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 9/25 :: EMA loss ~ 5.3598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 10/25 :: EMA loss ~ 5.3166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 11/25 :: EMA loss ~ 5.1868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 12/25 :: EMA loss ~ 5.1459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 13/25 :: EMA loss ~ 5.0024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 14/25 :: EMA loss ~ 4.9905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 15/25 :: EMA loss ~ 4.9319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 16/25 :: EMA loss ~ 4.8688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 17/25 :: EMA loss ~ 4.9494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 18/25 :: EMA loss ~ 4.7499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 19/25 :: EMA loss ~ 4.7407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 20/25 :: EMA loss ~ 4.7132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 21/25 :: EMA loss ~ 4.5270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 22/25 :: EMA loss ~ 4.5572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 23/25 :: EMA loss ~ 4.6234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 24/25 :: EMA loss ~ 4.5978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 25/25 :: EMA loss ~ 4.4452\n",
      "\n",
      "===== Fold 4/5 =====  train=3151  val=787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1/25 :: EMA loss ~ 9.1428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 2/25 :: EMA loss ~ 6.5644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 3/25 :: EMA loss ~ 6.1990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 4/25 :: EMA loss ~ 5.8578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 5/25 :: EMA loss ~ 5.7518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 6/25 :: EMA loss ~ 5.4681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 7/25 :: EMA loss ~ 5.3692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 8/25 :: EMA loss ~ 5.4518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 9/25 :: EMA loss ~ 5.3986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 10/25 :: EMA loss ~ 5.2169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 11/25 :: EMA loss ~ 5.2009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 12/25 :: EMA loss ~ 5.1468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 13/25 :: EMA loss ~ 5.1643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 14/25 :: EMA loss ~ 5.0149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 15/25 :: EMA loss ~ 5.1749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 16/25 :: EMA loss ~ 4.9642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 17/25 :: EMA loss ~ 5.3027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 18/25 :: EMA loss ~ 5.0913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 19/25 :: EMA loss ~ 4.8566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 20/25 :: EMA loss ~ 4.9365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 21/25 :: EMA loss ~ 4.9731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 22/25 :: EMA loss ~ 4.6993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 23/25 :: EMA loss ~ 4.6314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 24/25 :: EMA loss ~ 4.4896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 25/25 :: EMA loss ~ 4.4129\n",
      "\n",
      "===== Fold 5/5 =====  train=3151  val=787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1/25 :: EMA loss ~ 9.0954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 2/25 :: EMA loss ~ 6.4866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 3/25 :: EMA loss ~ 6.1261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 4/25 :: EMA loss ~ 5.8658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 5/25 :: EMA loss ~ 5.5485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 6/25 :: EMA loss ~ 5.4953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 7/25 :: EMA loss ~ 5.6248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 8/25 :: EMA loss ~ 5.2263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 9/25 :: EMA loss ~ 5.3466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 10/25 :: EMA loss ~ 5.2445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 11/25 :: EMA loss ~ 5.1479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 12/25 :: EMA loss ~ 5.1101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 13/25 :: EMA loss ~ 4.9894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 14/25 :: EMA loss ~ 4.9370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 15/25 :: EMA loss ~ 4.8379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 16/25 :: EMA loss ~ 4.8042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 17/25 :: EMA loss ~ 4.9296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 18/25 :: EMA loss ~ 4.7048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 19/25 :: EMA loss ~ 4.8873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 20/25 :: EMA loss ~ 5.0950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 21/25 :: EMA loss ~ 4.8355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 22/25 :: EMA loss ~ 4.7905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 23/25 :: EMA loss ~ 4.5785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 24/25 :: EMA loss ~ 4.4716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 25/25 :: EMA loss ~ 4.5308\n",
      "\n",
      "[OK] Saved:\n",
      "  Tidy CSV: vqa\\cv5_20251015_195205\\cv5_tidy_metrics.csv\n",
      "  Macro-F1 mean ± std: vqa\\cv5_20251015_195205\\cv5_macro_f1_mean_std.png\n",
      "  Accuracy mean ± std: vqa\\cv5_20251015_195205\\cv5_accuracy_mean_std.png\n",
      "  Per-fold JSON/heads under: vqa\\cv5_20251015_195205\n"
     ]
    }
   ],
   "source": [
    "# vqa_cv5.py\n",
    "# 5-fold cross-validation for cached-embedding VQA (train_vqa_fast style)\n",
    "# - Uses same normalization + class spaces\n",
    "# - GroupKFold by dataset_id to avoid leakage\n",
    "# - Trains heads (+ optional text-tower last_k) on train folds\n",
    "# - Evaluates on held-out fold, saves CSV/JSON, and seaborn plots\n",
    "# - tqdm progress per epoch/batch\n",
    "# - Cleans label variants (N/A, n a, —, etc.) -> 'unknown'\n",
    "# - Optional rare-class filtering (remap rare labels to 'unknown')\n",
    "\n",
    "import os, csv, json, math, hashlib, warnings, re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# -------------------------\n",
    "# Paths & I/O\n",
    "# -------------------------\n",
    "IDX_PARQUET = \"metaspace_images_dump/msi_fm_samples3.parquet\"\n",
    "MAN_PARQUET = \"metaspace_images_dump/manifest_expanded.parquet\"\n",
    "\n",
    "RUN_ROOT = Path(\"vqa\")\n",
    "IMG_CACHE_DIR = RUN_ROOT / \"_img_cache\"  # must exist from your previous caching step\n",
    "STATS_DIR = RUN_ROOT / \"_stats_cache\"    # not used here but kept for parity\n",
    "\n",
    "CV_DIR = RUN_ROOT / (\"cv5_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "CV_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Config (match train_vqa_fast)\n",
    "# -------------------------\n",
    "SEED = 6740\n",
    "torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "USE_TEXT   = True      # set False to try image-only baseline\n",
    "LABEL_SMOOTH = 0.05\n",
    "\n",
    "# Timmed image encoder was already used to create cached embeddings\n",
    "TIMM_ID        = \"vit_small_patch14_reg4_dinov2.lvd142m\"\n",
    "PATCH_MULTIPLE = 14\n",
    "TARGET_SIZE    = 518\n",
    "\n",
    "# HF text tower partial tuning setup (same as fast script)\n",
    "HF_TEXT_MODEL       = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "TEXT_MAX_LEN        = 64\n",
    "TEXT_OUT_DIM        = 384\n",
    "TEXT_TRAIN_STRATEGY = \"last_k\"  # [\"none\",\"proj\",\"last_k\"]\n",
    "TEXT_TRAIN_LAST_K   = 2\n",
    "TEXT_FREEZE_EMBED   = True\n",
    "LR_TXT_BASE         = 5e-5\n",
    "LR_TXT_PROJ         = 1e-4\n",
    "\n",
    "# Heads / fusion\n",
    "LR_FUSION = 5e-4\n",
    "LR_HEADS  = 8e-4\n",
    "WD        = 0.05\n",
    "BATCH_SIZE = 512          # cached embeddings => large batch ok\n",
    "EPOCHS     = 25           # keep modest for CV\n",
    "AMP        = False        # cached embeddings: little gain; keep False for reproducibility\n",
    "NUM_WORKERS = 0           # bump if your I/O can handle it\n",
    "\n",
    "# Optional rare-class handling: remap labels with corpus count < N to 'unknown'\n",
    "MIN_SAMPLES_PER_CLASS = {\n",
    "    \"organism\": 100,\n",
    "    \"polarity\": 100,\n",
    "    \"Organism_Part\": 100,\n",
    "    \"Condition\": 100,\n",
    "    \"analyzerType\": 100,\n",
    "    \"ionisationSource\": 100,\n",
    "}\n",
    "\n",
    "# -------------------------\n",
    "# Utilities\n",
    "# -------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "autocast_ctx = torch.cuda.amp.autocast(enabled=False)\n",
    "\n",
    "NA_LIKE = {\n",
    "    \"na\", \"n/a\", \"n\\\\a\", \"n.a.\", \"n . a .\", \"n a\", \"n - a\", \"not applicable\",\n",
    "    \"none\", \"null\", \"nan\", \"-\", \"—\", \"unknown\", \"\"\n",
    "}\n",
    "\n",
    "def _canon_base(x):\n",
    "    if x is None:\n",
    "        return \"unknown\"\n",
    "    s = str(x).strip().lower()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)  # collapse whitespace\n",
    "    s = s.replace(\"_\", \" \").replace(\".\", \".\").strip()\n",
    "    if s in NA_LIKE:\n",
    "        return \"unknown\"\n",
    "    # common OCR-ish or punctuation variants to unknown\n",
    "    if re.fullmatch(r\"n[\\s\\./\\\\-]*a\", s):\n",
    "        return \"unknown\"\n",
    "    return s if s else \"unknown\"\n",
    "\n",
    "def normalize_organism(x):\n",
    "    s = _canon_base(x)\n",
    "    if s == \"unknown\": return s\n",
    "    if \"mus musculus\" in s or s in {\"mouse\", \"mouse brain\", \"mus\", \"m. musculus\"}:\n",
    "        return \"mus musculus\"\n",
    "    if \"homo sapiens\" in s or s in {\"human\", \"h. sapiens\"}:\n",
    "        return \"homo sapiens\"\n",
    "    return \"unknown\"\n",
    "\n",
    "def normalize_polarity(x):\n",
    "    s = _canon_base(x)\n",
    "    if s in {\"pos\", \"+\", \"positive\", \"positive ion mode\", \"positive mode\"}:  return \"positive\"\n",
    "    if s in {\"neg\", \"-\", \"negative\", \"negative ion mode\", \"negative mode\"}:  return \"negative\"\n",
    "    return \"unknown\"\n",
    "\n",
    "def normalize_text(x): \n",
    "    return _canon_base(x)\n",
    "\n",
    "def _build_vocab(series):\n",
    "    vals = sorted(set(series.dropna().astype(str).tolist()))\n",
    "    if \"unknown\" not in vals:\n",
    "        vals.append(\"unknown\")\n",
    "    return vals\n",
    "\n",
    "def _embed_key(path: str, timm_id: str, patch_multiple: int, target_size: int) -> Path:\n",
    "    h = hashlib.sha1(f\"{timm_id}|{patch_multiple}|{target_size}|{path}\".encode()).hexdigest()\n",
    "    return IMG_CACHE_DIR / f\"{h}.npy\"\n",
    "\n",
    "def _remap_rare_classes(df: pd.DataFrame, col: str, min_count: int) -> pd.Series:\n",
    "    counts = df[col].value_counts()\n",
    "    rare = set(counts[counts < min_count].index.tolist())\n",
    "    rare.discard(\"unknown\")  # always keep unknown\n",
    "    if not rare:\n",
    "        return df[col]\n",
    "    return df[col].apply(lambda v: \"unknown\" if v in rare else v)\n",
    "\n",
    "# -------------------------\n",
    "# Model bits (match your fast script)\n",
    "# -------------------------\n",
    "class HFTextEnc(nn.Module):\n",
    "    def __init__(self, name: str, out_dim: int,\n",
    "                 train_strategy: str = \"last_k\",\n",
    "                 last_k: int = 2,\n",
    "                 freeze_embed: bool = True):\n",
    "        super().__init__()\n",
    "        self.model = AutoModel.from_pretrained(name)\n",
    "        hid = self.model.config.hidden_size\n",
    "        self.proj = nn.Linear(hid, out_dim)\n",
    "\n",
    "        # freeze all\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad_(False)\n",
    "        if not freeze_embed:\n",
    "            for p in self.model.get_input_embeddings().parameters():\n",
    "                p.requires_grad_(True)\n",
    "\n",
    "        # dropouts off\n",
    "        if hasattr(self.model.config, \"hidden_dropout_prob\"):\n",
    "            self.model.config.hidden_dropout_prob = 0.0\n",
    "        if hasattr(self.model.config, \"attention_probs_dropout_prob\"):\n",
    "            self.model.config.attention_probs_dropout_prob = 0.0\n",
    "\n",
    "        if train_strategy == \"none\":\n",
    "            pass\n",
    "        elif train_strategy == \"proj\":\n",
    "            for p in self.proj.parameters(): p.requires_grad_(True)\n",
    "        elif train_strategy == \"last_k\":\n",
    "            enc = getattr(self.model, \"encoder\", None) or getattr(self.model, \"transformer\", None)\n",
    "            layers = None\n",
    "            for cand in (\"layer\",\"layers\",\"block\",\"h\"):\n",
    "                if hasattr(enc, cand):\n",
    "                    layers = getattr(enc, cand); break\n",
    "            if layers is None:\n",
    "                raise RuntimeError(\"Cannot locate transformer blocks for last_k tuning.\")\n",
    "            k = max(1, min(last_k, len(layers)))\n",
    "            for layer in layers[-k:]:\n",
    "                for p in layer.parameters():\n",
    "                    p.requires_grad_(True)\n",
    "            for p in self.proj.parameters(): p.requires_grad_(True)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown train_strategy={train_strategy}\")\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        out = self.model(input_ids=ids, attention_mask=mask, return_dict=True)\n",
    "        x = (out.last_hidden_state * mask.unsqueeze(-1)).sum(1) / (mask.sum(1, keepdim=True) + 1e-6)\n",
    "        return self.proj(x)\n",
    "\n",
    "    def param_groups(self, lr_base: float, lr_proj: float):\n",
    "        base_params, proj_params = [], []\n",
    "        for n, p in self.named_parameters():\n",
    "            if not p.requires_grad: continue\n",
    "            (proj_params if n.startswith(\"proj.\") else base_params).append(p)\n",
    "        groups = []\n",
    "        if base_params: groups.append({\"params\": base_params, \"lr\": lr_base})\n",
    "        if proj_params: groups.append({\"params\": proj_params, \"lr\": lr_proj})\n",
    "        return groups\n",
    "\n",
    "class Fusion(nn.Module):\n",
    "    def __init__(self, d_img, d_txt, d_out):\n",
    "        super().__init__()\n",
    "        self.ln_img = nn.LayerNorm(d_img)\n",
    "        self.ln_txt = nn.LayerNorm(d_txt)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_img + d_txt, d_out),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(d_out, d_out),\n",
    "        )\n",
    "    def forward(self, zi, zt):\n",
    "        x = torch.cat([self.ln_img(zi), self.ln_txt(zt)], dim=-1)\n",
    "        return self.mlp(x)\n",
    "\n",
    "class VQAHeads(nn.Module):\n",
    "    def __init__(self, embed_dim: int, cls_spaces: dict):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleDict({k: nn.Linear(embed_dim, len(v)) for k, v in cls_spaces.items()})\n",
    "        for lin in self.heads.values():\n",
    "            nn.init.trunc_normal_(lin.weight, std=0.02); nn.init.zeros_(lin.bias)\n",
    "    def forward(self, z):  # [B, D]\n",
    "        return {\"cls\": {k: h(z) for k, h in self.heads.items()}}\n",
    "\n",
    "# -------------------------\n",
    "# Dataframe + normalization (same as training) + rare-class remap\n",
    "# -------------------------\n",
    "def build_eval_dataframe() -> pd.DataFrame:\n",
    "    df_idx = pd.read_parquet(IDX_PARQUET)\n",
    "    df_man = pd.read_parquet(MAN_PARQUET)\n",
    "\n",
    "    need_cols = [\"dataset_id\",\"organism\",\"polarity\",\"Organism_Part\",\"Condition\",\"analyzerType\",\"ionisationSource\"]\n",
    "    man_sub = df_man[[c for c in need_cols if c in df_man.columns]].drop_duplicates(\"dataset_id\")\n",
    "\n",
    "    df = df_idx.merge(man_sub, on=\"dataset_id\", how=\"left\", suffixes=(\"\", \"_man\"))\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "    if \"sample_path\" not in df.columns:\n",
    "        raise ValueError(\"Index parquet must include 'sample_path' pointing to .npz files.\")\n",
    "\n",
    "    # Normalize labels\n",
    "    norm_map = [\n",
    "        (\"organism\", normalize_organism),\n",
    "        (\"polarity\", normalize_polarity),\n",
    "        (\"Organism_Part\", normalize_text),\n",
    "        (\"Condition\", normalize_text),\n",
    "        (\"analyzerType\", normalize_text),\n",
    "        (\"ionisationSource\", normalize_text),\n",
    "    ]\n",
    "    for col, fn in norm_map:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].map(fn)\n",
    "        else:\n",
    "            df[col] = \"unknown\"\n",
    "\n",
    "    # Rare-class remap to 'unknown' (to be ignored)\n",
    "    for col, min_count in MIN_SAMPLES_PER_CLASS.items():\n",
    "        if col in df.columns and min_count and min_count > 0:\n",
    "            df[col] = _remap_rare_classes(df, col, min_count)\n",
    "\n",
    "    return df\n",
    "\n",
    "# -------------------------\n",
    "# Dataset over cached embeddings\n",
    "# -------------------------\n",
    "class EmbedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, cls_spaces):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.cls_spaces = cls_spaces\n",
    "        self.label_maps = {k: {c:i for i,c in enumerate(v)} for k,v in cls_spaces.items()}\n",
    "        self.paths = self.df[\"sample_path\"].tolist()\n",
    "\n",
    "    def __len__(self): return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        ep = _embed_key(row[\"sample_path\"], TIMM_ID, PATCH_MULTIPLE, TARGET_SIZE)\n",
    "        if not ep.exists():\n",
    "            raise FileNotFoundError(f\"Missing cached embedding for {row['sample_path']} -> {ep}\")\n",
    "        z = np.load(ep, mmap_mode=\"r\")\n",
    "        out = {\"z_img\": torch.from_numpy(np.array(z, dtype=np.float32))}\n",
    "        # labels, -100 for unknown so we can ignore in loss/metrics\n",
    "        for field, col in [\n",
    "            (\"organism\",\"organism\"),\n",
    "            (\"polarity\",\"polarity\"),\n",
    "            (\"organ\",\"Organism_Part\"),\n",
    "            (\"condition\",\"Condition\"),\n",
    "            (\"analyzerType\",\"analyzerType\"),\n",
    "            (\"ionisationSource\",\"ionisationSource\"),\n",
    "        ]:\n",
    "            val = str(row.get(col, \"unknown\"))\n",
    "            y = self.label_maps[field].get(val, self.label_maps[field][\"unknown\"])\n",
    "            if val == \"unknown\": y = -100\n",
    "            out[f\"y_cls_{field}\"] = torch.tensor(y, dtype=torch.long)\n",
    "        return out\n",
    "\n",
    "# -------------------------\n",
    "# Metrics (robust to absent classes)\n",
    "# -------------------------\n",
    "def compute_metrics(y_true: np.ndarray, y_pred: np.ndarray, labels_full: List[str]):\n",
    "    if y_true.size == 0:\n",
    "        return {\"accuracy\": None, \"macro_f1\": None, \"per_class_f1\": {}}\n",
    "    acc = float(accuracy_score(y_true, y_pred))\n",
    "    macro = float(f1_score(y_true, y_pred, average=\"macro\"))\n",
    "    present = sorted(set(np.unique(y_true)).union(set(np.unique(y_pred))))\n",
    "    present_names = [labels_full[i] for i in present] if present else []\n",
    "    report = classification_report(\n",
    "        y_true, y_pred, labels=present,\n",
    "        target_names=present_names, output_dict=True, zero_division=0\n",
    "    ) if present else {}\n",
    "    per_class = {cls: float(v[\"f1-score\"]) for cls, v in report.items() if cls in present_names}\n",
    "    return {\"accuracy\": acc, \"macro_f1\": macro, \"per_class_f1\": per_class}\n",
    "\n",
    "# -------------------------\n",
    "# Train / Eval loop (per fold)\n",
    "# -------------------------\n",
    "FIXED_QUESTION = \"what is the organism?\"\n",
    "def build_modules(d_img: int, cls_spaces: Dict[str, List[str]]):\n",
    "    if USE_TEXT:\n",
    "        text_enc = HFTextEnc(\n",
    "            HF_TEXT_MODEL, TEXT_OUT_DIM,\n",
    "            train_strategy=TEXT_TRAIN_STRATEGY,\n",
    "            last_k=TEXT_TRAIN_LAST_K,\n",
    "            freeze_embed=TEXT_FREEZE_EMBED\n",
    "        ).to(device)\n",
    "        fusion = Fusion(d_img, TEXT_OUT_DIM, d_img + TEXT_OUT_DIM).to(device)\n",
    "        heads = VQAHeads(embed_dim=d_img + TEXT_OUT_DIM, cls_spaces=cls_spaces).to(device)\n",
    "    else:\n",
    "        text_enc, fusion = None, None\n",
    "        heads = VQAHeads(embed_dim=d_img, cls_spaces=cls_spaces).to(device)\n",
    "\n",
    "    # params & opt\n",
    "    groups = []\n",
    "    if USE_TEXT:\n",
    "        groups.extend(text_enc.param_groups(lr_base=LR_TXT_BASE, lr_proj=LR_TXT_PROJ))\n",
    "        groups.append({\"params\": fusion.parameters(), \"lr\": LR_FUSION})\n",
    "    groups.append({\"params\": heads.parameters(), \"lr\": LR_HEADS})\n",
    "\n",
    "    try:\n",
    "        opt = AdamW(groups, weight_decay=WD, fused=torch.cuda.is_available())\n",
    "    except TypeError:\n",
    "        opt = AdamW(groups, weight_decay=WD)\n",
    "    return text_enc, fusion, heads, opt\n",
    "\n",
    "def forward_batch(batch, text_ctx):\n",
    "    z_img = batch[\"z_img\"].to(device, non_blocking=True)\n",
    "    if USE_TEXT:\n",
    "        ids, mask, text_enc, fusion = text_ctx\n",
    "        ids = ids.expand(z_img.shape[0], -1).contiguous()\n",
    "        mask = mask.expand(z_img.shape[0], -1).contiguous()\n",
    "        # normalize text only in eval; train lets gradients through proj/last_k\n",
    "        z_txt = text_enc(ids, mask)\n",
    "        if not text_enc.training:\n",
    "            z_txt = F.normalize(z_txt, dim=-1)\n",
    "        z = fusion(z_img, z_txt)\n",
    "    else:\n",
    "        z = z_img\n",
    "    return z\n",
    "\n",
    "def train_one_epoch(loader, heads, opt, text_ctx, fold_idx: int, ep: int):\n",
    "    heads.train()\n",
    "    if USE_TEXT:\n",
    "        text_ctx[2].train()   # text_enc\n",
    "        text_ctx[3].train()   # fusion\n",
    "\n",
    "    ema = None\n",
    "    pbar = tqdm(loader, total=len(loader), desc=f\"Fold {fold_idx} | Epoch {ep}\", leave=False)\n",
    "    for batch in pbar:\n",
    "        z = forward_batch(batch, text_ctx)\n",
    "        out = heads(z)[\"cls\"]\n",
    "        # masked multi-head loss\n",
    "        loss_list = []\n",
    "        for field in heads.heads.keys():\n",
    "            y = batch[f\"y_cls_{field}\"].to(device, non_blocking=True)\n",
    "            mask = (y != -100)\n",
    "            if mask.any():\n",
    "                loss_list.append(F.cross_entropy(out[field][mask], y[mask], label_smoothing=LABEL_SMOOTH))\n",
    "        if not loss_list:\n",
    "            continue\n",
    "        loss = torch.stack(loss_list).sum()\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(heads.parameters(), 1.0)\n",
    "        if USE_TEXT:\n",
    "            torch.nn.utils.clip_grad_norm_(list(text_ctx[2].parameters()) + list(text_ctx[3].parameters()), 1.0)\n",
    "        opt.step()\n",
    "\n",
    "        cur = float(loss.detach().item())\n",
    "        ema = cur if ema is None else 0.98 * ema + 0.02 * cur\n",
    "        pbar.set_postfix(loss=f\"{cur:.4f}\", ema=f\"{ema:.4f}\")\n",
    "    return ema if ema is not None else 0.0\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_loader(loader, heads, text_ctx, cls_spaces):\n",
    "    heads.eval()\n",
    "    if USE_TEXT:\n",
    "        text_ctx[2].eval()\n",
    "        text_ctx[3].eval()\n",
    "\n",
    "    rows = []\n",
    "    for batch in loader:\n",
    "        z = forward_batch(batch, text_ctx)\n",
    "        out = heads(z)[\"cls\"]\n",
    "        rec = {}\n",
    "        for field in cls_spaces.keys():\n",
    "            logits = out[field].detach().cpu().numpy()\n",
    "            pred = logits.argmax(axis=1)\n",
    "            true = batch[f\"y_cls_{field}\"].detach().cpu().numpy()\n",
    "            rec[f\"pred_{field}\"] = pred\n",
    "            rec[f\"true_{field}\"] = true\n",
    "        rows.append(rec)\n",
    "\n",
    "    # stack and compute metrics (skip unknown=-100)\n",
    "    metrics = {}\n",
    "    for field, labels in cls_spaces.items():\n",
    "        pred = np.concatenate([r[f\"pred_{field}\"] for r in rows], axis=0)\n",
    "        true = np.concatenate([r[f\"true_{field}\"] for r in rows], axis=0)\n",
    "        m = (true != -100)\n",
    "        y_true = true[m]\n",
    "        y_pred = pred[m]\n",
    "        mtr = compute_metrics(y_true, y_pred, labels)\n",
    "        metrics[field] = {\"num_eval\": int(y_true.size), \"accuracy\": mtr[\"accuracy\"], \"macro_f1\": mtr[\"macro_f1\"]}\n",
    "    return metrics\n",
    "\n",
    "# -------------------------\n",
    "# Main CV\n",
    "# -------------------------\n",
    "def main():\n",
    "    # Build dataframe & class spaces\n",
    "    df = build_eval_dataframe()\n",
    "\n",
    "    # Class spaces MUST come from entire corpus (as in training)\n",
    "    cls_spaces = {\n",
    "        \"organism\":         _build_vocab(df[\"organism\"]),\n",
    "        \"polarity\":         _build_vocab(df[\"polarity\"]),\n",
    "        \"organ\":            _build_vocab(df[\"Organism_Part\"]),\n",
    "        \"condition\":        _build_vocab(df[\"Condition\"]),\n",
    "        \"analyzerType\":     _build_vocab(df[\"analyzerType\"]),\n",
    "        \"ionisationSource\": _build_vocab(df[\"ionisationSource\"]),\n",
    "    }\n",
    "\n",
    "    # Check cached embeddings exist for all samples\n",
    "    missing = [p for p in df[\"sample_path\"].tolist()\n",
    "               if not _embed_key(p, TIMM_ID, PATCH_MULTIPLE, TARGET_SIZE).exists()]\n",
    "    if missing:\n",
    "        raise FileNotFoundError(\n",
    "            f\"{len(missing)} embeddings missing under {IMG_CACHE_DIR}. \"\n",
    "            f\"Example:\\n  {missing[0]}\\nPlease run your caching step first.\"\n",
    "        )\n",
    "\n",
    "    # Probe image embedding dim\n",
    "    any_ep = _embed_key(df.iloc[0][\"sample_path\"], TIMM_ID, PATCH_MULTIPLE, TARGET_SIZE)\n",
    "    d_img = int(np.load(any_ep, mmap_mode=\"r\").shape[0])\n",
    "    fused_dim = d_img + (TEXT_OUT_DIM if USE_TEXT else 0)\n",
    "\n",
    "    # Build dataset\n",
    "    ds_all = EmbedDataset(df, cls_spaces)\n",
    "\n",
    "    # Tokenizer / fixed question once\n",
    "    if USE_TEXT:\n",
    "        tok = AutoTokenizer.from_pretrained(HF_TEXT_MODEL)\n",
    "        toks_once = tok(FIXED_QUESTION, padding=False, truncation=True, max_length=TEXT_MAX_LEN, return_tensors=\"pt\")\n",
    "        ids_base  = toks_once[\"input_ids\"].to(device)\n",
    "        mask_base = toks_once[\"attention_mask\"].to(device)\n",
    "    else:\n",
    "        ids_base = mask_base = None\n",
    "\n",
    "    # GroupKFold by dataset_id (avoid leakage)\n",
    "    groups = df[\"dataset_id\"].astype(str).values\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "    all_fold_rows = []     # tidy rows for plotting\n",
    "    fold_summaries = []    # per-fold JSON\n",
    "\n",
    "    for fold_idx, (tr_idx, va_idx) in enumerate(gkf.split(df, groups=groups, y=None), start=1):\n",
    "        print(f\"\\n===== Fold {fold_idx}/5 =====  train={len(tr_idx)}  val={len(va_idx)}\")\n",
    "        ds_tr = Subset(ds_all, tr_idx.tolist())\n",
    "        ds_va = Subset(ds_all, va_idx.tolist())\n",
    "\n",
    "        ld_tr = DataLoader(ds_tr, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=False, drop_last=False)\n",
    "        ld_va = DataLoader(ds_va, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=False, drop_last=False)\n",
    "\n",
    "        # Build fresh modules per fold\n",
    "        text_enc, fusion, heads, opt = build_modules(d_img, cls_spaces)\n",
    "\n",
    "        # Text context tuple for forward\n",
    "        text_ctx = [ids_base, mask_base, text_enc, fusion] if USE_TEXT else [None, None, None, None]\n",
    "\n",
    "        # Train\n",
    "        for ep in range(1, EPOCHS + 1):\n",
    "            ema = train_one_epoch(ld_tr, heads, opt, text_ctx, fold_idx=fold_idx, ep=ep)\n",
    "            print(f\"  Epoch {ep}/{EPOCHS} :: EMA loss ~ {ema:.4f}\")\n",
    "\n",
    "        # Eval\n",
    "        metrics = eval_loader(ld_va, heads, text_ctx, cls_spaces)\n",
    "\n",
    "        # Save fold artifacts\n",
    "        fold_dir = CV_DIR / f\"fold_{fold_idx}\"\n",
    "        fold_dir.mkdir(parents=True, exist_ok=True)\n",
    "        (fold_dir / \"metrics.json\").write_text(json.dumps(metrics, indent=2))\n",
    "\n",
    "        # Flatten tidy rows for plotting\n",
    "        for task, m in metrics.items():\n",
    "            all_fold_rows.append({\n",
    "                \"fold\": fold_idx,\n",
    "                \"task\": task,\n",
    "                \"num_eval\": m[\"num_eval\"],\n",
    "                \"accuracy\": m[\"accuracy\"],\n",
    "                \"macro_f1\": m[\"macro_f1\"],\n",
    "            })\n",
    "\n",
    "        # Minimal state save (heads only)\n",
    "        torch.save({\"heads\": {k: v.detach().cpu() for k, v in heads.state_dict().items()},\n",
    "                    \"embed_dim_image\": d_img,\n",
    "                    \"embed_dim_text\": (TEXT_OUT_DIM if USE_TEXT else 0),\n",
    "                    \"fused_dim\": fused_dim,\n",
    "                    \"cls_spaces\": cls_spaces},\n",
    "                   fold_dir / \"heads.pt\")\n",
    "\n",
    "        fold_summaries.append({\"fold\": fold_idx, \"metrics\": metrics})\n",
    "\n",
    "    # -------------------------\n",
    "    # Aggregate & plots (mean ± std)\n",
    "    # -------------------------\n",
    "    tidy_df = pd.DataFrame(all_fold_rows)\n",
    "    tidy_csv = CV_DIR / \"cv5_tidy_metrics.csv\"\n",
    "    tidy_df.to_csv(tidy_csv, index=False)\n",
    "    (CV_DIR / \"cv5_summary.json\").write_text(json.dumps(fold_summaries, indent=2))\n",
    "\n",
    "    # Compute mean and std per task\n",
    "    agg_df = tidy_df.groupby(\"task\", as_index=False).agg(\n",
    "        mean_macro_f1=(\"macro_f1\", \"mean\"),\n",
    "        std_macro_f1=(\"macro_f1\", \"std\"),\n",
    "        mean_acc=(\"accuracy\", \"mean\"),\n",
    "        std_acc=(\"accuracy\", \"std\")\n",
    "    )\n",
    "\n",
    "    # Plot helpers\n",
    "    def _plot_bar_with_err(df, y_col_mean, y_col_std, title, ylabel, filename, color):\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        sns.barplot(\n",
    "            data=df,\n",
    "            x=\"task\",\n",
    "            y=y_col_mean,\n",
    "            color=color,\n",
    "            edgecolor=\"black\"\n",
    "        )\n",
    "        plt.errorbar(\n",
    "            x=np.arange(len(df)),\n",
    "            y=df[y_col_mean],\n",
    "            yerr=df[y_col_std],\n",
    "            fmt=\"none\",\n",
    "            ecolor=\"black\",\n",
    "            capsize=4,\n",
    "            lw=1.2,\n",
    "        )\n",
    "        plt.title(title)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.xlabel(\"Task\")\n",
    "        plt.ylim(0, 1)\n",
    "        plt.tight_layout()\n",
    "        out = CV_DIR / filename\n",
    "        plt.savefig(out, dpi=200)\n",
    "        plt.close()\n",
    "        return out\n",
    "\n",
    "    out_f1 = _plot_bar_with_err(\n",
    "        agg_df, \"mean_macro_f1\", \"std_macro_f1\",\n",
    "        \"5-Fold CV — Macro-F1 (mean ± std)\", \"Macro-F1\",\n",
    "        \"cv5_macro_f1_mean_std.png\", color=\"steelblue\"\n",
    "    )\n",
    "    out_acc = _plot_bar_with_err(\n",
    "        agg_df, \"mean_acc\", \"std_acc\",\n",
    "        \"5-Fold CV — Accuracy (mean ± std)\", \"Accuracy\",\n",
    "        \"cv5_accuracy_mean_std.png\", color=\"lightcoral\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\n[OK] Saved:\")\n",
    "    print(f\"  Tidy CSV: {tidy_csv}\")\n",
    "    print(f\"  Macro-F1 mean ± std: {out_f1}\")\n",
    "    print(f\"  Accuracy mean ± std: {out_acc}\")\n",
    "    print(f\"  Per-fold JSON/heads under: {CV_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5cc88a",
   "metadata": {},
   "source": [
    "### Gradio app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f9b909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7871\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7871/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 403, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\fastapi\\applications.py\", line 1133, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\applications.py\", line 113, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\gradio\\brotli_middleware.py\", line 74, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\gradio\\route_utils.py\", line 882, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 63, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\fastapi\\middleware\\asyncexitstack.py\", line 18, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\routing.py\", line 716, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\routing.py\", line 736, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\routing.py\", line 290, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\fastapi\\routing.py\", line 123, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\fastapi\\routing.py\", line 109, in app\n",
      "    response = await f(request)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\fastapi\\routing.py\", line 389, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\fastapi\\routing.py\", line 288, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\gradio\\routes.py\", line 1671, in get_upload_progress\n",
      "    await asyncio.wait_for(\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\asyncio\\tasks.py\", line 445, in wait_for\n",
      "    return fut.result()\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\gradio\\route_utils.py\", line 528, in is_tracked\n",
      "    return await self._signals[upload_id].wait()\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\asyncio\\locks.py\", line 211, in wait\n",
      "    fut = self._get_loop().create_future()\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\asyncio\\mixins.py\", line 30, in _get_loop\n",
      "    raise RuntimeError(f'{self!r} is bound to a different event loop')\n",
      "RuntimeError: <asyncio.locks.Event object at 0x0000020F7B87BB20 [unset]> is bound to a different event loop\n",
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 403, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\fastapi\\applications.py\", line 1133, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\applications.py\", line 113, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\gradio\\brotli_middleware.py\", line 74, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\gradio\\route_utils.py\", line 882, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 63, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\fastapi\\middleware\\asyncexitstack.py\", line 18, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\routing.py\", line 716, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\routing.py\", line 736, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\routing.py\", line 290, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\fastapi\\routing.py\", line 123, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\fastapi\\routing.py\", line 109, in app\n",
      "    response = await f(request)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\fastapi\\routing.py\", line 389, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\fastapi\\routing.py\", line 288, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\gradio\\routes.py\", line 1671, in get_upload_progress\n",
      "    await asyncio.wait_for(\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\asyncio\\tasks.py\", line 445, in wait_for\n",
      "    return fut.result()\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\gradio\\route_utils.py\", line 528, in is_tracked\n",
      "    return await self._signals[upload_id].wait()\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\asyncio\\locks.py\", line 211, in wait\n",
      "    fut = self._get_loop().create_future()\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\asyncio\\mixins.py\", line 30, in _get_loop\n",
      "    raise RuntimeError(f'{self!r} is bound to a different event loop')\n",
      "RuntimeError: <asyncio.locks.Event object at 0x0000020F7B87BB20 [unset]> is bound to a different event loop\n",
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 403, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\fastapi\\applications.py\", line 1133, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\applications.py\", line 113, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\gradio\\brotli_middleware.py\", line 74, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\gradio\\route_utils.py\", line 882, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 63, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\fastapi\\middleware\\asyncexitstack.py\", line 18, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\routing.py\", line 716, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\routing.py\", line 736, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\routing.py\", line 290, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\fastapi\\routing.py\", line 123, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\fastapi\\routing.py\", line 109, in app\n",
      "    response = await f(request)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\fastapi\\routing.py\", line 389, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\fastapi\\routing.py\", line 288, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\gradio\\routes.py\", line 1671, in get_upload_progress\n",
      "    await asyncio.wait_for(\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\asyncio\\tasks.py\", line 445, in wait_for\n",
      "    return fut.result()\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\gradio\\route_utils.py\", line 528, in is_tracked\n",
      "    return await self._signals[upload_id].wait()\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\asyncio\\locks.py\", line 211, in wait\n",
      "    fut = self._get_loop().create_future()\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\asyncio\\mixins.py\", line 30, in _get_loop\n",
      "    raise RuntimeError(f'{self!r} is bound to a different event loop')\n",
      "RuntimeError: <asyncio.locks.Event object at 0x0000020F7B87BB20 [unset]> is bound to a different event loop\n",
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 403, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\fastapi\\applications.py\", line 1133, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\applications.py\", line 113, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\gradio\\brotli_middleware.py\", line 74, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\gradio\\route_utils.py\", line 882, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 63, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\fastapi\\middleware\\asyncexitstack.py\", line 18, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\routing.py\", line 716, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\routing.py\", line 736, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\routing.py\", line 290, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\fastapi\\routing.py\", line 123, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\fastapi\\routing.py\", line 109, in app\n",
      "    response = await f(request)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\fastapi\\routing.py\", line 389, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\fastapi\\routing.py\", line 288, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\gradio\\routes.py\", line 1671, in get_upload_progress\n",
      "    await asyncio.wait_for(\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\asyncio\\tasks.py\", line 445, in wait_for\n",
      "    return fut.result()\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\gradio\\route_utils.py\", line 528, in is_tracked\n",
      "    return await self._signals[upload_id].wait()\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\asyncio\\locks.py\", line 211, in wait\n",
      "    fut = self._get_loop().create_future()\n",
      "  File \"c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\asyncio\\mixins.py\", line 30, in _get_loop\n",
      "    raise RuntimeError(f'{self!r} is bound to a different event loop')\n",
      "RuntimeError: <asyncio.locks.Event object at 0x0000020F7B87BB20 [unset]> is bound to a different event loop\n"
     ]
    }
   ],
   "source": [
    "# vqa_app.py — MSI viewer + single-question CLS Q/A\n",
    "# pip install gradio==4.* pandas scikit-learn timm transformers torch torchvision torchaudio\n",
    "\n",
    "# --- Windows event-loop fix (MUST be before importing gradio) ---\n",
    "import os, asyncio\n",
    "if os.name == \"nt\":\n",
    "    try:\n",
    "        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "import re, json, math, hashlib, glob\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import gradio as gr\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# =========================\n",
    "# Heads we expose (CLS-only)\n",
    "# =========================\n",
    "CLS_HEADS = [\n",
    "    \"organism\",\n",
    "    \"polarity\",\n",
    "    \"organ\",\n",
    "    \"condition\",\n",
    "    \"analyzerType\",\n",
    "    \"ionisationSource\",\n",
    "]\n",
    "\n",
    "SEED = 6740\n",
    "torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# =========================\n",
    "# Fixed question (as trained)\n",
    "# =========================\n",
    "FIXED_QUESTION = \"what is the organism?\"\n",
    "\n",
    "# ===========================================\n",
    "# Image helpers (preview + robust normalization)\n",
    "# ===========================================\n",
    "def _safe_uint8(img: np.ndarray) -> np.ndarray:\n",
    "    img = np.nan_to_num(img, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    lo, hi = np.percentile(img, [1, 99])\n",
    "    if hi <= lo: hi = lo + 1e-6\n",
    "    img = np.clip((img - lo) / (hi - lo), 0, 1)\n",
    "    return (img * 255).astype(np.uint8)\n",
    "\n",
    "def _rgb_from_patch_pca(patch: np.ndarray) -> np.ndarray:\n",
    "    # patch: (C,H,W) in [0,1]\n",
    "    C, H, W = patch.shape\n",
    "    X = patch.reshape(C, -1).T  # (H*W, C)\n",
    "    if C > 0:\n",
    "        X = X - X.mean(axis=0, keepdims=True)\n",
    "    comps = min(3, max(1, C))\n",
    "    if C >= comps:\n",
    "        pca = PCA(n_components=comps, svd_solver=\"randomized\")\n",
    "        Y = pca.fit_transform(X)\n",
    "    else:\n",
    "        Y = np.zeros((H * W, comps), dtype=np.float32)\n",
    "    rgb = np.zeros((H * W, 3), dtype=np.float32)\n",
    "    rgb[:, :comps] = Y\n",
    "    rgb = rgb.reshape(H, W, 3)\n",
    "    out = np.zeros_like(rgb, dtype=np.uint8)\n",
    "    for i in range(3):\n",
    "        out[..., i] = _safe_uint8(rgb[..., i])\n",
    "    return out\n",
    "\n",
    "def _single_channel_gray(patch: np.ndarray, idx: int) -> np.ndarray:\n",
    "    idx = int(np.clip(int(idx), 0, max(0, patch.shape[0] - 1)))\n",
    "    ch = patch[idx]\n",
    "    return np.stack([_safe_uint8(ch)] * 3, axis=-1)\n",
    "\n",
    "def _load_npz_patch(npz_file) -> Tuple[np.ndarray, np.ndarray, str]:\n",
    "    path = npz_file.name if hasattr(npz_file, \"name\") else npz_file\n",
    "    with np.load(path, mmap_mode=\"r\") as z:\n",
    "        patch = z[\"patch\"].astype(np.float32)  # (C,H,W)\n",
    "        mz    = z[\"mz\"].astype(np.float32)\n",
    "    # normalize if uint16 scale\n",
    "    if patch.max() > 1.0:\n",
    "        patch /= 65535.0\n",
    "    return patch, mz, path\n",
    "\n",
    "# ===========================================\n",
    "# Intent detection (CLS focus; legacy fallbacks)\n",
    "# ===========================================\n",
    "INTENT = {\n",
    "    \"organism\":         re.compile(r\"\\b(organism|species)\\b\", re.I),\n",
    "    \"polarity\":         re.compile(r\"\\bpolari(?:ty)?\\b\", re.I),\n",
    "    \"organ\":            re.compile(r\"\\b(organ(?:ism)?\\s*part|organ\\b|tissue)\\b\", re.I),\n",
    "    \"condition\":        re.compile(r\"\\b(condition|status)\\b\", re.I),\n",
    "    \"analyzerType\":     re.compile(r\"\\banaly[sz]er(?:\\s*type)?\\b\", re.I),\n",
    "    \"ionisationSource\": re.compile(r\"\\bion(i[sz]ation)?\\s*source\\b|\\bion[i|z]iser\\b\", re.I),\n",
    "    # legacy examples\n",
    "    \"left_right\": re.compile(r\"\\b(left|right).*(bright|darker|brighter)\\b\", re.I),\n",
    "    \"count_5pct\": re.compile(r\"\\bhow many\\b.*(five|5)\\s*percent.*(non[- ]?zero|nonzero)\", re.I),\n",
    "    \"mz_yesno\":   re.compile(r\"\\b(ion|peak).*(near|around)\\s*m/?z\\s*([0-9]+(?:\\.[0-9]+)?)\", re.I),\n",
    "}\n",
    "\n",
    "def detect_intent(question: str) -> Tuple[str, Optional[str]]:\n",
    "    q = (question or \"\").strip()\n",
    "    if not q:\n",
    "        return (\"none\", None)\n",
    "    for head in CLS_HEADS:\n",
    "        if INTENT[head].search(q):\n",
    "            return (\"cls\", head)\n",
    "    if INTENT[\"left_right\"].search(q):\n",
    "        return (\"legacy_left_right\", None)\n",
    "    if INTENT[\"count_5pct\"].search(q):\n",
    "        return (\"legacy_count_5pct\", None)\n",
    "    m = INTENT[\"mz_yesno\"].search(q)\n",
    "    if m:\n",
    "        return (\"legacy_mz_yesno\", float(m.group(3)))\n",
    "    return (\"cls\", \"auto\")\n",
    "\n",
    "# ===========================================\n",
    "# Summarization helpers\n",
    "# ===========================================\n",
    "def _best_cls_head(cls_dict: dict):\n",
    "    best_h, best_v = None, None\n",
    "    for h, d in (cls_dict or {}).items():\n",
    "        if not isinstance(d, dict):\n",
    "            continue\n",
    "        if best_v is None or float(d.get(\"confidence\", 0.0)) > float(best_v.get(\"confidence\", 0.0)):\n",
    "            best_h, best_v = h, d\n",
    "    return best_h, best_v\n",
    "\n",
    "def _cls_summary(cls_dict: dict, target: Optional[str]):\n",
    "    if not cls_dict:\n",
    "        return \"I couldn't infer a class from this model.\"\n",
    "    if target and target != \"auto\":\n",
    "        for k, d in cls_dict.items():\n",
    "            if k.lower() == target.lower():\n",
    "                return f\"**{k}** → **{d.get('pred','?')}**.\"\n",
    "    k, d = _best_cls_head(cls_dict)\n",
    "    if k is None or d is None:\n",
    "        return \"I couldn't infer a class from this model.\"\n",
    "    return f\"**{k}** → **{d.get('pred','?')}**.\"\n",
    "\n",
    "def summarize_filtered(result_item: dict, intent_kind: str, intent_target: Optional[str]):\n",
    "    r = result_item.get(\"result\", {})\n",
    "    cls_dict = r.get(\"cls\")\n",
    "    if intent_kind == \"cls\":\n",
    "        return _cls_summary(cls_dict, intent_target)\n",
    "    yn = r.get(\"yesno\", None)\n",
    "    if intent_kind in (\"legacy_left_right\", \"legacy_mz_yesno\", \"legacy_count_5pct\"):\n",
    "        if isinstance(yn, dict) and \"pred\" in yn:\n",
    "            return f\"**{str(yn['pred']).upper()}**.\"\n",
    "        return _cls_summary(cls_dict, target=None)\n",
    "    return _cls_summary(cls_dict, target=None)\n",
    "\n",
    "# ===========================================\n",
    "# Model components (match training/eval)\n",
    "# ===========================================\n",
    "import timm\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "class FrozenBackbone(nn.Module):\n",
    "    \"\"\"timm ViT backbone returning [B, D] CLS-like embedding; all params frozen.\"\"\"\n",
    "    def __init__(self, timm_name: str, pretrained: bool = True):\n",
    "        super().__init__()\n",
    "        self.m = timm.create_model(timm_name, pretrained=pretrained, num_classes=0)\n",
    "        for p in self.m.parameters():\n",
    "            p.requires_grad_(False)\n",
    "        self.m.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x3):  # [N,3,H,W]\n",
    "        feats = self.m.forward_features(x3)\n",
    "        if isinstance(feats, dict):\n",
    "            if 'x_norm_clstoken' in feats:  return feats['x_norm_clstoken']\n",
    "            if 'cls_token' in feats:        return feats['cls_token']\n",
    "            if 'avgpool' in feats:          return feats['avgpool']\n",
    "            for k in ('last_hidden_state', 'tokens', 'x'):\n",
    "                if k in feats and torch.is_tensor(feats[k]):\n",
    "                    t = feats[k]\n",
    "                    return t[:, 0] if t.dim() == 3 else t\n",
    "        if torch.is_tensor(feats):\n",
    "            return feats[:, 0] if feats.dim() == 3 else feats\n",
    "        return feats.mean(dim=-2)\n",
    "\n",
    "def crop_resize_to_target(x3: torch.Tensor, target=224, patch_multiple=16) -> torch.Tensor:\n",
    "    _, _, H, W = x3.shape\n",
    "    Hc = (H // patch_multiple) * patch_multiple\n",
    "    Wc = (W // patch_multiple) * patch_multiple\n",
    "    dh = (H - Hc) // 2; dw = (W - Wc) // 2\n",
    "    if Hc > 0 and Wc > 0:\n",
    "        x3 = x3[:, :, dh:dh+Hc, dw:dw+Wc]\n",
    "    if (Hc, Wc) != (target, target):\n",
    "        x3 = F.interpolate(x3, size=(target, target), mode=\"bilinear\", align_corners=False)\n",
    "    return x3\n",
    "\n",
    "class HFTextEnc(nn.Module):\n",
    "    \"\"\"Trainable HuggingFace encoder + linear projection (used during training).\"\"\"\n",
    "    def __init__(self, name: str, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.model = AutoModel.from_pretrained(name)\n",
    "        hid = self.model.config.hidden_size\n",
    "        self.proj = nn.Linear(hid, out_dim)\n",
    "        self.name = name\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        out = self.model(input_ids=ids, attention_mask=mask, return_dict=True)\n",
    "        x = (out.last_hidden_state * mask.unsqueeze(-1)).sum(1) / (mask.sum(1, keepdim=True) + 1e-6)\n",
    "        return self.proj(x)  # [B, out_dim]\n",
    "\n",
    "class Fusion(nn.Module):\n",
    "    \"\"\"Exactly matches training/eval: LN(img), LN(txt), MLP on concat.\"\"\"\n",
    "    def __init__(self, d_img, d_txt, d_out):\n",
    "        super().__init__()\n",
    "        self.ln_img = nn.LayerNorm(d_img)\n",
    "        self.ln_txt = nn.LayerNorm(d_txt)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_img + d_txt, d_out),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(d_out, d_out),\n",
    "        )\n",
    "    def forward(self, zi, zt):\n",
    "        x = torch.cat([self.ln_img(zi), self.ln_txt(zt)], dim=-1)\n",
    "        return self.mlp(x)\n",
    "\n",
    "class VQAHeads(nn.Module):\n",
    "    \"\"\"Per-task linear heads over fused embedding z_fused.\"\"\"\n",
    "    def __init__(self, embed_dim: int, cls_spaces: Dict[str, List[str]]):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleDict({k: nn.Linear(embed_dim, len(v)) for k, v in cls_spaces.items()})\n",
    "        for lin in self.heads.values():\n",
    "            nn.init.trunc_normal_(lin.weight, std=0.02); nn.init.zeros_(lin.bias)\n",
    "\n",
    "    def forward(self, z):  # [B, D_fused]\n",
    "        return {k: h(z) for k, h in self.heads.items()}\n",
    "\n",
    "# ===========================================\n",
    "# Loading & inference utils\n",
    "# ===========================================\n",
    "RUN_ROOT = Path(\"vqa\")\n",
    "IMG_CACHE_DIR = RUN_ROOT / \"_img_cache\"\n",
    "STATS_DIR    = RUN_ROOT / \"_stats_cache\"\n",
    "\n",
    "def _pick_ckpt(run_dir: str) -> str:\n",
    "    best = os.path.join(run_dir, \"best.pt\")\n",
    "    last = os.path.join(run_dir, \"last.pt\")\n",
    "    if os.path.exists(best): return best\n",
    "    if os.path.exists(last): return last\n",
    "    cand = [os.path.join(run_dir, f) for f in os.listdir(run_dir) if f.endswith(\".pt\")]\n",
    "    if not cand:\n",
    "        raise FileNotFoundError(f\"No checkpoint found in {run_dir}\")\n",
    "    return sorted(cand)[-1]\n",
    "\n",
    "def _load_config(run_dir: str) -> dict:\n",
    "    cfg_path = os.path.join(run_dir, \"config.json\")\n",
    "    if not os.path.exists(cfg_path):\n",
    "        raise FileNotFoundError(f\"Missing config.json in {run_dir}\")\n",
    "    return json.load(open(cfg_path, \"r\"))\n",
    "\n",
    "def _per_image_channel_zscore(x: torch.Tensor) -> torch.Tensor:\n",
    "    # x: [1,C,H,W], return z-scored per-channel over H*W (fallback of last resort)\n",
    "    B, C, H, W = x.shape\n",
    "    xm = x.view(B, C, -1).mean(dim=-1, keepdim=True).view(B, C, 1, 1)\n",
    "    xs = x.view(B, C, -1).std(dim=-1, keepdim=True).view(B, C, 1, 1).clamp_min(1e-6)\n",
    "    return (x - xm) / xs\n",
    "\n",
    "def _embed_key(path, timm_id, patch_multiple, target_size):\n",
    "    h = hashlib.sha1(f\"{timm_id}|{patch_multiple}|{target_size}|{path}\".encode()).hexdigest()\n",
    "    return IMG_CACHE_DIR / f\"{h}.npy\"\n",
    "\n",
    "def _load_stats_or_none(channels_per_view: int, input_size: int):\n",
    "    \"\"\"\n",
    "    Try to load a mu/std cache that matches channels_per_view & input_size.\n",
    "    We pick the most recent matching file from STATS_DIR.\n",
    "    \"\"\"\n",
    "    pattern = str(STATS_DIR / f\"mu_std_c{int(channels_per_view)}_in{int(input_size)}_*.npz\")\n",
    "    files = sorted(glob.glob(pattern))\n",
    "    if not files:\n",
    "        return None, None\n",
    "    z = np.load(files[-1], allow_pickle=True)\n",
    "    mu, std = z[\"mu\"], z[\"std\"]\n",
    "    return mu.astype(np.float32), np.maximum(std.astype(np.float32), 1e-6)\n",
    "\n",
    "def _select_k_first(patch: np.ndarray, k: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Match training behavior: keep channels in original order, truncate/pad to k.\n",
    "    If C >= k: take first k. If C < k: repeat from start to reach k.\n",
    "    \"\"\"\n",
    "    C, H, W = patch.shape\n",
    "    if C >= k:\n",
    "        return patch[:k]\n",
    "    reps = int(np.ceil(k / max(C, 1)))\n",
    "    tiled = np.tile(patch, (reps, 1, 1))[:k]\n",
    "    return tiled\n",
    "\n",
    "@torch.no_grad()\n",
    "def _encode_image_mean_cls_fallback_with_stats(\n",
    "    patch_chw: np.ndarray,\n",
    "    bb: FrozenBackbone,\n",
    "    *,\n",
    "    target: int,\n",
    "    patch_multiple: int,\n",
    "    channels_per_view: int,\n",
    "    input_size: int,\n",
    "    mu: Optional[np.ndarray],\n",
    "    std: Optional[np.ndarray],\n",
    "    channels_per_step: int = 16\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Fallback encoder that attempts to match training:\n",
    "    - select top-variance channels -> exactly channels_per_view\n",
    "    - normalize with dataset mu/std if available; else per-image z-score\n",
    "    - crop/resize & mean-CLS over channels\n",
    "    \"\"\"\n",
    "    # 1) channel selection\n",
    "    x = _select_k_first(patch_chw, channels_per_view)  # [K,H,W]\n",
    "    x = torch.from_numpy(x).unsqueeze(0)  # [1,K,H,W]\n",
    "\n",
    "    # 2) normalization\n",
    "    if mu is not None and std is not None and len(mu) >= channels_per_view and len(std) >= channels_per_view:\n",
    "        mu_t = torch.from_numpy(mu[:channels_per_view]).view(1, -1, 1, 1)\n",
    "        sd_t = torch.from_numpy(std[:channels_per_view]).view(1, -1, 1, 1)\n",
    "        x = (x - mu_t) / sd_t\n",
    "    else:\n",
    "        x = _per_image_channel_zscore(x)\n",
    "\n",
    "    B, K, H, W = x.shape\n",
    "\n",
    "    # 3) flatten channels to batch, replicate to 3ch\n",
    "    x_flat = x.permute(0, 2, 3, 1).contiguous().view(B * K, 1, H, W)\n",
    "    x_rgb  = x_flat.repeat(1, 3, 1, 1)\n",
    "\n",
    "    # 4) chunk over channels\n",
    "    cls_chunks = []\n",
    "    step = channels_per_step if target == 224 else max(4, channels_per_step // 2)\n",
    "    for s in range(0, B * K, step):\n",
    "        e = min(s + step, B * K)\n",
    "        xr = crop_resize_to_target(x_rgb[s:e], target=target, patch_multiple=patch_multiple)\n",
    "        cls = bb(xr.to(DEVICE))             # [N, D_img]\n",
    "        cls_chunks.append(cls.float().cpu())\n",
    "    cls_all = torch.cat(cls_chunks, dim=0)  # [K, D_img]\n",
    "    z_img = cls_all.mean(dim=0, keepdim=True)  # [1, D_img]\n",
    "    return F.normalize(z_img, dim=-1)\n",
    "\n",
    "def _softmax_top(logits: torch.Tensor, vocab: List[str]) -> Tuple[str, float]:\n",
    "    probs = logits.softmax(dim=-1)[0]  # [K]\n",
    "    conf, idx = float(probs.max().item()), int(probs.argmax().item())\n",
    "    pred = vocab[idx] if 0 <= idx < len(vocab) else \"unknown\"\n",
    "    return pred, conf\n",
    "\n",
    "def _encode_fixed_question(tok, text_enc, max_len):\n",
    "    t = tok(FIXED_QUESTION, padding=False, truncation=True, max_length=max_len, return_tensors=\"pt\")\n",
    "    ids  = t[\"input_ids\"].to(DEVICE)\n",
    "    mask = t[\"attention_mask\"].to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        z_txt = F.normalize(text_enc(ids, mask), dim=-1)\n",
    "    return z_txt\n",
    "\n",
    "def _normalize_question(q: str) -> str:\n",
    "    q = (q or \"\").strip()\n",
    "    if not q:\n",
    "        return \"What organism is this sample?\"\n",
    "    return q\n",
    "\n",
    "def _apply_unknown_penalty(logits: torch.Tensor, vocab: List[str], penalty: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Subtract a constant from the 'unknown' logit (if present) to reduce its dominance.\n",
    "    logits: [1, K]\n",
    "    \"\"\"\n",
    "    if penalty <= 0:\n",
    "        return logits\n",
    "    try:\n",
    "        unk_idx = vocab.index(\"unknown\")\n",
    "    except ValueError:\n",
    "        return logits\n",
    "    out = logits.clone()\n",
    "    out[0, unk_idx] = out[0, unk_idx] - float(penalty)\n",
    "    return out\n",
    "\n",
    "# ===========================================\n",
    "# Core: run one question on one .npz\n",
    "# ===========================================\n",
    "def vqa_on_npz_single(\n",
    "    run_dir: str,\n",
    "    npz_path: str,\n",
    "    question: str\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Loads ckpt + config from run_dir (frozen image backbone setup), runs one question on one .npz.\n",
    "    Returns:\n",
    "      { \"result\": { \"cls\": { head: {\"pred\": str, \"confidence\": float}, ... } },\n",
    "        \"meta\": { \"used_cached_embedding\": bool, \"used_dataset_stats\": bool } }\n",
    "    \"\"\"\n",
    "    # ----- Load cfg + ckpt\n",
    "    ckpt_path = _pick_ckpt(run_dir)\n",
    "    cfg = _load_config(run_dir)\n",
    "    state = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "    # Backbone config from training\n",
    "    timm_id        = cfg.get(\"timm_id\", \"vit_small_patch14_reg4_dinov2.lvd142m\")\n",
    "    patch_multiple = int(cfg.get(\"patch_multiple\", 14))\n",
    "    target_size    = int(cfg.get(\"target_size\", 518))\n",
    "    text_model_id  = cfg.get(\"hf_text_model\", \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    text_out_dim   = int(cfg.get(\"text_out_dim\", cfg.get(\"embed_dim_text\", 384)))\n",
    "    channels_per_view = int(cfg.get(\"channels_per_view\", 64))\n",
    "    input_size     = int(cfg.get(\"input_size\", 256))\n",
    "\n",
    "    # Class spaces (from ckpt if available to ensure exact vocab)\n",
    "    cls_spaces = state.get(\"cls_spaces\", cfg.get(\"cls_spaces\", {h: [\"unknown\"] for h in CLS_HEADS}))\n",
    "\n",
    "    # ----- Build modules\n",
    "    bb = FrozenBackbone(timm_id, pretrained=True).to(DEVICE).eval()\n",
    "    tok = AutoTokenizer.from_pretrained(text_model_id)\n",
    "    text_enc = HFTextEnc(text_model_id, out_dim=text_out_dim).to(DEVICE).eval()\n",
    "\n",
    "    d_img     = int(cfg.get(\"embed_dim_image\", 384))\n",
    "    fused_dim = int(cfg.get(\"embed_dim_fused\", d_img + text_out_dim))\n",
    "    fusion    = Fusion(d_img, text_out_dim, fused_dim).to(DEVICE).eval()\n",
    "    heads     = VQAHeads(embed_dim=fused_dim, cls_spaces=cls_spaces).to(DEVICE).eval()\n",
    "\n",
    "    # ----- Load weights (strict for fusion/heads, relaxed for text_enc)\n",
    "    if \"text_enc\" in state:\n",
    "        text_enc.load_state_dict(state[\"text_enc\"], strict=False)\n",
    "    if \"fusion\" in state:\n",
    "        fusion.load_state_dict(state[\"fusion\"], strict=True)\n",
    "    if \"heads\" in state:\n",
    "        heads.load_state_dict(state[\"heads\"], strict=True)\n",
    "\n",
    "    # ----- Load sample\n",
    "    with np.load(npz_path, mmap_mode=\"r\") as z:\n",
    "        patch = z[\"patch\"].astype(np.float32)  # (C,H,W)\n",
    "        if patch.max() > 1.0: patch /= 65535.0\n",
    "\n",
    "    # ----- Prefer cached image embedding (identical to eval); fallback to train-like path\n",
    "    embed_path = _embed_key(npz_path, timm_id, patch_multiple, target_size)\n",
    "    meta_used_cache = False\n",
    "    meta_used_stats = False\n",
    "\n",
    "    if embed_path.exists():\n",
    "        z_img = torch.from_numpy(np.load(embed_path, mmap_mode=\"r\")).unsqueeze(0).to(DEVICE).float()\n",
    "        z_img = F.normalize(z_img, dim=-1)\n",
    "        meta_used_cache = True\n",
    "    else:\n",
    "        mu, std = _load_stats_or_none(channels_per_view=channels_per_view, input_size=input_size)\n",
    "        meta_used_stats = mu is not None and std is not None\n",
    "        z_img = _encode_image_mean_cls_fallback_with_stats(\n",
    "            patch, bb,\n",
    "            target=target_size,\n",
    "            patch_multiple=patch_multiple,\n",
    "            channels_per_view=channels_per_view,\n",
    "            input_size=input_size,\n",
    "            mu=mu,\n",
    "            std=std,\n",
    "            channels_per_step=16\n",
    "        ).to(DEVICE)\n",
    "\n",
    "    # ----- Encode text as fixed prompt (matches training)\n",
    "    _ = _normalize_question(question)  # user text ignored to match training; kept for future multi-prompt training\n",
    "    z_txt = _encode_fixed_question(tok, text_enc, max_len=int(cfg.get(\"text_max_len\", 64)))\n",
    "\n",
    "    # ----- Fuse & predict\n",
    "    with torch.no_grad():\n",
    "        z_fused = fusion(z_img, z_txt)            # [1, D_fused]\n",
    "        logits_dict = heads(z_fused)              # dict of head_name -> [1, K]\n",
    "\n",
    "    UNKNOWN_LOGIT_PENALTY = 0.7  # try 0.5..1.0 if 'unknown' is still over-predicted\n",
    "\n",
    "    # ----- Build result\n",
    "    res = {\"cls\": {}, \"meta\": {\"used_cached_embedding\": bool(meta_used_cache),\n",
    "                            \"used_dataset_stats\": bool(meta_used_stats)}}\n",
    "    for head, vocab in cls_spaces.items():\n",
    "        if head not in logits_dict:\n",
    "            continue\n",
    "        # apply penalty before softmax\n",
    "        adj = _apply_unknown_penalty(logits_dict[head], vocab, UNKNOWN_LOGIT_PENALTY)\n",
    "        pred, conf = _softmax_top(adj, vocab)\n",
    "        res[\"cls\"][head] = {\"pred\": pred, \"confidence\": conf}\n",
    "\n",
    "    return {\"result\": res}\n",
    "\n",
    "# ===========================================\n",
    "# Preview handler (no models)\n",
    "# ===========================================\n",
    "def _preview(npz_file, view_mode, ch_index):\n",
    "    if npz_file is None:\n",
    "        return None\n",
    "    patch, mz, _ = _load_npz_patch(npz_file)\n",
    "    if view_mode == \"PCA RGB\":\n",
    "        return _rgb_from_patch_pca(patch)\n",
    "    return _single_channel_gray(patch, int(ch_index))\n",
    "\n",
    "# ===========================================\n",
    "# Core run handler (loads run, answers one question)\n",
    "# ===========================================\n",
    "def run_basic(vqa_run, npz_file, view_mode, ch_index, question, state):\n",
    "    if npz_file is None:\n",
    "        return None, \"Please upload a sample first.\", state\n",
    "\n",
    "    # preview image\n",
    "    patch, mz, real_path = _load_npz_patch(npz_file)\n",
    "    if view_mode == \"PCA RGB\":\n",
    "        rgb = _rgb_from_patch_pca(patch)\n",
    "    else:\n",
    "        rgb = _single_channel_gray(patch, int(ch_index))\n",
    "\n",
    "    if not question or not str(question).strip():\n",
    "        return rgb, \"Type a question like **What organism is this sample?**\", state\n",
    "\n",
    "    try:\n",
    "        out = vqa_on_npz_single(run_dir=str(vqa_run), npz_path=real_path, question=str(question))\n",
    "        ikind, itarget = detect_intent(question)\n",
    "        answer_text = summarize_filtered(out, ikind, itarget)\n",
    "\n",
    "        meta = out.get(\"result\", {}).get(\"meta\", {})\n",
    "        used_cache = meta.get(\"used_cached_embedding\", False)\n",
    "        used_stats = meta.get(\"used_dataset_stats\", False)\n",
    "\n",
    "    except Exception as e:\n",
    "        answer_text = f\"Error while running the model: {e}\"\n",
    "\n",
    "    return rgb, answer_text, state\n",
    "\n",
    "# ===========================================\n",
    "# UI (Gradio)\n",
    "# ===========================================\n",
    "with gr.Blocks(title=\"metaboFM VQA\") as demo:\n",
    "    gr.Markdown(\n",
    "        \"## metaboFM\\n\"\n",
    "        \"Upload an MSI patch (`.npz` with arrays `patch` (C,H,W)), preview it, and ask:\\n\"\n",
    "        \"- *What organism is this sample?*\\n\"\n",
    "        \"- *What is the ionization polarity?*\\n\"\n",
    "        \"- *Which organ is this sample from?*\\n\"\n",
    "        \"- *What is the sample condition?*\\n\"\n",
    "        \"- *What analyzer type / ionisation source was used?*\\n\"\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=2):\n",
    "            npz_file  = gr.File(label=\"Upload .npz (must contain 'patch' and 'mz')\", file_types=[\".npz\"])\n",
    "            view_mode = gr.Radio(choices=[\"PCA RGB\", \"Single Channel\"], value=\"PCA RGB\", label=\"View\")\n",
    "            ch_index  = gr.Slider(label=\"Channel (for Single Channel view)\", minimum=0, maximum=255, step=1, value=0)\n",
    "            img_out   = gr.Image(label=\"Preview\", type=\"numpy\")\n",
    "\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Image(\"metabofm.png\", label=\"\", show_label=False, container=False, interactive=False, height=220)\n",
    "\n",
    "            question  = gr.Textbox(\n",
    "                label=\"Ask a question\",\n",
    "                placeholder=\"e.g., What organism is this sample?\",\n",
    "            )\n",
    "            with gr.Row():\n",
    "                btn_org = gr.Button(\"What organism is this sample?\")\n",
    "                btn_pol = gr.Button(\"What is the ionization polarity?\")\n",
    "            with gr.Row():\n",
    "                btn_orgn = gr.Button(\"Which organ is this sample from?\")\n",
    "                btn_cond = gr.Button(\"What is the sample condition?\")\n",
    "            with gr.Row():\n",
    "                btn_an   = gr.Button(\"What analyzer type was used?\")\n",
    "                btn_ions = gr.Button(\"What ionisation source was used?\")\n",
    "\n",
    "            ask_btn   = gr.Button(\"Ask\", variant=\"primary\")\n",
    "            answer    = gr.Markdown(\"\")\n",
    "\n",
    "            with gr.Accordion(\"Model run directory\", open=False):\n",
    "                vqa_run   = gr.Textbox(label=\"VQA run directory (contains best.pt/last.pt + config.json)\", value=\"vqa/20251015_171627\")\n",
    "\n",
    "            state = gr.State(value=None)\n",
    "\n",
    "    # Preview on change\n",
    "    for ctrl in [npz_file, view_mode, ch_index]:\n",
    "        ctrl.change(\n",
    "            fn=_preview,\n",
    "            inputs=[npz_file, view_mode, ch_index],\n",
    "            outputs=img_out\n",
    "        )\n",
    "\n",
    "    # Quick-pick question helpers\n",
    "    def _q_org():   return \"What organism is this sample?\"\n",
    "    def _q_pol():   return \"What is the ionization polarity?\"\n",
    "    def _q_orgn():  return \"Which organ is this sample from?\"\n",
    "    def _q_cond():  return \"What is the sample condition?\"\n",
    "    def _q_an():    return \"What analyzer type was used?\"\n",
    "    def _q_ions():  return \"What ionisation source was used?\"\n",
    "\n",
    "    btn_org.click(fn=_q_org, outputs=question).then(\n",
    "        fn=run_basic,\n",
    "        inputs=[vqa_run, npz_file, view_mode, ch_index, question, state],\n",
    "        outputs=[img_out, answer, state],\n",
    "    )\n",
    "    btn_pol.click(fn=_q_pol, outputs=question).then(\n",
    "        fn=run_basic,\n",
    "        inputs=[vqa_run, npz_file, view_mode, ch_index, question, state],\n",
    "        outputs=[img_out, answer, state],\n",
    "    )\n",
    "    btn_orgn.click(fn=_q_orgn, outputs=question).then(\n",
    "        fn=run_basic,\n",
    "        inputs=[vqa_run, npz_file, view_mode, ch_index, question, state],\n",
    "        outputs=[img_out, answer, state],\n",
    "    )\n",
    "    btn_cond.click(fn=_q_cond, outputs=question).then(\n",
    "        fn=run_basic,\n",
    "        inputs=[vqa_run, npz_file, view_mode, ch_index, question, state],\n",
    "        outputs=[img_out, answer, state],\n",
    "    )\n",
    "    btn_an.click(fn=_q_an, outputs=question).then(\n",
    "        fn=run_basic,\n",
    "        inputs=[vqa_run, npz_file, view_mode, ch_index, question, state],\n",
    "        outputs=[img_out, answer, state],\n",
    "    )\n",
    "    btn_ions.click(fn=_q_ions, outputs=question).then(\n",
    "        fn=run_basic,\n",
    "        inputs=[vqa_run, npz_file, view_mode, ch_index, question, state],\n",
    "        outputs=[img_out, answer, state],\n",
    "    )\n",
    "\n",
    "    # Ask\n",
    "    ask_btn.click(\n",
    "        fn=run_basic,\n",
    "        inputs=[vqa_run, npz_file, view_mode, ch_index, question, state],\n",
    "        outputs=[img_out, answer, state]\n",
    "    )\n",
    "    question.submit(\n",
    "        fn=run_basic,\n",
    "        inputs=[vqa_run, npz_file, view_mode, ch_index, question, state],\n",
    "        outputs=[img_out, answer, state]\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Gradio queue helps keep a single event loop path for uploads/progress on Windows\n",
    "    demo.queue(status_update_rate=1)\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ff2912f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>question</th>\n",
       "      <th>head</th>\n",
       "      <th>gt_normalized</th>\n",
       "      <th>gt_original</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-10-11_19h38m30s</td>\n",
       "      <td>What organism is this sample?</td>\n",
       "      <td>organism</td>\n",
       "      <td>homo sapiens</td>\n",
       "      <td>Homo sapiens (human)</td>\n",
       "      <td>metaspace_images_dump/2021-10-11_19h38m30s/met...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-10-11_19h38m30s</td>\n",
       "      <td>What is the ionization polarity?</td>\n",
       "      <td>polarity</td>\n",
       "      <td>positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>metaspace_images_dump/2021-10-11_19h38m30s/met...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-10-11_19h38m30s</td>\n",
       "      <td>Which organ is this sample from?</td>\n",
       "      <td>organ</td>\n",
       "      <td>lung</td>\n",
       "      <td>Lung</td>\n",
       "      <td>metaspace_images_dump/2021-10-11_19h38m30s/met...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-10-11_19h38m30s</td>\n",
       "      <td>What is the sample condition?</td>\n",
       "      <td>condition</td>\n",
       "      <td>biopsy</td>\n",
       "      <td>biopsy</td>\n",
       "      <td>metaspace_images_dump/2021-10-11_19h38m30s/met...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-10-11_19h38m30s</td>\n",
       "      <td>What analyzer type was used?</td>\n",
       "      <td>analyzerType</td>\n",
       "      <td>fticr</td>\n",
       "      <td>FTICR</td>\n",
       "      <td>metaspace_images_dump/2021-10-11_19h38m30s/met...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-10-11_19h38m30s</td>\n",
       "      <td>What ionisation source was used?</td>\n",
       "      <td>ionisationSource</td>\n",
       "      <td>maldi</td>\n",
       "      <td>MALDI</td>\n",
       "      <td>metaspace_images_dump/2021-10-11_19h38m30s/met...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             dataset_id                          question              head  \\\n",
       "0  2021-10-11_19h38m30s     What organism is this sample?          organism   \n",
       "1  2021-10-11_19h38m30s  What is the ionization polarity?          polarity   \n",
       "2  2021-10-11_19h38m30s  Which organ is this sample from?             organ   \n",
       "3  2021-10-11_19h38m30s     What is the sample condition?         condition   \n",
       "4  2021-10-11_19h38m30s      What analyzer type was used?      analyzerType   \n",
       "5  2021-10-11_19h38m30s  What ionisation source was used?  ionisationSource   \n",
       "\n",
       "  gt_normalized           gt_original  \\\n",
       "0  homo sapiens  Homo sapiens (human)   \n",
       "1      positive              Positive   \n",
       "2          lung                  Lung   \n",
       "3        biopsy                biopsy   \n",
       "4         fticr                 FTICR   \n",
       "5         maldi                 MALDI   \n",
       "\n",
       "                                              source  \n",
       "0  metaspace_images_dump/2021-10-11_19h38m30s/met...  \n",
       "1  metaspace_images_dump/2021-10-11_19h38m30s/met...  \n",
       "2  metaspace_images_dump/2021-10-11_19h38m30s/met...  \n",
       "3  metaspace_images_dump/2021-10-11_19h38m30s/met...  \n",
       "4  metaspace_images_dump/2021-10-11_19h38m30s/met...  \n",
       "5  metaspace_images_dump/2021-10-11_19h38m30s/met...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fm_utils import *\n",
    "from vqa_utils import *\n",
    "\n",
    "# ---------------- Example ----------------\n",
    "SAMPLE_PATH = r\"metaspace_images_dump\\2021-10-11_19h38m30s\\metadata_full.json\"\n",
    "df_gt = gt_answers_for_questions_json(\n",
    "[\n",
    "\"What organism is this sample?\",\n",
    "\"What is the ionization polarity?\",\n",
    "\"Which organ is this sample from?\",\n",
    "\"What is the sample condition?\",\n",
    "\"What analyzer type was used?\",\n",
    "\"What ionisation source was used?\"\n",
    "],\n",
    "sample_path=SAMPLE_PATH\n",
    ")\n",
    "display(df_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ef1ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
