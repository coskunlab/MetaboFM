{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3dab531-6168-4f51-8473-f20ec74859a4",
   "metadata": {},
   "source": [
    "### Gather mouse and human datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84f94711-362e-403a-9bd5-a4020bfe3d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total mouse/human datasets available: 8854\n"
     ]
    }
   ],
   "source": [
    "from metaspace import SMInstance\n",
    "import time\n",
    "\n",
    "TARGET_ORGS = {\"Homo sapiens\", \"Mus musculus\"}\n",
    "SLEEP_BETWEEN   = 0.15 \n",
    "\n",
    "def _from_metadata(ds):\n",
    "    md = getattr(ds, \"metadata\", None) or {}\n",
    "    try:\n",
    "        for key in (\"organism\", \"Organism\", \"sample_organism\", \"Sample organism\"):\n",
    "            if key in md and md[key]:\n",
    "                return str(md[key])\n",
    "        for sec in (\"Sample_Information\", \"Sample Information\", \"sample\"):\n",
    "            if isinstance(md.get(sec), dict):\n",
    "                for key in (\"organism\", \"Organism\"):\n",
    "                    val = md[sec].get(key)\n",
    "                    if val:\n",
    "                        return str(val)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def resolve_organism(ds):\n",
    "    org = getattr(ds, \"organism\", None)\n",
    "    if org: return org\n",
    "    org2 = _from_metadata(ds)\n",
    "    return org2 or \"-\"\n",
    "\n",
    "def is_mouse_or_human_texty(ds):\n",
    "    blob = \" \".join(map(str, [\n",
    "        resolve_organism(ds),\n",
    "        getattr(ds, \"name\", \"\"),\n",
    "        getattr(ds, \"description\", \"\"),\n",
    "        getattr(ds, \"projects\", \"\"),\n",
    "    ])).lower()\n",
    "    return (\"human\" in blob) or (\"homo sapiens\" in blob) or (\"mouse\" in blob) or (\"mus musculus\" in blob)\n",
    "\n",
    "def iter_datasets_mouse_human(sm):\n",
    "    seen = set()\n",
    "    for org in [\"Mus musculus\", \"Homo sapiens\"]:\n",
    "        try:\n",
    "            for ds in sm.datasets(organism=org):\n",
    "                if ds.id not in seen:\n",
    "                    seen.add(ds.id); yield ds\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] iter(organism={org}) failed: {e}\")\n",
    "        time.sleep(SLEEP_BETWEEN)\n",
    "\n",
    "    try:\n",
    "        for ds in sm.datasets():\n",
    "            if ds.id in seen: continue\n",
    "            if resolve_organism(ds) in TARGET_ORGS or is_mouse_or_human_texty(ds):\n",
    "                seen.add(ds.id); yield ds\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] iter(all) failed: {e}\")\n",
    "\n",
    "sm = SMInstance()\n",
    "\n",
    "def count_mouse_human(sm):\n",
    "    seen = set()\n",
    "    count = 0\n",
    "    for ds in iter_datasets_mouse_human(sm):\n",
    "        if ds.id not in seen:\n",
    "            seen.add(ds.id)\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "n_datasets = count_mouse_human(sm)\n",
    "print(f\"Total mouse/human datasets available: {n_datasets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2ff29f-a855-442f-b4bd-f6aac3f07c1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# metaspace_ion_images_dump_fm.py\n",
    "# ------------------------------------------------------------\n",
    "# Fetch mouse/human datasets from METASPACE and save ion (isotope) images\n",
    "# as arrays (uint16 .npy / .npz) plus an FM-ready manifest (per tile).\n",
    "#\n",
    "# Upgrades vs. original:\n",
    "#  - Deterministic tiling with blank-tile filtering\n",
    "#  - Optional float32 tile saves alongside uint16\n",
    "#  - Per-tile manifest rows with QC stats and a stable train/val/test split\n",
    "#  - Keeps per-annotation CSV + dataset metadata\n",
    "#\n",
    "# Notes:\n",
    "#  - For a true FM, also build a parallel imzML->cube pipeline. This script\n",
    "#    is ideal for weakly-labeled 2D supervision at scale.\n",
    "\n",
    "from metaspace import SMInstance\n",
    "from pathlib import Path\n",
    "import os, json, time, traceback, hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "OUT_ROOT        = Path(\"metaspace_images_dump\")\n",
    "DB              = (\"HMDB\", \"v4\")\n",
    "FDR_MAX         = 0.10\n",
    "TOP_K_ANN       = 48          # cap per-dataset so a few huge sets don’t dominate\n",
    "MAX_ISOTOPES    = 3           # M, M+1, M+2 are usually enough\n",
    "MAX_DATASETS    = 5000\n",
    "SAVE_FORMAT     = \"npz\"       # compressed; cuts storage 30–70% vs .npy\n",
    "SLEEP_BETWEEN   = 0.15        # be polite; lower if you parallelize clients\n",
    "\n",
    "# --- Tiling & QC ---\n",
    "DO_TILING       = True\n",
    "TILE_SIZE       = 256         # great balance of context vs count\n",
    "TILE_STRIDE     = 256         # start with no-overlap to keep volume sane\n",
    "MIN_NNZ_PCT     = 3.0         # % of pixels > 0 required in a tile (see note below)\n",
    "SAVE_FLOAT_TILES= False       # save only uint16; re-materialize float on load\n",
    "USE_WHOLE_ROWS  = False       # train on tiles; whole images for QA only\n",
    "\n",
    "# --- Splits (grouped by dataset to avoid leakage) ---\n",
    "TRAIN_FRAC      = 0.85\n",
    "VAL_FRAC        = 0.10        # test ends up ~0.05\n",
    "\n",
    "# ----------------------------------------\n",
    "\n",
    "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ------------- AUTH / LOGIN (optional) -------------\n",
    "sm = SMInstance()\n",
    "API_KEY = os.getenv(\"METASPACE_API_KEY\", \"\").strip()\n",
    "if API_KEY and hasattr(sm, \"login\"):\n",
    "    try:\n",
    "        sm.login(API_KEY)\n",
    "        print(\"[AUTH] Logged in via API key\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] sm.login failed: {e}  (you can run sm.save_login() once)\")\n",
    "else:\n",
    "    print(\"[AUTH] Proceeding with current access (public or saved login).\")\n",
    "\n",
    "# --------- DATASET DISCOVERY ----------\n",
    "TARGET_ORGS = {\"Homo sapiens\", \"Mus musculus\"}\n",
    "\n",
    "def _from_metadata(ds):\n",
    "    md = getattr(ds, \"metadata\", None) or {}\n",
    "    try:\n",
    "        for key in (\"organism\", \"Organism\", \"sample_organism\", \"Sample organism\"):\n",
    "            if key in md and md[key]:\n",
    "                return str(md[key])\n",
    "        for sec in (\"Sample_Information\", \"Sample Information\", \"sample\"):\n",
    "            if isinstance(md.get(sec), dict):\n",
    "                for key in (\"organism\", \"Organism\"):\n",
    "                    val = md[sec].get(key)\n",
    "                    if val:\n",
    "                        return str(val)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def resolve_organism(ds):\n",
    "    org = getattr(ds, \"organism\", None)\n",
    "    if org: return org\n",
    "    org2 = _from_metadata(ds)\n",
    "    return org2 or \"-\"\n",
    "\n",
    "def is_mouse_or_human_texty(ds):\n",
    "    blob = \" \".join(map(str, [\n",
    "        resolve_organism(ds),\n",
    "        getattr(ds, \"name\", \"\"),\n",
    "        getattr(ds, \"description\", \"\"),\n",
    "        getattr(ds, \"projects\", \"\"),\n",
    "    ])).lower()\n",
    "    return (\"human\" in blob) or (\"homo sapiens\" in blob) or (\"mouse\" in blob) or (\"mus musculus\" in blob)\n",
    "\n",
    "def iter_datasets_mouse_human(sm):\n",
    "    seen = set()\n",
    "    for org in [\"Mus musculus\", \"Homo sapiens\"]:\n",
    "        try:\n",
    "            for ds in sm.datasets(organism=org):\n",
    "                if ds.id not in seen:\n",
    "                    seen.add(ds.id); yield ds\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] iter(organism={org}) failed: {e}\")\n",
    "        time.sleep(SLEEP_BETWEEN)\n",
    "\n",
    "    try:\n",
    "        for ds in sm.datasets():\n",
    "            if ds.id in seen: continue\n",
    "            if resolve_organism(ds) in TARGET_ORGS or is_mouse_or_human_texty(ds):\n",
    "                seen.add(ds.id); yield ds\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] iter(all) failed: {e}\")\n",
    "\n",
    "# --------- HELPERS ----------\n",
    "def safe_results(ds, db):\n",
    "    \"\"\"Return results DataFrame with MultiIndex (formula, adduct) if possible.\"\"\"\n",
    "    try:\n",
    "        res = ds.results(database=db)\n",
    "        if res is None or len(res) == 0: return None\n",
    "        # ensure index is (formula, adduct) for easy use\n",
    "        if not isinstance(res.index, pd.MultiIndex):\n",
    "            cols = set(res.columns)\n",
    "            if {\"formula\", \"adduct\"}.issubset(cols):\n",
    "                res.index = pd.MultiIndex.from_frame(res[[\"formula\",\"adduct\"]])\n",
    "        return res\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] results() failed for {getattr(ds, 'id', '?')}: {e}\")\n",
    "        return None\n",
    "\n",
    "def to_uint16_robust(img: np.ndarray):\n",
    "    \"\"\"Percentile-based normalization -> uint16 for compact storage.\n",
    "       Returns (uint16_img, (lo, hi)) where lo/hi are float percentiles (1,99).\"\"\"\n",
    "    img = np.asarray(img, dtype=np.float32)\n",
    "    if img.size == 0:\n",
    "        return np.zeros_like(img, dtype=np.uint16), (0.0, 1.0)\n",
    "    lo, hi = np.percentile(img, [1, 99])\n",
    "    if not np.isfinite(lo) or not np.isfinite(hi) or hi <= lo:\n",
    "        lo = float(np.min(img))\n",
    "        hi = float(np.max(img))\n",
    "        if hi <= lo:\n",
    "            hi = lo + 1e-6\n",
    "    img_n = np.clip((img - lo) / (hi - lo), 0, 1)\n",
    "    return (img_n * 65535).astype(np.uint16), (float(lo), float(hi))\n",
    "\n",
    "def save_array(arr_uint16: np.ndarray, out_path: Path, meta: dict, fmt: str = \"npy\"):\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if fmt == \"npy\":\n",
    "        np.save(out_path, arr_uint16)\n",
    "        (out_path.with_suffix(\".json\")).write_text(json.dumps(meta, indent=2))\n",
    "    elif fmt == \"npz\":\n",
    "        # store meta inside the archive as JSON bytes\n",
    "        np.savez_compressed(out_path, image=arr_uint16, meta=json.dumps(meta).encode(\"utf-8\"))\n",
    "    else:\n",
    "        raise ValueError(\"SAVE_FORMAT must be 'npy' or 'npz'\")\n",
    "\n",
    "def stable_split(dataset_id: str, p_train=TRAIN_FRAC, p_val=VAL_FRAC):\n",
    "    \"\"\"Deterministic split by hashing dataset_id.\"\"\"\n",
    "    h = int(hashlib.md5(dataset_id.encode(\"utf-8\")).hexdigest(), 16) % 10_000\n",
    "    x = h / 10_000.0\n",
    "    return \"train\" if x < p_train else (\"val\" if x < p_train + p_val else \"test\")\n",
    "\n",
    "def tile_and_save(img_uint16: np.ndarray,\n",
    "                  img_float32: np.ndarray,\n",
    "                  base_out: Path,\n",
    "                  tile=256,\n",
    "                  stride=256,\n",
    "                  min_nnz_pct=0.5,\n",
    "                  save_float=True):\n",
    "    \"\"\"Tile uint16 and (optionally) float32 images; filter blank tiles; return tile records.\"\"\"\n",
    "    H, W = img_uint16.shape[:2]\n",
    "    tiles = []\n",
    "    base_out.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for r in range(0, max(1, H - tile + 1), stride):\n",
    "        for c in range(0, max(1, W - tile + 1), stride):\n",
    "            patch_u16 = img_uint16[r:r+tile, c:c+tile]\n",
    "            if patch_u16.shape[:2] != (tile, tile):\n",
    "                continue\n",
    "\n",
    "            nnz = np.count_nonzero(patch_u16)\n",
    "            nnz_pct = 100.0 * (nnz / float(tile * tile))\n",
    "            if nnz_pct < min_nnz_pct:\n",
    "                continue\n",
    "\n",
    "            p_stem = f\"{base_out.stem}_r{r:04d}_c{c:04d}\"\n",
    "            p_u16 = base_out.parent / f\"{p_stem}.npy\"\n",
    "            np.save(p_u16, patch_u16)\n",
    "\n",
    "            p_f32 = None\n",
    "            if save_float and img_float32 is not None:\n",
    "                patch_f32 = img_float32[r:r+tile, c:c+tile].astype(np.float32, copy=False)\n",
    "                p_f32 = base_out.parent / f\"{p_stem}.float32.npy\"\n",
    "                np.save(p_f32, patch_f32)\n",
    "\n",
    "            tiles.append({\n",
    "                \"tile_r\": r, \"tile_c\": c, \"tile_h\": tile, \"tile_w\": tile,\n",
    "                \"nnz_pct\": nnz_pct, \"path_u16\": str(p_u16),\n",
    "                \"path_f32\": (str(p_f32) if p_f32 else None),\n",
    "            })\n",
    "    return tiles\n",
    "\n",
    "import json, math\n",
    "import numpy as np\n",
    "\n",
    "def _jsonify(obj):\n",
    "    # Make numpy / sets / NaNs JSON-friendly\n",
    "    if isinstance(obj, (np.integer,)):  return int(obj)\n",
    "    if isinstance(obj, (np.floating,)): return float(obj)\n",
    "    if isinstance(obj, (np.ndarray,)):  return obj.tolist()\n",
    "    if isinstance(obj, set):            return list(obj)\n",
    "    if isinstance(obj, dict):           return {str(k): _jsonify(v) for k,v in obj.items()}\n",
    "    if isinstance(obj, (list, tuple)):  return [_jsonify(v) for v in obj]\n",
    "    if isinstance(obj, float) and (math.isnan(obj) or math.isinf(obj)): return None\n",
    "    return obj\n",
    "\n",
    "def collect_full_metadata(ds, db=(\"HMDB\",\"v4\")):\n",
    "    \"\"\"Best-effort snapshot of everything we can reasonably serialize.\"\"\"\n",
    "    base = {\n",
    "        # common top-level attributes (safe getattr)\n",
    "        \"dataset_id\":      getattr(ds, \"id\", None),\n",
    "        \"name\":            getattr(ds, \"name\", None),\n",
    "        \"description\":     getattr(ds, \"description\", None),\n",
    "        \"organism\":        getattr(ds, \"organism\", None),\n",
    "        \"polarity\":        getattr(ds, \"polarity\", None),\n",
    "        \"analyzerType\":    getattr(ds, \"analyzerType\", None),\n",
    "        \"ionisationSource\":getattr(ds, \"ionisationSource\", None),\n",
    "        \"maldiMatrix\":     getattr(ds, \"maldiMatrix\", None),\n",
    "        \"submitter\":       getattr(ds, \"submitter\", None),   # may be dict-like or str depending on client\n",
    "        \"principalInvestigator\": getattr(ds, \"principalInvestigator\", None),\n",
    "        \"group\":           getattr(ds, \"group\", None),\n",
    "        \"projects\":        getattr(ds, \"projects\", None),\n",
    "        \"license\":         getattr(ds, \"license\", None),\n",
    "        \"status\":          getattr(ds, \"status\", None),\n",
    "        \"uploaded_dt\":     getattr(ds, \"uploadDT\", None) or getattr(ds, \"uploadedDT\", None),\n",
    "        \"db_used\":         db,\n",
    "    }\n",
    "\n",
    "    # user-provided metadata JSON attached to the dataset (often rich)\n",
    "    try:\n",
    "        base[\"metadata_uploaded\"] = _jsonify(getattr(ds, \"metadata\", {}) or {})\n",
    "    except Exception:\n",
    "        base[\"metadata_uploaded\"] = {}\n",
    "\n",
    "    # raw GraphQL record (contains fields not exposed as attrs). This is a private field\n",
    "    # in many client versions, so wrap carefully.\n",
    "    try:\n",
    "        gql = getattr(ds, \"_ds\", None)   # WARNING: private; may change across versions\n",
    "        if isinstance(gql, dict):\n",
    "            base[\"graphql_record\"] = _jsonify(gql)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # diagnostics (header info, quick stats), best-effort\n",
    "    diags = {}\n",
    "    for key in [\"IMZML_METADATA\", \"DATASET_SUMMARY\", \"ANNOTATION_COUNTS\", \"OFF_SAMPLE_MASK\"]:\n",
    "        try:\n",
    "            d = ds.diagnostic(key)\n",
    "            if d is not None:\n",
    "                diags[key] = _jsonify(d)\n",
    "        except Exception:\n",
    "            continue\n",
    "    if diags:\n",
    "        base[\"diagnostics\"] = diags\n",
    "\n",
    "    # results summary: how many annotations pass FDR thresholds, etc.\n",
    "    try:\n",
    "        res = ds.results(database=db)\n",
    "        if res is not None and \"fdr\" in res.columns:\n",
    "            counts = {}\n",
    "            for thr in [0.01, 0.05, 0.1, 0.2, 0.5]:\n",
    "                counts[f\"n_fdr_le_{thr}\"] = int((res[\"fdr\"] <= thr).sum())\n",
    "            base[\"results_summary\"] = counts\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # spatial size hint (from one isotope image)\n",
    "    try:\n",
    "        imgs = ds.isotope_images(\"C6H12O6\", \"+Na\")  # harmless probe; likely missing\n",
    "        if imgs and len(imgs) > 0:\n",
    "            sh = np.asarray(imgs[0]).shape\n",
    "            base[\"spatial_shape_probe\"] = list(sh)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return base\n",
    "\n",
    "def save_full_metadata(ds, out_dir: Path, db=(\"HMDB\",\"v4\")):\n",
    "    info = collect_full_metadata(ds, db=db)\n",
    "    (out_dir / \"metadata_full.json\").write_text(json.dumps(_jsonify(info), indent=2))\n",
    "\n",
    "# --------- CORE: per-dataset processing ----------\n",
    "def process_one_dataset(ds) -> list:\n",
    "    \"\"\"Returns a list of manifest rows (per tile or per whole image) for this dataset.\"\"\"\n",
    "    rows = []\n",
    "    ds_id = getattr(ds, \"id\", None)\n",
    "    if not ds_id: return rows\n",
    "\n",
    "    ds_dir = OUT_ROOT / ds_id\n",
    "\n",
    "    if ds_dir.exists():\n",
    "        print(f\"[SKIP] {ds_id}: folder exists\")\n",
    "        return []\n",
    "    \n",
    "    ds_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    save_full_metadata(ds, ds_dir, db=DB)\n",
    "\n",
    "    org = resolve_organism(ds)\n",
    "    nm  = getattr(ds, \"name\", None)\n",
    "    split = stable_split(ds_id)\n",
    "\n",
    "    print(f\"[INFO] Processing {ds_id} | org={org} | name={nm or '-'} | split={split}\")\n",
    "\n",
    "    # Save basic metadata (dataset-level)\n",
    "    meta = {\n",
    "        \"dataset_id\": ds_id, \"name\": nm, \"organism\": org,\n",
    "        \"polarity\": getattr(ds, \"polarity\", None),\n",
    "        \"analyzerType\": getattr(ds, \"analyzerType\", None),\n",
    "        \"ionisationSource\": getattr(ds, \"ionisationSource\", None),\n",
    "        \"db\": DB, \"fdr_max\": FDR_MAX, \"top_k_ann\": TOP_K_ANN,\n",
    "        \"max_isotopes\": MAX_ISOTOPES,\n",
    "        \"save_format\": SAVE_FORMAT, \"tiling\": DO_TILING,\n",
    "        \"tile_size\": TILE_SIZE, \"tile_stride\": TILE_STRIDE,\n",
    "    }\n",
    "    (ds_dir / \"metadata.json\").write_text(json.dumps(meta, indent=2))\n",
    "\n",
    "    # Get annotations\n",
    "    res = safe_results(ds, DB)\n",
    "    if res is None or \"fdr\" not in res.columns:\n",
    "        print(f\"[SKIP] {ds_id}: no usable results/FDR\")\n",
    "        return rows\n",
    "\n",
    "    keep = res[res[\"fdr\"] <= FDR_MAX].copy()\n",
    "    if len(keep) == 0:\n",
    "        print(f\"[SKIP] {ds_id}: 0 annotations pass FDR ≤ {FDR_MAX}\")\n",
    "        return rows\n",
    "\n",
    "    # Sort and cap\n",
    "    if TOP_K_ANN is not None and len(keep) > TOP_K_ANN:\n",
    "        if \"msm\" in keep.columns:\n",
    "            keep = keep.sort_values(\"msm\", ascending=False).head(TOP_K_ANN)\n",
    "        else:\n",
    "            keep = keep.sort_values(\"fdr\", ascending=True).head(TOP_K_ANN)\n",
    "\n",
    "    # Save a copy of kept annotations (useful later)\n",
    "    keep.to_csv(ds_dir / \"annotations_kept.csv\")\n",
    "\n",
    "    # Iterate annotations\n",
    "    for (sf, adduct) in tqdm(list(keep.index), desc=f\"{ds_id}: pulling images\", leave=False):\n",
    "        ann_fdr = float(keep.loc[(sf, adduct)].get(\"fdr\", np.nan))\n",
    "        ann_msm = float(keep.loc[(sf, adduct)].get(\"msm\", np.nan)) if \"msm\" in keep.columns else np.nan\n",
    "\n",
    "        try:\n",
    "            images = ds.isotope_images(sf, adduct)  # list-like of 2D arrays\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] {ds_id} {sf} {adduct}: isotope_images failed -> {e}\")\n",
    "            continue\n",
    "\n",
    "        n = min(len(images), MAX_ISOTOPES)\n",
    "        ann_dir = ds_dir / \"images\" / f\"{sf}_{adduct}\".replace(\"/\", \"_\")\n",
    "\n",
    "        for j in range(n):\n",
    "            try:\n",
    "                im = np.asarray(images[j], dtype=np.float32)\n",
    "                arr16, (lo, hi) = to_uint16_robust(im)\n",
    "                try:\n",
    "                    peak_mz = float(images.peak(index=j))\n",
    "                except Exception:\n",
    "                    peak_mz = float(\"nan\")\n",
    "\n",
    "                # Whole-image save (uint16)\n",
    "                base = ann_dir / f\"peak{j}\"\n",
    "                out_path = base.with_suffix(\".npy\") if SAVE_FORMAT == \"npy\" else base.with_suffix(\".npz\")\n",
    "                img_meta = {\n",
    "                    \"dataset_id\": ds_id,\n",
    "                    \"name\": nm,\n",
    "                    \"organism\": org,\n",
    "                    \"split\": split,\n",
    "                    \"db\": DB,\n",
    "                    \"fdr\": ann_fdr,\n",
    "                    \"msm\": ann_msm,\n",
    "                    \"sum_formula\": sf,\n",
    "                    \"adduct\": adduct,\n",
    "                    \"isotope_index\": j,\n",
    "                    \"peak_mz\": peak_mz,\n",
    "                    \"shape\": list(arr16.shape),\n",
    "                    \"dtype\": \"uint16\",\n",
    "                    \"norm_lo\": lo,\n",
    "                    \"norm_hi\": hi,\n",
    "                    \"polarity\": getattr(ds, \"polarity\", None),\n",
    "                    \"analyzerType\": getattr(ds, \"analyzerType\", None),\n",
    "                    \"ionisationSource\": getattr(ds, \"ionisationSource\", None),\n",
    "                }\n",
    "\n",
    "                save_array(arr16, out_path, img_meta, fmt=SAVE_FORMAT)\n",
    "\n",
    "                # QC for whole image (optional fields)\n",
    "                whole_mean = float(np.mean(arr16)) if arr16.size else 0.0\n",
    "                whole_std  = float(np.std(arr16)) if arr16.size else 0.0\n",
    "                nnz_pct_whole = 100.0 * (np.count_nonzero(arr16) / float(arr16.size)) if arr16.size else 0.0\n",
    "\n",
    "                # Tiling for FM\n",
    "                tile_count = 0\n",
    "                if DO_TILING:\n",
    "                    tiles = tile_and_save(\n",
    "                        arr16, im, out_path,\n",
    "                        tile=TILE_SIZE,\n",
    "                        stride=TILE_STRIDE,\n",
    "                        min_nnz_pct=MIN_NNZ_PCT,\n",
    "                        save_float=SAVE_FLOAT_TILES\n",
    "                    )\n",
    "                    for t in tiles:\n",
    "                        rows.append({\n",
    "                            **img_meta,\n",
    "                            \"path\": t[\"path_u16\"],\n",
    "                            \"path_float32\": t[\"path_f32\"],\n",
    "                            \"tile_r\": t[\"tile_r\"], \"tile_c\": t[\"tile_c\"],\n",
    "                            \"tile_h\": t[\"tile_h\"], \"tile_w\": t[\"tile_w\"],\n",
    "                            \"nnz_pct\": t[\"nnz_pct\"],\n",
    "                            \"whole_mean_u16\": whole_mean,\n",
    "                            \"whole_std_u16\": whole_std,\n",
    "                            \"whole_nnz_pct\": nnz_pct_whole,\n",
    "                            \"n_tiles\": 1\n",
    "                        })\n",
    "                    tile_count = len(tiles)\n",
    "\n",
    "                # Optionally also include a row for the whole image\n",
    "                if USE_WHOLE_ROWS or (not DO_TILING or tile_count == 0):\n",
    "                    rows.append({\n",
    "                        **img_meta,\n",
    "                        \"path\": str(out_path),\n",
    "                        \"path_float32\": None,\n",
    "                        \"tile_r\": None, \"tile_c\": None,\n",
    "                        \"tile_h\": arr16.shape[0], \"tile_w\": arr16.shape[1],\n",
    "                        \"nnz_pct\": nnz_pct_whole,\n",
    "                        \"whole_mean_u16\": whole_mean,\n",
    "                        \"whole_std_u16\": whole_std,\n",
    "                        \"whole_nnz_pct\": nnz_pct_whole,\n",
    "                        \"n_tiles\": 0\n",
    "                    })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] {ds_id} {sf} {adduct} peak{j}: save failed -> {e}\")\n",
    "                continue\n",
    "\n",
    "    print(f\"[DONE] {ds_id}: wrote {sum(1 for _ in rows if _['dataset_id']==ds_id)} manifest rows\")\n",
    "    return rows\n",
    "\n",
    "# ---------------- MAIN -----------------\n",
    "def main():\n",
    "    manifest_rows = []\n",
    "    seen, count = set(), 0\n",
    "\n",
    "    for ds in iter_datasets_mouse_human(sm):\n",
    "        dsid = getattr(ds, \"id\", None)\n",
    "        if not dsid or dsid in seen:\n",
    "            continue\n",
    "        seen.add(dsid)\n",
    "\n",
    "        count += 1\n",
    "        if MAX_DATASETS is not None and count > MAX_DATASETS:\n",
    "            print(f\"[STOP] Reached MAX_DATASETS={MAX_DATASETS}\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            rows = process_one_dataset(ds)\n",
    "            manifest_rows.extend(rows)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] {dsid}: processing crashed -> {e}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "    if manifest_rows:\n",
    "        man_df = pd.DataFrame(manifest_rows)\n",
    "\n",
    "        # Reorder columns for readability\n",
    "        preferred = [\n",
    "            \"dataset_id\",\"name\",\"organism\",\"split\",\n",
    "            \"db\",\"fdr\",\"msm\",\"sum_formula\",\"adduct\",\"isotope_index\",\"peak_mz\",\n",
    "            \"path\",\"path_float32\",\n",
    "            \"tile_r\",\"tile_c\",\"tile_h\",\"tile_w\",\"nnz_pct\",\n",
    "            \"whole_mean_u16\",\"whole_std_u16\",\"whole_nnz_pct\",\n",
    "            \"polarity\",\"analyzerType\",\"ionisationSource\",\n",
    "            \"dtype\",\"shape\",\"norm_lo\",\"norm_hi\",\"n_tiles\"\n",
    "        ]\n",
    "        cols = [c for c in preferred if c in man_df.columns] + [c for c in man_df.columns if c not in preferred]\n",
    "        man_df = man_df[cols]\n",
    "\n",
    "        OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "        man_df.to_parquet(OUT_ROOT / \"manifest.parquet\", index=False)\n",
    "        man_df.to_csv(OUT_ROOT / \"manifest.csv\", index=False)\n",
    "        n_ds = man_df['dataset_id'].nunique()\n",
    "        n_tiles = (man_df['n_tiles'] == 1).sum()\n",
    "        print(f\"\\n[SUMMARY] Wrote {len(man_df)} manifest rows \"\n",
    "              f\"({n_tiles} tiles) across {n_ds} datasets\")\n",
    "    else:\n",
    "        print(\"\\n[SUMMARY] No images saved\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75e5427",
   "metadata": {},
   "source": [
    "### Expand manifest with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf295cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] manifest loaded with 384403 rows, 3774 datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading metadata: 100%|██████████| 3774/3774 [01:58<00:00, 31.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Enriched manifest saved → Y:\\coskun-lab\\Efe\\MSI Foundation Model\\metaspace_images_dump\\manifest_expanded.parquet\n",
      "Columns added (new): ['Organism_Part', 'Condition']\n",
      "Rows: 384403 | Datasets: 3774\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>Organism_Part</th>\n",
       "      <th>Condition</th>\n",
       "      <th>analyzerType</th>\n",
       "      <th>ionisationSource</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-08-27_01h37m31s</td>\n",
       "      <td>Kidney</td>\n",
       "      <td>biopsy</td>\n",
       "      <td>timsTOF fleX</td>\n",
       "      <td>MALDI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-08-27_01h37m31s</td>\n",
       "      <td>Kidney</td>\n",
       "      <td>biopsy</td>\n",
       "      <td>timsTOF fleX</td>\n",
       "      <td>MALDI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-08-27_01h37m31s</td>\n",
       "      <td>Kidney</td>\n",
       "      <td>biopsy</td>\n",
       "      <td>timsTOF fleX</td>\n",
       "      <td>MALDI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             dataset_id Organism_Part Condition  analyzerType ionisationSource\n",
       "0  2025-08-27_01h37m31s        Kidney    biopsy  timsTOF fleX            MALDI\n",
       "1  2025-08-27_01h37m31s        Kidney    biopsy  timsTOF fleX            MALDI\n",
       "2  2025-08-27_01h37m31s        Kidney    biopsy  timsTOF fleX            MALDI"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Enrich manifest using metadata_full.json files (adds/updates analyzerType & ionisationSource) ---\n",
    "import os, re, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "MANIFEST_PATH = Path(r\"Y:\\coskun-lab\\Efe\\MSI Foundation Model\\metaspace_images_dump\\manifest.parquet\")\n",
    "SIDECAR_ROOT = MANIFEST_PATH.parent   # base folder that contains <dataset_id>/metadata_full.json\n",
    "\n",
    "# For Organism_Part / Condition we read from Sample_Information;\n",
    "# For analyzerType / ionisationSource we read from MS_Analysis (robust to key variants).\n",
    "FIELD_MAP = {\n",
    "    \"Organism_Part\":    (\"metadata_uploaded\", \"Sample_Information\", \"Organism_Part\"),\n",
    "    \"Condition\":        (\"metadata_uploaded\", \"Sample_Information\", \"Condition\"),\n",
    "    \"analyzerType\":     (\"metadata_uploaded\", \"MS_Analysis\", (\"Analyzer\",\"Analyser\",\"Analyzer Type\",\"Analyzer_Type\",\"analyzerType\")),\n",
    "    \"ionisationSource\": (\"metadata_uploaded\", \"MS_Analysis\", (\"Ionisation_Source\",\"Ionization_Source\",\"Ion Source\",\"Ion_Source\",\"ionisationSource\",\"ionization source\")),\n",
    "}\n",
    "# ----------------------------------------\n",
    "\n",
    "# --- Robust, case/format-insensitive nested getter ---\n",
    "def _norm_key(s: str) -> str:\n",
    "    s = s.lower()\n",
    "    s = s.replace(\"ionization\", \"ionisation\")  # US→UK normalization\n",
    "    s = s.replace(\"analyser\", \"analyzer\")      # UK→US normalization\n",
    "    return re.sub(r\"[^a-z0-9]+\", \"\", s)        # strip spaces/_/-\n",
    "\n",
    "def _ci_get(d: dict, candidates, default=np.nan):\n",
    "    \"\"\"Case/format-insensitive key fetch from dict using list/tuple of candidate keys.\"\"\"\n",
    "    if not isinstance(d, dict) or not d:\n",
    "        return default\n",
    "    lut = {_norm_key(k): v for k, v in d.items()}\n",
    "    for c in (candidates if isinstance(candidates, (list, tuple)) else [candidates]):\n",
    "        v = lut.get(_norm_key(str(c)))\n",
    "        if v is not None:\n",
    "            return v\n",
    "    return default\n",
    "\n",
    "def _get_nested_ci(d, keys, default=np.nan):\n",
    "    \"\"\"\n",
    "    Keys can be strings or tuples/lists of variants.\n",
    "    Also tolerant to section-name variants:\n",
    "      - 'metadata_uploaded' / 'metadata uploaded'\n",
    "      - 'Sample_Information' / 'Sample Information' / 'Sample'\n",
    "      - 'MS_Analysis' / 'MS Analysis' / 'MS'\n",
    "    \"\"\"\n",
    "    cur = d\n",
    "    section_variants = {\n",
    "        \"metadata_uploaded\": (\"metadata_uploaded\", \"metadata uploaded\"),\n",
    "        \"Sample_Information\": (\"Sample_Information\", \"Sample Information\", \"Sample\"),\n",
    "        \"MS_Analysis\": (\"MS_Analysis\", \"MS Analysis\", \"MS\"),\n",
    "    }\n",
    "    for k in keys:\n",
    "        if not isinstance(cur, dict):\n",
    "            return default\n",
    "        if isinstance(k, (list, tuple)):\n",
    "            cur = _ci_get(cur, list(k), default)\n",
    "        else:\n",
    "            cand = section_variants.get(k, (k,))\n",
    "            cur = _ci_get(cur, cand, default)\n",
    "        if cur is default:\n",
    "            return default\n",
    "    return cur\n",
    "\n",
    "def load_sidecar_metadata(dataset_id: str) -> dict:\n",
    "    \"\"\"Load metadata_full.json for a given dataset_id, return dict or {}.\"\"\"\n",
    "    json_path = SIDECAR_ROOT / dataset_id / \"metadata_full.json\"\n",
    "    if json_path.exists():\n",
    "        try:\n",
    "            with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                return json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] Failed to read {json_path}: {e}\")\n",
    "    return {}\n",
    "\n",
    "# ---- Load manifest ----\n",
    "if MANIFEST_PATH.suffix == \".parquet\":\n",
    "    df = pd.read_parquet(MANIFEST_PATH, engine=\"pyarrow\")\n",
    "elif MANIFEST_PATH.suffix == \".csv\":\n",
    "    df = pd.read_csv(MANIFEST_PATH)\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported manifest format: {MANIFEST_PATH.suffix}\")\n",
    "\n",
    "print(f\"[info] manifest loaded with {len(df)} rows, {df['dataset_id'].nunique()} datasets\")\n",
    "\n",
    "# Track which columns already existed to avoid suffix collisions\n",
    "pre_cols = set(df.columns)\n",
    "\n",
    "# ---- Enrich per-dataset ----\n",
    "meta_records = {}\n",
    "for dsid in tqdm(df[\"dataset_id\"].unique(), desc=\"Loading metadata\"):\n",
    "    meta = load_sidecar_metadata(str(dsid))\n",
    "    if not meta:\n",
    "        continue\n",
    "    rec = {}\n",
    "    for out_col, path in FIELD_MAP.items():\n",
    "        rec[out_col] = _get_nested_ci(meta, path, default=np.nan)\n",
    "    meta_records[dsid] = rec\n",
    "\n",
    "# Nothing to merge?\n",
    "if not meta_records:\n",
    "    print(\"[warn] No sidecar metadata found; nothing to enrich.\")\n",
    "else:\n",
    "    meta_df = pd.DataFrame.from_dict(meta_records, orient=\"index\").reset_index().rename(columns={\"index\":\"dataset_id\"})\n",
    "\n",
    "    # Merge with explicit suffix so we can safely fill existing cols (e.g., analyzerType, ionisationSource)\n",
    "    df = df.merge(meta_df, on=\"dataset_id\", how=\"left\", suffixes=(\"\", \"_sidecar\"))\n",
    "\n",
    "    # For any field that already exists in manifest, fill missing values from sidecar and drop helper column\n",
    "    for col in FIELD_MAP.keys():\n",
    "        sidecar_col = f\"{col}_sidecar\"\n",
    "        if sidecar_col in df.columns:\n",
    "            if col in pre_cols:\n",
    "                df[col] = df[col].where(~df[col].isna(), df[sidecar_col])\n",
    "                df.drop(columns=[sidecar_col], inplace=True)\n",
    "            else:\n",
    "                # Column did not exist originally; rename sidecar → main\n",
    "                df.rename(columns={sidecar_col: col}, inplace=True)\n",
    "\n",
    "# ---- Save enriched manifest ----\n",
    "OUT_PATH = MANIFEST_PATH.with_name(MANIFEST_PATH.stem + \"_expanded\").with_suffix(\".parquet\")\n",
    "df.to_parquet(OUT_PATH, index=False, engine=\"pyarrow\")\n",
    "\n",
    "# Report what changed/added\n",
    "added_now = [c for c in FIELD_MAP.keys() if c in df.columns and c not in pre_cols]\n",
    "print(f\"[OK] Enriched manifest saved → {OUT_PATH}\")\n",
    "print(\"Columns added (new):\", added_now)\n",
    "print(\"Rows:\", len(df), \"| Datasets:\", df[\"dataset_id\"].nunique())\n",
    "\n",
    "# Quick peek\n",
    "cols_to_show = [\"dataset_id\"] + [c for c in FIELD_MAP.keys() if c in df.columns]\n",
    "display_cols = [c for c in cols_to_show if c in df.columns]\n",
    "df.head(3)[display_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12873fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Assembling samples: 100%|██████████| 4471/4471 [29:32<00:00,  2.52it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Wrote 3938 samples to metaspace_images_dump\\msi_fm_samples3 and list to metaspace_images_dump\\msi_fm_samples3.parquet\n",
      "---- Diagnostics ----\n",
      "Rows in manifest: 384403\n",
      "Datasets: 3774\n",
      "Rows with complete tile coords: 31348\n",
      "Tiled groups: 763\n",
      "Untiled (dataset) groups: 3708\n",
      "Skip reasons: {'too_few_channels': 533}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# assemble_kronos_samples_npz_aware.py\n",
    "# Build (patch, mz) .npz samples for KRONOS/MSI-FM pretrain from your tiling manifest.\n",
    "# - Single process, no pin memory / multiprocessing\n",
    "# - Supports .npy and .npz: picks first 2D array or squeezes (1,H,W)->(H,W)\n",
    "# - Ignores `n_tiles`; uses tile coords if present; optional dataset-level fallback\n",
    "# - Vectorized ranking (fdr asc, msm desc), mmap reads, pre-allocation\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "# ====================== CONFIG ======================\n",
    "DUMP_ROOT   = Path(\"metaspace_images_dump\")\n",
    "SRC_MANIFEST= DUMP_ROOT / \"manifest_expanded.parquet\"    # or .csv\n",
    "OUT_SAMPLES = DUMP_ROOT / \"msi_fm_samples\"\n",
    "OUT_LIST    = DUMP_ROOT / \"msi_fm_samples.parquet\"\n",
    "\n",
    "C_TARGET = 64                     # max channels per sample\n",
    "MIN_CHANNELS_PER_SAMPLE = 8       # skip groups with fewer channels\n",
    "N_SAMPLES_PER_DATASET_CAP = None  # e.g. 200; None disables\n",
    "\n",
    "USE_UINT16_INPUT = True           # cast channels to uint16 for compact storage\n",
    "USE_COMPRESSION  = False          # np.savez (False) is faster; use True for smaller files\n",
    "WRITE_CSV_LIST   = True           # also write CSV index\n",
    "\n",
    "# Rows without tile coords:\n",
    "PROCESS_UNTILED = True            # try dataset-level fallback groups\n",
    "UNTILED_PATH_PATTERN = None       # e.g. \"peak\" to include only certain files in fallback\n",
    "UNTILED_ACCEPT_3D_SQUEEZE = True  # accept (1,H,W)->(H,W) by squeeze\n",
    "# ====================================================\n",
    "\n",
    "OUT_SAMPLES.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- Load manifest ----------\n",
    "if SRC_MANIFEST.suffix == \".parquet\":\n",
    "    df = pd.read_parquet(SRC_MANIFEST, engine=\"pyarrow\")\n",
    "else:\n",
    "    df = pd.read_csv(SRC_MANIFEST)\n",
    "\n",
    "# ---------- Required columns ----------\n",
    "if \"dataset_id\" not in df.columns or \"path\" not in df.columns:\n",
    "    raise ValueError(\"Manifest must contain at least ['dataset_id','path'].\")\n",
    "\n",
    "# ---------- Vectorized ranking: lower fdr better; higher msm better ----------\n",
    "if \"fdr\" in df.columns:\n",
    "    fdr = df[\"fdr\"].to_numpy()\n",
    "    fdr = np.where(np.isfinite(fdr), fdr, 1.0)\n",
    "else:\n",
    "    fdr = np.ones(len(df), dtype=np.float32)\n",
    "\n",
    "if \"msm\" in df.columns:\n",
    "    msm = df[\"msm\"].to_numpy()\n",
    "    msm = np.where(np.isfinite(msm), -msm, 0.0)  # negative so asc == original desc\n",
    "else:\n",
    "    msm = np.zeros(len(df), dtype=np.float32)\n",
    "\n",
    "order_key = np.lexsort((msm, fdr))  # stable global order\n",
    "df = df.iloc[order_key].reset_index(drop=True)\n",
    "\n",
    "has_mz = \"peak_mz\" in df.columns\n",
    "\n",
    "# ---------- Tiled vs untiled split ----------\n",
    "tile_cols = [\"tile_r\", \"tile_c\", \"tile_h\", \"tile_w\"]\n",
    "tile_cols_exist = all(c in df.columns for c in tile_cols)\n",
    "mask_complete = df[tile_cols].notna().all(axis=1) if tile_cols_exist else pd.Series(False, index=df.index)\n",
    "\n",
    "tiled_df   = df[mask_complete].copy()\n",
    "untiled_df = df[~mask_complete].copy()\n",
    "\n",
    "# Optional filter for untiled paths\n",
    "if PROCESS_UNTILED and UNTILED_PATH_PATTERN:\n",
    "    untiled_df = untiled_df[untiled_df[\"path\"].astype(str).str.contains(UNTILED_PATH_PATTERN, case=False, na=False)]\n",
    "\n",
    "# ---------- NPZ/Numpy loader helpers ----------\n",
    "def _pick_first_2d_from_npz(npz_file: np.lib.npyio.NpzFile):\n",
    "    # Prefer common names, else first 2D\n",
    "    preferred = (\"tile\", \"image\", \"img\", \"data\", \"arr\", \"arr_0\")\n",
    "    keys = list(npz_file.keys())\n",
    "    for k in preferred:\n",
    "        if k in keys:\n",
    "            a = npz_file[k]\n",
    "            if getattr(a, \"ndim\", 0) == 2:\n",
    "                return a\n",
    "    for k in keys:\n",
    "        a = npz_file[k]\n",
    "        if getattr(a, \"ndim\", 0) == 2:\n",
    "            return a\n",
    "    return None\n",
    "\n",
    "def load_2d_array(path: str):\n",
    "    \"\"\"\n",
    "    Return (H,W) np.ndarray or None.\n",
    "    - .npy: accept 2D; or squeeze (1,H,W)->(H,W) if enabled\n",
    "    - .npz: pick first 2D array; or squeeze a (1,H,W) if enabled\n",
    "    \"\"\"\n",
    "    try:\n",
    "        obj = np.load(path, mmap_mode='r', allow_pickle=False)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    if isinstance(obj, np.lib.npyio.NpzFile):\n",
    "        a2 = _pick_first_2d_from_npz(obj)\n",
    "        if a2 is not None:\n",
    "            return a2\n",
    "        if UNTILED_ACCEPT_3D_SQUEEZE:\n",
    "            for k in obj.files:\n",
    "                a = obj[k]\n",
    "                if getattr(a, \"ndim\", 0) == 3 and 1 in a.shape:\n",
    "                    a = np.squeeze(a)\n",
    "                    if a.ndim == 2:\n",
    "                        return a\n",
    "        return None\n",
    "\n",
    "    # Regular .npy\n",
    "    a = obj\n",
    "    if getattr(a, \"ndim\", 0) == 2:\n",
    "        return a\n",
    "    if UNTILED_ACCEPT_3D_SQUEEZE and getattr(a, \"ndim\", 0) == 3 and 1 in a.shape:\n",
    "        a = np.squeeze(a)\n",
    "        if a.ndim == 2:\n",
    "            return a\n",
    "    return None\n",
    "\n",
    "# ---------- Processing ----------\n",
    "skip_reasons = Counter()\n",
    "records = []\n",
    "by_ds_counts = {}\n",
    "\n",
    "def process_group(dsid, r, c, th, tw, grp):\n",
    "    # Cap per dataset\n",
    "    if N_SAMPLES_PER_DATASET_CAP is not None and by_ds_counts.get(dsid, 0) >= N_SAMPLES_PER_DATASET_CAP:\n",
    "        skip_reasons[\"per_dataset_cap\"] += 1\n",
    "        return\n",
    "\n",
    "    # Take top C_TARGET rows from already globally ranked group\n",
    "    if len(grp) > C_TARGET:\n",
    "        grp = grp.iloc[:C_TARGET]\n",
    "\n",
    "    # Keep only real files with acceptable extensions\n",
    "    all_paths = grp[\"path\"].astype(str).to_numpy()\n",
    "    paths = [p for p in all_paths if os.path.exists(p) and (p.endswith(\".npy\") or p.endswith(\".npz\"))]\n",
    "    if not paths:\n",
    "        skip_reasons[\"no_valid_paths\"] += 1\n",
    "        return\n",
    "\n",
    "    # m/z vector aligned to kept paths\n",
    "    if has_mz:\n",
    "        mzes_full = grp[\"peak_mz\"].to_numpy()\n",
    "    else:\n",
    "        mzes_full = np.full(len(grp), np.nan, dtype=np.float32)\n",
    "\n",
    "    # Load first usable channel to determine shape/dtype\n",
    "    arr0 = None\n",
    "    first_idx = -1\n",
    "    for i, p in enumerate(paths):\n",
    "        arr0 = load_2d_array(p)\n",
    "        if arr0 is not None:\n",
    "            first_idx = i\n",
    "            break\n",
    "    if arr0 is None:\n",
    "        skip_reasons[\"load_error_or_not_2d\"] += 1\n",
    "        return\n",
    "\n",
    "    H, W = int(arr0.shape[0]), int(arr0.shape[1])\n",
    "    dtype = np.uint16 if USE_UINT16_INPUT else arr0.dtype\n",
    "\n",
    "    # Collect up to C_TARGET valid channels (shape-consistent)\n",
    "    valid_arrays = []\n",
    "    valid_mz = []\n",
    "    for i, p in enumerate(paths):\n",
    "        a = load_2d_array(p)\n",
    "        if a is None:\n",
    "            continue\n",
    "        if a.shape != (H, W):\n",
    "            skip_reasons[\"shape_mismatch\"] += 1\n",
    "            continue\n",
    "        valid_arrays.append(a)\n",
    "        # align mz\n",
    "        mz_val = mzes_full[i] if i < len(mzes_full) else np.nan\n",
    "        valid_mz.append(mz_val)\n",
    "        if len(valid_arrays) == C_TARGET:\n",
    "            break\n",
    "\n",
    "    C = len(valid_arrays)\n",
    "    if C < MIN_CHANNELS_PER_SAMPLE:\n",
    "        skip_reasons[\"too_few_channels\"] += 1\n",
    "        return\n",
    "\n",
    "    # Stack\n",
    "    patch = np.empty((C, H, W), dtype=dtype)\n",
    "    for i, a in enumerate(valid_arrays):\n",
    "        patch[i] = a.astype(dtype, copy=False) if a.dtype != dtype else np.asarray(a)\n",
    "\n",
    "    mzes = np.asarray(valid_mz, dtype=np.float32)\n",
    "\n",
    "    # Save\n",
    "    stem = f\"{dsid}_r{int(r)}_c{int(c)}_C{C}\"\n",
    "    out_path = OUT_SAMPLES / f\"{stem}.npz\"\n",
    "    if USE_COMPRESSION:\n",
    "        np.savez_compressed(out_path, patch=patch, mz=mzes)\n",
    "    else:\n",
    "        np.savez(out_path, patch=patch, mz=mzes)\n",
    "\n",
    "    records.append({\n",
    "        \"sample_path\": str(out_path),\n",
    "        \"dataset_id\": dsid,\n",
    "        \"tile_r\": int(r), \"tile_c\": int(c),\n",
    "        \"tile_h\": int(th), \"tile_w\": int(tw),\n",
    "        \"channels\": int(C),\n",
    "    })\n",
    "    by_ds_counts[dsid] = by_ds_counts.get(dsid, 0) + 1\n",
    "\n",
    "# ---------- Grouping and main loop ----------\n",
    "if tile_cols_exist and not tiled_df.empty:\n",
    "    groups_tiled = tiled_df.groupby([\"dataset_id\",\"tile_r\",\"tile_c\",\"tile_h\",\"tile_w\"], sort=False)\n",
    "    n_tiled = groups_tiled.ngroups\n",
    "else:\n",
    "    groups_tiled = None\n",
    "    n_tiled = 0\n",
    "\n",
    "if PROCESS_UNTILED and not untiled_df.empty:\n",
    "    groups_untiled = untiled_df.groupby([\"dataset_id\"], sort=False)\n",
    "    n_untiled = groups_untiled.ngroups\n",
    "else:\n",
    "    groups_untiled = None\n",
    "    n_untiled = 0\n",
    "\n",
    "total_groups = n_tiled + n_untiled\n",
    "\n",
    "with tqdm(total=total_groups, desc=\"Assembling samples\") as pbar:\n",
    "    if groups_tiled is not None:\n",
    "        for (dsid, r, c, th, tw), grp in groups_tiled:\n",
    "            process_group(dsid, r, c, th, tw, grp)\n",
    "            pbar.update(1)\n",
    "    if groups_untiled is not None:\n",
    "        for (dsid,), grp in groups_untiled:\n",
    "            # nominal coords for naming (best-effort)\n",
    "            th = int(grp[\"tile_h\"].iloc[0]) if \"tile_h\" in grp.columns and pd.notna(grp[\"tile_h\"].iloc[0]) else 0\n",
    "            tw = int(grp[\"tile_w\"].iloc[0]) if \"tile_w\" in grp.columns and pd.notna(grp[\"tile_w\"].iloc[0]) else 0\n",
    "            process_group(dsid, r=0, c=0, th=th, tw=tw, grp=grp)\n",
    "            pbar.update(1)\n",
    "\n",
    "# ---------- Write index ----------\n",
    "if records:\n",
    "    out_df = pd.DataFrame(records)\n",
    "    out_df.to_parquet(OUT_LIST, index=False, engine=\"pyarrow\")\n",
    "    if WRITE_CSV_LIST:\n",
    "        out_df.to_csv(OUT_LIST.with_suffix(\".csv\"), index=False)\n",
    "    print(f\"[OK] Wrote {len(out_df)} samples to {OUT_SAMPLES} and list to {OUT_LIST}\")\n",
    "else:\n",
    "    print(\"[WARN] No samples assembled. Check manifest/grouping.\")\n",
    "\n",
    "# ---------- Diagnostics ----------\n",
    "print(\"---- Diagnostics ----\")\n",
    "print(\"Rows in manifest:\", len(df))\n",
    "print(\"Datasets:\", df[\"dataset_id\"].nunique())\n",
    "if tile_cols_exist:\n",
    "    print(\"Rows with complete tile coords:\", int(mask_complete.sum()))\n",
    "    print(\"Tiled groups:\", n_tiled)\n",
    "else:\n",
    "    print(\"Tile columns missing; all rows treated as untiled.\")\n",
    "print(\"Untiled (dataset) groups:\", n_untiled)\n",
    "print(\"Skip reasons:\", dict(skip_reasons))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
