{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89e0c002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STATS] Loaded cached mu/std from: fm_ssl_run\\pretrained_feats2\\_stats_cache\\mu_std_c64_in256_8f39161ffd68646249a4caf6a1d88d7330ab27e1.npz\n",
      "[SKIP] Found existing feats for imagenet_deit_s16: fm_ssl_run\\pretrained_feats2\\imagenet_deit_s16\\image_feats.npy\n",
      "[SKIP] Found existing feats for imagenet_deit_b16: fm_ssl_run\\pretrained_feats2\\imagenet_deit_b16\\image_feats.npy\n",
      "[SKIP] Found existing feats for dinov2_vits14: fm_ssl_run\\pretrained_feats2\\dinov2_vits14\\image_feats.npy\n",
      "[SKIP] Found existing feats for dinov2_vitb14: fm_ssl_run\\pretrained_feats2\\dinov2_vitb14\\image_feats.npy\n",
      "[SKIP] Found existing feats for mae_vitb16: fm_ssl_run\\pretrained_feats2\\mae_vitb16\\image_feats.npy\n",
      "[SKIP] Found existing feats for deit_s16_random: fm_ssl_run\\pretrained_feats2\\deit_s16_random\\image_feats.npy\n",
      "[INFO] Extracting MEAN-CLS for imagenet_deit_b16_random :: deit_base_distilled_patch16_224 (patch-multiple=16, pretrained=False, target=224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extract (deit_base_distilled_patch16_224): 100%|██████████| 493/493 [06:51<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Saved MEAN-CLS feats to: fm_ssl_run\\pretrained_feats2\\imagenet_deit_b16_random\n",
      "[SKIP] Found existing feats for dinov2_vits14_random: fm_ssl_run\\pretrained_feats2\\dinov2_vits14_random\\image_feats.npy\n",
      "[SKIP] Found existing feats for dinov2_vitb14_random: fm_ssl_run\\pretrained_feats2\\dinov2_vitb14_random\\image_feats.npy\n",
      "[INFO] Extracting MEAN-CLS for mae_vitb16_random :: vit_base_patch16_224.mae (patch-multiple=16, pretrained=False, target=224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extract (vit_base_patch16_224.mae): 100%|██████████| 493/493 [05:59<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Saved MEAN-CLS feats to: fm_ssl_run\\pretrained_feats2\\mae_vitb16_random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# OFF-THE-SHELF FEATURES (MEAN CLS ONLY):\n",
    "# DINOv2 / DeiT / MAE on MSI via per-channel replication\n",
    "# - Each MSI channel -> replicate to RGB -> frozen ViT/CNN\n",
    "# - Extract CLS per channel, then MEAN across channels (no PCA)\n",
    "# - Caches per-channel mean/std to disk (no re-compute every run)\n",
    "# - Skips models that already have image_feats.npy\n",
    "# - Also outputs PCA(256) pixel baseline (optional, cached)\n",
    "# =====================================================\n",
    "\n",
    "import os, json, hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import timm\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "from fm_utils import (\n",
    "    CFG, NPZDataset, collate_simple, nullcontext\n",
    ")\n",
    "\n",
    "# --------------- SPEED / BEHAVIOR KNOBS ---------------\n",
    "AMP = True                        # automatic mixed precision on GPU\n",
    "CHANNELS_PER_VIEW = 64            # load up to this many MSI channels per tile\n",
    "CHANNELS_PER_STEP = 16            # process this many channels per chunk (tune for memory)\n",
    "BATCH_SIZE = 8                    # MSI tiles per batch; tune with CHANNELS_PER_STEP\n",
    "NUM_WORKERS = 0                   # dataloader workers\n",
    "TORCH_BENCHMARK = True            # cudnn autotune for fixed shapes\n",
    "SEED = 6740\n",
    "\n",
    "# -------------- PATHS / DATA --------------\n",
    "IDX_PARQUET = \"metaspace_images_dump/msi_fm_samples3.parquet\"\n",
    "OUT_DIR     = os.path.join(\"fm_ssl_run\", \"pretrained_feats2\")\n",
    "STATS_DIR   = os.path.join(OUT_DIR, \"_stats_cache\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "os.makedirs(STATS_DIR, exist_ok=True)\n",
    "\n",
    "df = pd.read_parquet(IDX_PARQUET)\n",
    "all_paths = df[\"sample_path\"].tolist()\n",
    "\n",
    "# -------------- BASE CONFIG --------------\n",
    "BASE_CFG = CFG(\n",
    "    channels_per_view=CHANNELS_PER_VIEW,\n",
    "    input_size=256,   # dataset crop size, independent from backbone target\n",
    "    crop_size=256,\n",
    "    patch_size=16,    # overridden per model to 14 for ViT/14\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "# -------------- TORCH SETUP --------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available() and TORCH_BENCHMARK:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# -------------- DATALOADER --------------\n",
    "def make_loader(paths, cfg, shuffle=False):\n",
    "    ds = NPZDataset(\n",
    "        paths,\n",
    "        target_h=int(cfg.input_size),\n",
    "        target_w=int(cfg.input_size),\n",
    "        k_target=int(cfg.channels_per_view),\n",
    "        scale_u16=True,             # returns float in [0,1]\n",
    "        pad_mode=\"repeat\",\n",
    "        sort_by_mz=False\n",
    "    )\n",
    "    ld = DataLoader(\n",
    "        ds,\n",
    "        batch_size=int(cfg.batch_size),\n",
    "        shuffle=shuffle,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        persistent_workers=(NUM_WORKERS > 0),\n",
    "        collate_fn=collate_simple,\n",
    "    )\n",
    "    return ds, ld\n",
    "\n",
    "# -------------- BACKBONE (FROZEN) --------------\n",
    "class FrozenBackbone(nn.Module):\n",
    "    \"\"\"\n",
    "    timm backbone with num_classes=0; returns a single [B, D] embedding.\n",
    "    Robust to different timm return types (dict, [B,N,D], [B,D]).\n",
    "    \"\"\"\n",
    "    def __init__(self, timm_name: str, pretrained: bool = True):\n",
    "        super().__init__()\n",
    "        self.m = timm.create_model(timm_name, pretrained=pretrained, num_classes=0)\n",
    "        for p in self.m.parameters():\n",
    "            p.requires_grad_(False)\n",
    "        self.m.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x3):  # [B,3,H,W]\n",
    "        feats = self.m.forward_features(x3)\n",
    "\n",
    "        # Common DINOv2 path (dict with normalized CLS)\n",
    "        if isinstance(feats, dict):\n",
    "            if 'x_norm_clstoken' in feats:         # DINOv2 ViT-*/14\n",
    "                return feats['x_norm_clstoken']    # [B, D]\n",
    "            if 'cls_token' in feats:               # some ViT variants\n",
    "                return feats['cls_token']          # [B, D]\n",
    "            if 'avgpool' in feats:                 # DeiT/MAE heads-off fallback\n",
    "                return feats['avgpool']            # [B, D]\n",
    "            for k in ('last_hidden_state', 'tokens', 'x'):\n",
    "                if k in feats and torch.is_tensor(feats[k]):\n",
    "                    t = feats[k]\n",
    "                    if t.dim() == 3:               # [B, N, D]\n",
    "                        return t[:, 0]             # CLS at index 0\n",
    "                    if t.dim() == 2:               # [B, D]\n",
    "                        return t\n",
    "\n",
    "        # Tensor outputs\n",
    "        if torch.is_tensor(feats):\n",
    "            if feats.dim() == 3:                   # [B, N, D]\n",
    "                return feats[:, 0]                 # CLS\n",
    "            if feats.dim() == 2:                   # [B, D]\n",
    "                return feats\n",
    "\n",
    "        # Fallback: global average over spatial/token dim if ambiguous\n",
    "        if torch.is_tensor(feats) and feats.dim() >= 3:\n",
    "            return feats.mean(dim=-2)              # [B, D] assuming [-2] is token dim\n",
    "\n",
    "        raise RuntimeError(\"Unsupported backbone forward_features() return type\")\n",
    "\n",
    "# -------------- CENTER-CROP TO PATCH MULTIPLE + RESIZE --------------\n",
    "def crop_resize_to_target(x3, target=224, patch_multiple=16):\n",
    "    \"\"\"\n",
    "    x3: [N,3,H,W] float; returns [N,3,target,target]\n",
    "    Center-crop H,W to be divisible by patch_multiple, then bilinear resize to target.\n",
    "    \"\"\"\n",
    "    _, _, H, W = x3.shape\n",
    "    Hc = (H // patch_multiple) * patch_multiple\n",
    "    Wc = (W // patch_multiple) * patch_multiple\n",
    "    dh = (H - Hc) // 2\n",
    "    dw = (W - Wc) // 2\n",
    "    if Hc > 0 and Wc > 0:\n",
    "        x3 = x3[:, :, dh:dh+Hc, dw:dw+Wc]\n",
    "    if (Hc, Wc) != (target, target):\n",
    "        x3 = F.interpolate(x3, size=(target, target), mode=\"bilinear\", align_corners=False)\n",
    "    return x3\n",
    "\n",
    "# -------------- PER-CHANNEL STATS (z-score) --------------\n",
    "@torch.no_grad()\n",
    "def compute_channel_stats(paths, cfg):\n",
    "    \"\"\"\n",
    "    Compute per-channel mean/std over provided paths.\n",
    "    In your real pipeline, compute on TRAIN split only.\n",
    "    \"\"\"\n",
    "    _, ld = make_loader(paths, cfg, shuffle=False)\n",
    "    sum_c, sumsq_c, total = None, None, 0\n",
    "    for batch in tqdm(ld, desc=\"Compute per-channel mean/std\"):\n",
    "        x = batch[\"patch\"].float()   # [B,C,H,W] in [0,1]\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.view(B, C, -1)\n",
    "        if sum_c is None:\n",
    "            sum_c   = x.sum(dim=(0,2))            # [C]\n",
    "            sumsq_c = (x**2).sum(dim=(0,2))       # [C]\n",
    "        else:\n",
    "            sum_c   += x.sum(dim=(0,2))\n",
    "            sumsq_c += (x**2).sum(dim=(0,2))\n",
    "        total += B * H * W\n",
    "    mu  = (sum_c / (total + 1e-8)).cpu().numpy()\n",
    "    var = (sumsq_c / (total + 1e-8)).cpu().numpy() - mu**2\n",
    "    std = np.sqrt(np.maximum(var, 1e-12))\n",
    "    return mu, std\n",
    "\n",
    "def _paths_signature(paths):\n",
    "    \"\"\"\n",
    "    Stable, short signature for the given path list to key the stats cache.\n",
    "    Uses count + SHA1 over first/last 100 paths to avoid huge strings.\n",
    "    \"\"\"\n",
    "    n = len(paths)\n",
    "    head = paths[:100]\n",
    "    tail = paths[-100:]\n",
    "    sig_src = json.dumps([n, head, tail], separators=(',', ':')).encode('utf-8')\n",
    "    return hashlib.sha1(sig_src).hexdigest()\n",
    "\n",
    "def _stats_cache_path(paths, cfg):\n",
    "    sig = _paths_signature(paths)\n",
    "    fname = f\"mu_std_c{int(cfg.channels_per_view)}_in{int(cfg.input_size)}_{sig}.npz\"\n",
    "    return os.path.join(STATS_DIR, fname)\n",
    "\n",
    "def load_or_compute_stats(paths, cfg, force_recompute=False):\n",
    "    \"\"\"\n",
    "    Load cached mu/std if present; otherwise compute once and cache.\n",
    "    \"\"\"\n",
    "    cache_path = _stats_cache_path(paths, cfg)\n",
    "    if (not force_recompute) and os.path.exists(cache_path):\n",
    "        z = np.load(cache_path)\n",
    "        mu, std = z[\"mu\"], z[\"std\"]\n",
    "        print(f\"[STATS] Loaded cached mu/std from: {cache_path}\")\n",
    "        return mu, std, cache_path\n",
    "\n",
    "    print(\"[STATS] Computing mu/std (once) ...\")\n",
    "    mu, std = compute_channel_stats(paths, cfg)\n",
    "    np.savez_compressed(cache_path, mu=mu, std=std, meta=dict(\n",
    "        channels_per_view=int(cfg.channels_per_view),\n",
    "        input_size=int(cfg.input_size),\n",
    "        crop_size=int(cfg.crop_size),\n",
    "        n_paths=len(paths)\n",
    "    ))\n",
    "    print(f\"[STATS] Saved mu/std to: {cache_path}\")\n",
    "    return mu, std, cache_path\n",
    "\n",
    "# -------------- EXTRACTION: MEAN CLS ONLY (FAST) --------------\n",
    "@torch.no_grad()\n",
    "def extract_offtheshelf_mean(paths, cfg, timm_id, patch_multiple, pretrained=True,\n",
    "                             mu=None, std=None, target_size=224):\n",
    "    \"\"\"\n",
    "    Mean-CLS aggregation:\n",
    "      - [B,C,H,W] -> z-score per channel (using cached mu/std)\n",
    "      - reshape to [B*C,1,H,W], replicate to RGB -> [B*C,3,H,W]\n",
    "      - center-crop to patch multiple & resize to target_size\n",
    "      - run frozen backbone in channel chunks\n",
    "      - get CLS: [B*C,D] -> reshape [B,C,D] -> mean over C -> [B,D]\n",
    "    \"\"\"\n",
    "    # Loader\n",
    "    ds, ld = make_loader(paths, cfg, shuffle=False)\n",
    "\n",
    "    # Enforce mu/std provided\n",
    "    assert mu is not None and std is not None, \"mu/std must be provided (use load_or_compute_stats).\"\n",
    "    mu_t = torch.tensor(mu, device=device).view(1, -1, 1, 1)\n",
    "    sd_t = torch.tensor(std, device=device).view(1, -1, 1, 1).clamp_min(1e-6)\n",
    "\n",
    "    # Backbone + AMP context\n",
    "    bb = FrozenBackbone(timm_id, pretrained=pretrained).to(device)\n",
    "    autocast_ctx = (torch.autocast(device_type=\"cuda\", dtype=torch.float16)\n",
    "                    if (AMP and torch.cuda.is_available()) else nullcontext())\n",
    "\n",
    "    # Adjust channel stepping for large targets to save VRAM\n",
    "    step = CHANNELS_PER_STEP if target_size == 224 else max(4, CHANNELS_PER_STEP // 2)\n",
    "\n",
    "    all_embs, all_ids = [], []\n",
    "\n",
    "    for batch in tqdm(ld, desc=f\"Extract ({timm_id})\"):\n",
    "        x = batch[\"patch\"].to(device=device).float()  # [B,C,H,W] in [0,1]\n",
    "        ids = batch[\"path\"]\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        # Z-score per channel\n",
    "        x = (x - mu_t[:, :C]) / sd_t[:, :C]\n",
    "\n",
    "        # Flatten channels into batch dimension: [B*C,1,H,W]\n",
    "        x_flat = x.permute(0, 2, 3, 1).contiguous().view(B * C, 1, H, W)\n",
    "        # Replicate to RGB\n",
    "        x_rgb = x_flat.repeat(1, 3, 1, 1)  # [B*C,3,H,W]\n",
    "\n",
    "        # Chunk over channels for memory control\n",
    "        cls_chunks = []\n",
    "        with autocast_ctx:\n",
    "            for s in range(0, B * C, step):\n",
    "                e = min(s + step, B * C)\n",
    "                xr = crop_resize_to_target(x_rgb[s:e], target=target_size, patch_multiple=patch_multiple)\n",
    "                cls = bb(xr)                             # [N,D]\n",
    "                cls_chunks.append(cls.float())\n",
    "\n",
    "        cls_all = torch.cat(cls_chunks, dim=0)          # [B*C,D]\n",
    "        D = cls_all.shape[1]\n",
    "        cls_all = cls_all.view(B, C, D)                 # [B,C,D]\n",
    "        mean_emb = cls_all.mean(dim=1)                  # [B,D]\n",
    "        mean_emb = F.normalize(mean_emb, dim=-1)        # L2 normalize\n",
    "        all_embs.append(mean_emb.cpu().numpy())\n",
    "        all_ids.extend(ids)\n",
    "\n",
    "    feats = np.concatenate(all_embs, axis=0)\n",
    "    return feats, all_ids\n",
    "\n",
    "# -------------- MODEL MENU --------------\n",
    "# key: output folder; timm: timm model id; patch: 14 or 16; pretrained: True/False; target: input resolution\n",
    "MODELS = [\n",
    "    # ImageNet-supervised\n",
    "    dict(key=\"imagenet_deit_s16\", timm=\"deit_small_distilled_patch16_224\",         patch=16, pretrained=True,  target=224),\n",
    "    dict(key=\"imagenet_deit_b16\", timm=\"deit_base_distilled_patch16_224\",         patch=16, pretrained=True,  target=224),\n",
    "\n",
    "    # DINOv2 (reg4 expects 518)\n",
    "    dict(key=\"dinov2_vits14\",     timm=\"vit_small_patch14_reg4_dinov2.lvd142m\",    patch=14, pretrained=True,  target=518),\n",
    "    dict(key=\"dinov2_vitb14\",     timm=\"vit_base_patch14_reg4_dinov2.lvd142m\",     patch=14, pretrained=True,  target=518),\n",
    "\n",
    "    # MAE\n",
    "    dict(key=\"mae_vitb16\",        timm=\"vit_base_patch16_224.mae\",                 patch=16, pretrained=True,  target=224),\n",
    "\n",
    "    # Random-init controls\n",
    "    dict(key=\"deit_s16_random\",   timm=\"deit_small_distilled_patch16_224\",         patch=16, pretrained=False, target=224),\n",
    "    dict(key=\"imagenet_deit_b16_random\",   timm=\"deit_base_distilled_patch16_224\",         patch=16, pretrained=False, target=224),\n",
    "    dict(key=\"dinov2_vits14_random\",timm=\"vit_small_patch14_reg4_dinov2\",            patch=14, pretrained=False, target=518),\n",
    "    dict(key=\"dinov2_vitb14_random\",timm=\"vit_base_patch14_reg4_dinov2\",             patch=14, pretrained=False, target=518),\n",
    "    dict(key=\"mae_vitb16_random\",        timm=\"vit_base_patch16_224.mae\",                 patch=16, pretrained=False,  target=224),\n",
    "]\n",
    "\n",
    "# Uncomment to debug a subset\n",
    "# MODELS = MODELS[:2]\n",
    "\n",
    "# -------------- PRECOMPUTE / LOAD STATS ONCE --------------\n",
    "mu_cached, std_cached, stats_path = load_or_compute_stats(all_paths, BASE_CFG, force_recompute=False)\n",
    "\n",
    "# -------------- RUN EXTRACTION (MEAN ONLY) --------------\n",
    "for m in MODELS:\n",
    "    key, timm_id, pm, use_pt, tgt = m[\"key\"], m[\"timm\"], m[\"patch\"], m[\"pretrained\"], m[\"target\"]\n",
    "    out_dir = os.path.join(OUT_DIR, f\"{key}\")\n",
    "    feats_path = os.path.join(out_dir, \"image_feats.npy\")\n",
    "\n",
    "    # Skip if already extracted\n",
    "    if os.path.exists(feats_path):\n",
    "        print(f\"[SKIP] Found existing feats for {key}: {feats_path}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"[INFO] Extracting MEAN-CLS for {key} :: {timm_id} (patch-multiple={pm}, pretrained={use_pt}, target={tgt})\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    cfg_m = CFG(**BASE_CFG.__dict__)\n",
    "    cfg_m.patch_size = pm\n",
    "\n",
    "    try:\n",
    "        feats, ids = extract_offtheshelf_mean(\n",
    "            all_paths, cfg_m, timm_id,\n",
    "            patch_multiple=pm, pretrained=use_pt,\n",
    "            mu=mu_cached, std=std_cached,\n",
    "            target_size=tgt\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Extraction failed for {key}: {e}\")\n",
    "        continue\n",
    "\n",
    "    np.save(feats_path, feats)\n",
    "    pd.DataFrame({\"sample_path\": ids}).to_csv(os.path.join(out_dir, \"index.csv\"), index=False)\n",
    "    meta = dict(\n",
    "        timm=timm_id, patch_multiple=int(pm), agg=\"mean\",\n",
    "        embed_dim=int(feats.shape[1]), pretrained=bool(use_pt),\n",
    "        adapter=\"per-channel-replicate\", input_size=int(tgt),\n",
    "        channels_per_view=int(cfg_m.channels_per_view),\n",
    "        batch_size=int(cfg_m.batch_size), channels_per_step=int(CHANNELS_PER_STEP),\n",
    "        stats_path=os.path.relpath(stats_path, out_dir)\n",
    "    )\n",
    "    json.dump(meta, open(os.path.join(out_dir, \"meta.json\"), \"w\"), indent=2)\n",
    "    print(f\"[OK] Saved MEAN-CLS feats to: {out_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8ad92b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Extracting RANDOM PROJECTION pixel baseline ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RP(256) transform:   3%|▎         | 13/493 [00:21<13:23,  1.67s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 133\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Extracting RANDOM PROJECTION pixel baseline ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 133\u001b[0m     p_feats, p_ids, p_meta \u001b[38;5;241m=\u001b[39m \u001b[43mextract_random_projection_feats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBASE_CFG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRP_N_COMPONENTS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m np\u001b[38;5;241m.\u001b[39msave(feats_path, p_feats)\n\u001b[0;32m    136\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_path\u001b[39m\u001b[38;5;124m\"\u001b[39m: p_ids})\u001b[38;5;241m.\u001b[39mto_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(out_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m), index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[2], line 35\u001b[0m, in \u001b[0;36mextract_random_projection_feats\u001b[1;34m(paths, cfg, n_components, pool_hw)\u001b[0m\n\u001b[0;32m     32\u001b[0m feats, ids \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m     33\u001b[0m rp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# instantiated after seeing input dim\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(ld, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRP(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_components\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     36\u001b[0m     x \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpatch\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mfloat()                  \u001b[38;5;66;03m# [B,C,H,W]\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     x \u001b[38;5;241m=\u001b[39m _downsample_hw(x, hw\u001b[38;5;241m=\u001b[39mpool_hw)           \u001b[38;5;66;03m# [B,C,hw,hw]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    739\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32my:\\coskun-lab\\Efe\\MSI Foundation Model\\fm_utils.py:832\u001b[0m, in \u001b[0;36mNPZDataset.__getitem__\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m    830\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpaths[i]\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39mload(path, mmap_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m data:\n\u001b[1;32m--> 832\u001b[0m     patch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpatch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# (C,H,W), often uint16\u001b[39;00m\n\u001b[0;32m    833\u001b[0m     mz    \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmz\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mfloat()     \u001b[38;5;66;03m# (C,)\u001b[39;00m\n\u001b[0;32m    835\u001b[0m \u001b[38;5;66;03m# scale to [0,1] if values look like uint16\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\numpy\\lib\\_npyio_impl.py:254\u001b[0m, in \u001b[0;36mNpzFile.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mMAGIC_PREFIX:\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28mbytes\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip\u001b[38;5;241m.\u001b[39mopen(key)\n\u001b[1;32m--> 254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip\u001b[38;5;241m.\u001b[39mread(key)\n",
      "File \u001b[1;32mc:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\numpy\\lib\\format.py:851\u001b[0m, in \u001b[0;36mread_array\u001b[1;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[0;32m    849\u001b[0m             read_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(max_read_count, count \u001b[38;5;241m-\u001b[39m i)\n\u001b[0;32m    850\u001b[0m             read_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(read_count \u001b[38;5;241m*\u001b[39m dtype\u001b[38;5;241m.\u001b[39mitemsize)\n\u001b[1;32m--> 851\u001b[0m             data \u001b[38;5;241m=\u001b[39m \u001b[43m_read_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marray data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    852\u001b[0m             array[i:i\u001b[38;5;241m+\u001b[39mread_count] \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mfrombuffer(data, dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    853\u001b[0m                                                      count\u001b[38;5;241m=\u001b[39mread_count)\n\u001b[0;32m    855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fortran_order:\n",
      "File \u001b[1;32mc:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\numpy\\lib\\format.py:986\u001b[0m, in \u001b[0;36m_read_bytes\u001b[1;34m(fp, size, error_template)\u001b[0m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    982\u001b[0m     \u001b[38;5;66;03m# io files (default in python3) return None or raise on\u001b[39;00m\n\u001b[0;32m    983\u001b[0m     \u001b[38;5;66;03m# would-block, python2 file will truncate, probably nothing can be\u001b[39;00m\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;66;03m# done about that.  note that regular files can't be non-blocking\u001b[39;00m\n\u001b[0;32m    985\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 986\u001b[0m         r \u001b[38;5;241m=\u001b[39m \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    987\u001b[0m         data \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m r\n\u001b[0;32m    988\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(r) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m==\u001b[39m size:\n",
      "File \u001b[1;32mc:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\zipfile.py:930\u001b[0m, in \u001b[0;36mZipExtFile.read\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m n \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof:\n\u001b[1;32m--> 930\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    931\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(data):\n\u001b[0;32m    932\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_readbuffer \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[1;32mc:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\zipfile.py:1000\u001b[0m, in \u001b[0;36mZipExtFile._read1\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    998\u001b[0m         data \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read2(n \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(data))\n\u001b[0;32m    999\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1000\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compress_type \u001b[38;5;241m==\u001b[39m ZIP_STORED:\n\u001b[0;32m   1003\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compress_left \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\zipfile.py:1030\u001b[0m, in \u001b[0;36mZipExtFile._read2\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1027\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(n, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMIN_READ_SIZE)\n\u001b[0;32m   1028\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(n, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compress_left)\n\u001b[1;32m-> 1030\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fileobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compress_left \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data)\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n",
      "File \u001b[1;32mc:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\zipfile.py:750\u001b[0m, in \u001b[0;36m_SharedFile.read\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    746\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt read from the ZIP file while there \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    747\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis an open writing handle on it. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    748\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClose the writing handle before trying to read.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pos)\n\u001b[1;32m--> 750\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file\u001b[38;5;241m.\u001b[39mtell()\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ===================== FAST PIXEL BASELINES =====================\n",
    "# Option A (default): Sparse Random Projection (RP) to 256 dims\n",
    "#   - No training, super fast, JL guarantee\n",
    "# Option B: Accelerated IncrementalPCA with spatial downsampling\n",
    "#   - If you still want PCA, set USE_IPCA=True\n",
    "\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "USE_IPCA = False          # << flip to True if you really want IPCA\n",
    "RP_N_COMPONENTS = 256\n",
    "IPCA_N_COMPONENTS = 256\n",
    "DOWNSAMPLE_HW = 64        # << downsample H,W to this before PCA (was 256)\n",
    "\n",
    "def _downsample_hw(x, hw=DOWNSAMPLE_HW):\n",
    "    # x: [B,C,H,W] -> [B,C,hw,hw]\n",
    "    if x.shape[-1] == hw and x.shape[-2] == hw:\n",
    "        return x\n",
    "    return F.interpolate(x, size=(hw, hw), mode=\"area\")\n",
    "\n",
    "# ---------- Option A: Random Projection (fast) ----------\n",
    "@torch.no_grad()\n",
    "def extract_random_projection_feats(paths, cfg, n_components=RP_N_COMPONENTS, pool_hw=DOWNSAMPLE_HW):\n",
    "    \"\"\"\n",
    "    Fast, training-free JL projection baseline.\n",
    "    Steps:\n",
    "      - Load [B,C,H,W] in [0,1]\n",
    "      - Downsample to [B,C,hw,hw] (area pooling)\n",
    "      - Flatten to [B, C*hw*hw]\n",
    "      - SparseRandomProjection to n_components, per-batch\n",
    "    \"\"\"\n",
    "    ds, ld = make_loader(paths, cfg, shuffle=False)\n",
    "    feats, ids = [], []\n",
    "    rp = None  # instantiated after seeing input dim\n",
    "\n",
    "    for batch in tqdm(ld, desc=f\"RP({n_components}) transform\"):\n",
    "        x = batch[\"patch\"].float()                  # [B,C,H,W]\n",
    "        x = _downsample_hw(x, hw=pool_hw)           # [B,C,hw,hw]\n",
    "        B, C, H, W = x.shape\n",
    "        arr = x.permute(0,2,3,1).contiguous().view(B, -1).cpu().numpy()  # [B, C*H*W]\n",
    "\n",
    "        if rp is None:\n",
    "            rp = SparseRandomProjection(n_components=n_components)  # defines components from input dim\n",
    "            z = rp.fit_transform(arr)  # first batch fits the random matrix shape\n",
    "        else:\n",
    "            z = rp.transform(arr)\n",
    "        # L2 norm\n",
    "        z = z / (np.linalg.norm(z, axis=1, keepdims=True) + 1e-8)\n",
    "        feats.append(z.astype(np.float32))\n",
    "        ids.extend(batch[\"path\"])\n",
    "    feats = np.concatenate(feats, 0)\n",
    "    meta = dict(\n",
    "        method=\"sparse_random_projection\",\n",
    "        n_components=int(n_components),\n",
    "        downsample_hw=int(pool_hw),\n",
    "        D_input=int(C*H*W),\n",
    "        note=\"JL projection; no training\"\n",
    "    )\n",
    "    return feats, ids, meta\n",
    "\n",
    "# ---------- Option B: Accelerated IPCA (still reasonably fast) ----------\n",
    "def extract_pca_feats_fast(paths, cfg, n_components=IPCA_N_COMPONENTS, batch_bytes_target=768*1024*1024):\n",
    "    \"\"\"\n",
    "    Faster IPCA by (1) aggressive downsampling to DOWNSAMPLE_HW,\n",
    "    (2) ensuring first partial_fit sees >= n_components samples,\n",
    "    (3) larger warmup batches, and (4) keeping data on CPU float32.\n",
    "    \"\"\"\n",
    "    ds, ld = make_loader(paths, cfg, shuffle=False)\n",
    "    H, W, C = DOWNSAMPLE_HW, DOWNSAMPLE_HW, int(cfg.channels_per_view)\n",
    "    D = H * W * C\n",
    "\n",
    "    # Memory-based batch size (in samples)\n",
    "    bytes_per_sample = D * 4  # float32\n",
    "    mem_based = max(1, int(batch_bytes_target // bytes_per_sample))\n",
    "\n",
    "    total_samples = len(paths)\n",
    "    n_comp_eff = min(n_components, total_samples)\n",
    "    if n_comp_eff < n_components:\n",
    "        print(f\"[IPCA] Reducing n_components {n_components} -> {n_comp_eff} (only {total_samples} samples).\")\n",
    "    batch_n = max(mem_based, n_comp_eff)\n",
    "\n",
    "    ipca = IncrementalPCA(n_components=n_comp_eff, batch_size=batch_n)\n",
    "\n",
    "    # -------- Pass 1: fit --------\n",
    "    buf = []\n",
    "    buf_rows = 0\n",
    "    for batch in tqdm(ld, desc=\"IPCA Pass1 (fit, downsampled)\"):\n",
    "        x = batch[\"patch\"].float()                  # [B,C,256,256]\n",
    "        x = _downsample_hw(x, hw=DOWNSAMPLE_HW)     # [B,C,H,W] smaller\n",
    "        B, C2, H2, W2 = x.shape\n",
    "        arr = x.permute(0,2,3,1).contiguous().view(B, -1).cpu().numpy()  # [B, D]\n",
    "        buf.append(arr); buf_rows += arr.shape[0]\n",
    "\n",
    "        if buf_rows >= batch_n:\n",
    "            ipca.partial_fit(np.vstack(buf)); buf, buf_rows = [], 0\n",
    "    if buf_rows > 0:\n",
    "        ipca.partial_fit(np.vstack(buf)); buf, buf_rows = [], 0\n",
    "\n",
    "    # -------- Pass 2: transform --------\n",
    "    feats, ids = [], []\n",
    "    for batch in tqdm(ld, desc=\"IPCA Pass2 (transform)\"):\n",
    "        x = batch[\"patch\"].float()\n",
    "        x = _downsample_hw(x, hw=DOWNSAMPLE_HW)\n",
    "        B, C2, H2, W2 = x.shape\n",
    "        arr = x.permute(0,2,3,1).contiguous().view(B, -1).cpu().numpy()\n",
    "        z = ipca.transform(arr)\n",
    "        z = z / (np.linalg.norm(z, axis=1, keepdims=True) + 1e-8)\n",
    "        feats.append(z.astype(np.float32))\n",
    "        ids.extend(batch[\"path\"])\n",
    "\n",
    "    feats = np.concatenate(feats, 0)\n",
    "    meta = dict(\n",
    "        method=\"incremental_pca\",\n",
    "        n_components=int(ipca.n_components_),\n",
    "        downsample_hw=int(DOWNSAMPLE_HW),\n",
    "        D_input=int(D),\n",
    "        batch_size_used=int(batch_n)\n",
    "    )\n",
    "    return feats, ids, meta\n",
    "\n",
    "# ---------- Call: choose RP (fast) or IPCA (fallback) ----------\n",
    "try:\n",
    "    out_dir = os.path.join(OUT_DIR, \"pixels_fast256\")\n",
    "    feats_path = os.path.join(out_dir, \"image_feats.npy\")\n",
    "    if os.path.exists(feats_path):\n",
    "        print(f\"[SKIP] Fast pixel baseline already exists: {feats_path}\")\n",
    "    else:\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        if USE_IPCA:\n",
    "            print(\"[INFO] Extracting FAST IPCA pixel baseline ...\")\n",
    "            p_feats, p_ids, p_meta = extract_pca_feats_fast(all_paths, BASE_CFG, n_components=IPCA_N_COMPONENTS)\n",
    "        else:\n",
    "            print(\"[INFO] Extracting RANDOM PROJECTION pixel baseline ...\")\n",
    "            p_feats, p_ids, p_meta = extract_random_projection_feats(all_paths, BASE_CFG, n_components=RP_N_COMPONENTS)\n",
    "\n",
    "        np.save(feats_path, p_feats)\n",
    "        pd.DataFrame({\"sample_path\": p_ids}).to_csv(os.path.join(out_dir, \"index.csv\"), index=False)\n",
    "        json.dump(p_meta, open(os.path.join(out_dir, \"meta.json\"), \"w\"), indent=2)\n",
    "        print(f\"[OK] Saved fast pixel baseline to: {out_dir} ({p_meta['method']}, n_components={p_meta['n_components']})\")\n",
    "except Exception as e:\n",
    "    print(f\"[WARN] Fast pixel baseline failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ffa2c1",
   "metadata": {},
   "source": [
    "### DOWNSTREAM EVALUATION ON HELD-OUT SPLITS with Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcf3a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using TRAIN_OUT: fm_ssl_run\\20251007_235600\n",
      "[WARN] duplicate sample_path rows in metadata; keeping first.\n",
      "[SKIP] 'imagenet_deit_s16': results already exist and are up-to-date.\n",
      "[SKIP] 'imagenet_deit_b16': results already exist and are up-to-date.\n",
      "[SKIP] 'dinov2_vitb14': results already exist and are up-to-date.\n",
      "[SKIP] 'mae_vitb16': results already exist and are up-to-date.\n",
      "[SKIP] 'dinov2_vits14': results already exist and are up-to-date.\n",
      "[SKIP] 'deit_s16_random': results already exist and are up-to-date.\n",
      "[SKIP] 'imagenet_deit_b16_random': results already exist and are up-to-date.\n",
      "[SKIP] 'dinov2_vits14_random': results already exist and are up-to-date.\n",
      "[SKIP] 'dinov2_vitb14_random': results already exist and are up-to-date.\n",
      "\n",
      "[RUN] Evaluating model 'mae_vitb16_random' from fm_ssl_run\\pretrained_feats2\\mae_vitb16_random\n",
      "[INFO] (mae_vitb16_random) Using BEST embeddings: fm_ssl_run\\pretrained_feats2\\mae_vitb16_random\\image_feats.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tasks[mae_vitb16_random]:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== [mae_vitb16_random] Task: organism ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "Tasks[mae_vitb16_random]:  17%|█▋        | 1/6 [00:25<02:08, 25.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== [mae_vitb16_random] Task: polarity ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "Tasks[mae_vitb16_random]:  33%|███▎      | 2/6 [00:47<01:34, 23.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== [mae_vitb16_random] Task: Organism_Part ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "Tasks[mae_vitb16_random]:  50%|█████     | 3/6 [01:43<01:54, 38.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== [mae_vitb16_random] Task: Condition ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "Tasks[mae_vitb16_random]:  67%|██████▋   | 4/6 [02:31<01:24, 42.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== [mae_vitb16_random] Task: analyzerType ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "Tasks[mae_vitb16_random]:  83%|████████▎ | 5/6 [03:18<00:43, 43.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== [mae_vitb16_random] Task: ionisationSource ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "Tasks[mae_vitb16_random]: 100%|██████████| 6/6 [04:05<00:00, 40.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] (mae_vitb16_random) Saved per-model results to: fm_ssl_run\\pretrained_feats2\\mae_vitb16_random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# DOWNSTREAM EVALUATION ON HELD-OUT SPLITS (multi-baseline)\n",
    "# - Canonicalize/merge label variants\n",
    "# - Evaluate many embedding sets with linear probe & k-NN\n",
    "# - Extras: Few-shot curves & k-NN sensitivity\n",
    "# - Save per-model results + combined CSV + plots\n",
    "# - NEW: Skip guards to avoid recomputing existing baselines\n",
    "# =====================================================\n",
    "\n",
    "import os, copy, re, glob, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "SEED = 6740\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# -------------------------\n",
    "# Speed / behavior knobs\n",
    "# -------------------------\n",
    "FAST_MODE = True                       # flip False for full evaluation\n",
    "FEW_SHOT_GRID = [1, 5, 10, 25, None]   # None means \"All train\"\n",
    "K_GRID = [1, 5, 10, 20, 50]\n",
    "K_FOR_KNN = 10 if FAST_MODE else 20\n",
    "C_GRID = [0.1, 1.0] if FAST_MODE else [0.01, 0.1, 0.5, 1.0, 2.0, 5.0]\n",
    "MAX_ITER_LR = 800 if FAST_MODE else 2000\n",
    "MIN_CLASS_COUNT = 100 if FAST_MODE else 50\n",
    "PCA_DIM = None                         # e.g., 256 to speed up; None disables\n",
    "METABOFM_VERSION = \"last\"\n",
    "\n",
    "# -------------------------\n",
    "# Skips & guards\n",
    "# -------------------------\n",
    "ONLY_EVAL_EXISTING = True     # only run models that already have embeddings saved\n",
    "SKIP_IF_RESULTS_EXIST = True  # don't recompute per-model results if CSVs already exist\n",
    "FORCE_REEVAL = False          # set True to ignore SKIP_IF_RESULTS_EXIST\n",
    "\n",
    "FEATS_PRIMARY = \"image_feats.npy\"\n",
    "FEATS_ALT_LAST = \"image_feats_last.npy\"\n",
    "INDEX_FILE = \"index.csv\"\n",
    "\n",
    "# keep a simple constant for the index\n",
    "REQUIRED_INDEX = INDEX_FILE\n",
    "\n",
    "def get_feats_path(model_tag: str, emb_dir: str) -> str | None:\n",
    "    \"\"\"Return the feature path for the given model.\"\"\"\n",
    "    if model_tag == \"metabofm\":\n",
    "        fname = FEATS_PRIMARY if METABOFM_VERSION == \"best\" else FEATS_ALT_LAST\n",
    "        path = os.path.join(emb_dir, fname)\n",
    "        if os.path.exists(path):\n",
    "            print(f\"[INFO] Using MetaboFM-{METABOFM_VERSION.upper()} embeddings: {path}\")\n",
    "            return path\n",
    "        else:\n",
    "            print(f\"[WARN] MetaboFM-{METABOFM_VERSION.upper()} embeddings not found at {path}\")\n",
    "            return None\n",
    "    else:\n",
    "        path = os.path.join(emb_dir, FEATS_PRIMARY)\n",
    "        return path if os.path.exists(path) else None\n",
    "\n",
    "def embeddings_ready(model_tag: str, emb_dir: str) -> bool:\n",
    "    feats_path = get_feats_path(model_tag, emb_dir)\n",
    "    return feats_path is not None and os.path.exists(os.path.join(emb_dir, REQUIRED_INDEX))\n",
    "\n",
    "def resolve_model_outdir(model_tag: str, emb_dir: str) -> str:\n",
    "    if model_tag == \"metabofm\":\n",
    "        return TRAIN_OUT\n",
    "    return emb_dir\n",
    "\n",
    "def results_already_done(model_tag: str, emb_dir: str) -> bool:\n",
    "    primary_out_dir = resolve_model_outdir(model_tag, emb_dir)\n",
    "    res_csv = os.path.join(primary_out_dir, \"downstream_results.csv\")\n",
    "    return os.path.exists(res_csv)\n",
    "\n",
    "def inputs_newer_than_results(model_tag: str, emb_dir: str) -> bool:\n",
    "    \"\"\"If inputs are newer than results, you may want to recompute.\"\"\"\n",
    "    primary_out_dir = resolve_model_outdir(model_tag, emb_dir)\n",
    "    res_csv = os.path.join(primary_out_dir, \"downstream_results.csv\")\n",
    "    if not os.path.exists(res_csv):\n",
    "        return True\n",
    "    res_mtime = os.path.getmtime(res_csv)\n",
    "    latest_input_mtime = max(\n",
    "        os.path.getmtime(os.path.join(emb_dir, f))\n",
    "        for f in REQUIRED_FILES\n",
    "        if os.path.exists(os.path.join(emb_dir, f))\n",
    "    )\n",
    "    return latest_input_mtime > res_mtime\n",
    "\n",
    "def inputs_newer_than_results(model_tag: str, emb_dir: str) -> bool:\n",
    "    primary_out_dir = resolve_model_outdir(model_tag, emb_dir)\n",
    "    res_csv = os.path.join(primary_out_dir, \"downstream_results.csv\")\n",
    "    if not os.path.exists(res_csv):\n",
    "        return True\n",
    "    res_mtime = os.path.getmtime(res_csv)\n",
    "\n",
    "    feats_path = get_feats_path(model_tag, emb_dir)\n",
    "    idx_path = os.path.join(emb_dir, REQUIRED_INDEX)\n",
    "    inputs = [p for p in [feats_path, idx_path] if p and os.path.exists(p)]\n",
    "    if not inputs:\n",
    "        return True\n",
    "    latest_input_mtime = max(os.path.getmtime(p) for p in inputs)\n",
    "    return latest_input_mtime > res_mtime\n",
    "\n",
    "# -------------------------\n",
    "# Paths / run root\n",
    "# -------------------------\n",
    "TRAIN_OUT = os.path.join(\"fm_ssl_run\", \"20251007_235600\")\n",
    "print(\"[INFO] Using TRAIN_OUT:\", TRAIN_OUT)\n",
    "\n",
    "# Metadata + splits\n",
    "SPLIT_CSV    = os.path.join(TRAIN_OUT, \"splits_by_dataset_id.csv\")\n",
    "IDX_PARQUET  = \"metaspace_images_dump/msi_fm_samples3.parquet\"\n",
    "MAN_PARQUET  = \"metaspace_images_dump/manifest_expanded.parquet\"\n",
    "\n",
    "# Centralized benchmark outputs\n",
    "BENCH_OUT = os.path.join(TRAIN_OUT, \"baseline_eval\")\n",
    "PLOTS_OUT = os.path.join(BENCH_OUT, \"plots\")\n",
    "os.makedirs(BENCH_OUT, exist_ok=True)\n",
    "os.makedirs(PLOTS_OUT, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Which embedding sets to evaluate\n",
    "# (ensure these folders contain image_feats.npy + index.csv)\n",
    "# -------------------------\n",
    "EMB_SETS = {\n",
    "    # Off-the-shelf baselines\n",
    "    \"imagenet_deit_s16\":      os.path.join(\"fm_ssl_run\", \"pretrained_feats2\", \"imagenet_deit_s16\"),\n",
    "    \"imagenet_deit_b16\":      os.path.join(\"fm_ssl_run\", \"pretrained_feats2\", \"imagenet_deit_b16\"),\n",
    "    \"dinov2_vitb14\":          os.path.join(\"fm_ssl_run\", \"pretrained_feats2\", \"dinov2_vitb14\"),\n",
    "    \"mae_vitb16\":             os.path.join(\"fm_ssl_run\", \"pretrained_feats2\", \"mae_vitb16\"),\n",
    "\n",
    "    # New recommended baselines\n",
    "    \"dinov2_vits14\":          os.path.join(\"fm_ssl_run\", \"pretrained_feats2\", \"dinov2_vits14\"),\n",
    "    \"deit_s16_random\":        os.path.join(\"fm_ssl_run\", \"pretrained_feats2\", \"deit_s16_random\"),\n",
    "    \"imagenet_deit_b16_random\":        os.path.join(\"fm_ssl_run\", \"pretrained_feats2\", \"imagenet_deit_b16_random\"),\n",
    "    \"dinov2_vits14_random\":        os.path.join(\"fm_ssl_run\", \"pretrained_feats2\", \"dinov2_vits14_random\"),\n",
    "    \"dinov2_vitb14_random\":        os.path.join(\"fm_ssl_run\", \"pretrained_feats2\", \"dinov2_vitb14_random\"),\n",
    "    \"mae_vitb16_random\":             os.path.join(\"fm_ssl_run\", \"pretrained_feats2\", \"mae_vitb16_random\"),\n",
    "    #\"pca256_pixels\":          os.path.join(\"fm_ssl_run\", \"pretrained_feats2\", \"pixels_fast256\"),\n",
    "}\n",
    "\n",
    "# -------------------------\n",
    "# Canonicalization / merging rules\n",
    "# -------------------------\n",
    "def _clean(s):\n",
    "    if pd.isna(s): return None\n",
    "    s = str(s).strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def canonicalize_labels(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1) Polarity\n",
    "    pol_map = {\"pos\":\"Positive\",\"positive\":\"Positive\",\"+\":\"Positive\",\n",
    "               \"neg\":\"Negative\",\"negative\":\"Negative\",\"-\":\"Negative\"}\n",
    "    def canon_polarity(s):\n",
    "        if s is None: return None\n",
    "        t = _clean(s).lower()\n",
    "        t2 = pol_map.get(t, t)\n",
    "        if t2 in (\"positive\",\"negative\"):\n",
    "            return t2.capitalize()\n",
    "        if \"pos\" in t: return \"Positive\"\n",
    "        if \"neg\" in t: return \"Negative\"\n",
    "        return _clean(s)\n",
    "    if \"polarity\" in df.columns:\n",
    "        df[\"polarity\"] = df[\"polarity\"].map(canon_polarity)\n",
    "\n",
    "    # 2) Ionisation Source (map DESI-MSI -> DESI, etc.)\n",
    "    def canon_ion_src(s):\n",
    "        if s is None: return None\n",
    "        t_raw = _clean(s)\n",
    "        t = t_raw.upper().replace(\"-\", \"\").replace(\"_\",\"\")\n",
    "        if \"APSMALDI\" in t: return \"AP-SMALDI\"\n",
    "        if \"IRMALDESI\" in t or \"IRMALDI\" in t: return \"IR-MALDESI\"\n",
    "        if \"APMALDI\" in t: return \"AP-MALDI\"\n",
    "        if \"DESIMSI\" in t: return \"DESI\"\n",
    "        if \"DESI\" in t: return \"DESI\"\n",
    "        if \"MALDI\" in t: return \"MALDI\"\n",
    "        return t_raw\n",
    "    if \"ionisationSource\" in df.columns:\n",
    "        df[\"ionisationSource\"] = df[\"ionisationSource\"].map(canon_ion_src)\n",
    "\n",
    "    # 3) Analyzer Type\n",
    "    def canon_analyzer(s):\n",
    "        if s is None: return None\n",
    "        t = _clean(s); tl = t.lower()\n",
    "        if \"timstof\" in tl and \"flex\" in tl: return \"timsTOF Flex\"\n",
    "        if \"fticr\" in tl:\n",
    "            if \"12t\" in tl: return \"12T FTICR\"\n",
    "            if \"7t\" in tl and \"scimax\" in tl: return \"FTICR scimaX 7T\"\n",
    "            return \"FTICR\"\n",
    "        if \"orbitrap\" in tl or \"q-exactive\" in tl: return \"Orbitrap\"\n",
    "        if \"tof\" in tl and \"reflector\" in tl: return \"TOF reflector\"\n",
    "        if tl.strip() == \"qtof\": return \"qTOF\"\n",
    "        return t\n",
    "    if \"analyzerType\" in df.columns:\n",
    "        df[\"analyzerType\"] = df[\"analyzerType\"].map(canon_analyzer)\n",
    "\n",
    "    # 4) Organism\n",
    "    def canon_organism(s):\n",
    "        if s is None: return None\n",
    "        t = _clean(s); tl = t.lower()\n",
    "        if \"|\" in t or \",\" in t:\n",
    "            if (\"human\" in tl or \"homo sapiens\" in tl) and (\"mouse\" in tl or \"mus musculus\" in tl):\n",
    "                return \"Mixed\"\n",
    "        if \"homo sapiens\" in tl or tl.strip() in {\"human\",\"h. sapiens\",\"homo\"}:\n",
    "            return \"Homo sapiens\"\n",
    "        if \"mus musculus\" in tl or tl.strip() in {\"mouse\",\"m. musculus\"}:\n",
    "            return \"Mus musculus\"\n",
    "        return t\n",
    "    if \"organism\" in df.columns:\n",
    "        df[\"organism\"] = df[\"organism\"].map(canon_organism)\n",
    "\n",
    "    # 5) Organism_Part\n",
    "    def canon_part(s):\n",
    "        if s is None: return None\n",
    "        t = _clean(s); tl = t.lower()\n",
    "        if \"kidney\" in tl: return \"Kidney\"\n",
    "        if \"brain\"  in tl: return \"Brain\"\n",
    "        if \"liver\"  in tl: return \"Liver\"\n",
    "        if \"lung\"   in tl: return \"Lung\"\n",
    "        if \"breast\" in tl: return \"Breast\"\n",
    "        if \"skin\"   in tl: return \"Skin\"\n",
    "        if \"heart\"  in tl or \"cardiac\" in tl: return \"Heart\"\n",
    "        return t\n",
    "    if \"Organism_Part\" in df.columns:\n",
    "        df[\"Organism_Part\"] = df[\"Organism_Part\"].map(canon_part)\n",
    "\n",
    "    # 6) Condition (keep \"NA\" but we'll exclude it later)\n",
    "    def canon_condition(s):\n",
    "        if s is None: return None\n",
    "        t = _clean(s); tl = t.lower()\n",
    "        if tl in {\"n/a\",\"na\",\"none\",\"not available\",\"\"}: return \"NA\"\n",
    "        if tl in {\"biopsy\",\"biopsies\"}: return \"Biopsy\"\n",
    "        if \"fresh frozen\" in tl or \"frozen\" in tl: return \"Frozen\"\n",
    "        if \"tumor\" in tl or \"tumour\" in tl: return \"Tumor\"\n",
    "        if \"cancer\" in tl: return \"Cancer\"\n",
    "        if \"wildtype\" in tl or tl == \"wt\": return \"Wildtype\"\n",
    "        if \"healthy\" in tl or \"control\" in tl: return \"Healthy\"\n",
    "        if \"diseased\" in tl or \"disease\" in tl: return \"Diseased\"\n",
    "        return t\n",
    "    if \"Condition\" in df.columns:\n",
    "        df[\"Condition\"] = df[\"Condition\"].map(canon_condition)\n",
    "\n",
    "    return df\n",
    "\n",
    "# -------------------------\n",
    "# Load metadata + splits (once)\n",
    "# -------------------------\n",
    "idx = pd.read_parquet(IDX_PARQUET)\n",
    "man = pd.read_parquet(MAN_PARQUET)\n",
    "\n",
    "need_cols = [\n",
    "    \"dataset_id\", \"organism\", \"polarity\", \"Organism_Part\", \"Condition\",\n",
    "    \"analyzerType\", \"ionisationSource\"\n",
    "]\n",
    "man_sub = man[[c for c in need_cols if c in man.columns]].drop_duplicates(\"dataset_id\")\n",
    "\n",
    "df_meta = idx.merge(man_sub, on=\"dataset_id\", how=\"left\", suffixes=(\"\", \"_man\"))\n",
    "df_meta = df_meta.loc[:, ~df_meta.columns.duplicated()].copy().reset_index(drop=True)\n",
    "splits = pd.read_csv(SPLIT_CSV)\n",
    "df_meta = df_meta.merge(splits, on=\"dataset_id\", how=\"left\")\n",
    "\n",
    "# Dedup metadata by sample_path and canonicalize labels\n",
    "if df_meta.duplicated(\"sample_path\").sum():\n",
    "    print(\"[WARN] duplicate sample_path rows in metadata; keeping first.\")\n",
    "    df_meta = df_meta.drop_duplicates(\"sample_path\", keep=\"first\").reset_index(drop=True)\n",
    "df_meta = canonicalize_labels(df_meta)\n",
    "\n",
    "# -------------------------\n",
    "# Common helpers\n",
    "# -------------------------\n",
    "TASKS = [\"organism\", \"polarity\", \"Organism_Part\", \"Condition\", \"analyzerType\", \"ionisationSource\"]\n",
    "EXCLUDE_LABELS = {\"Condition\": {\"NA\"}}\n",
    "\n",
    "def filter_valid(df_task, yname, min_count=5):\n",
    "    x = df_task.dropna(subset=[yname]).copy()\n",
    "    if yname in EXCLUDE_LABELS:\n",
    "        x = x[~x[yname].isin(EXCLUDE_LABELS[yname])]\n",
    "    x = x[x[yname].astype(str).str.len() > 0]\n",
    "    vc = x[yname].value_counts()\n",
    "    keep = vc[vc >= min_count].index\n",
    "    x = x[x[yname].isin(keep)].copy()\n",
    "    return x\n",
    "\n",
    "def few_shot_subset(df_task, yname, shots_per_class=None, seed=SEED):\n",
    "    if not shots_per_class or shots_per_class <= 0:\n",
    "        return (df_task[\"split\"] == \"train\").values\n",
    "    rng = np.random.RandomState(seed)\n",
    "    m_train = (df_task[\"split\"] == \"train\").values\n",
    "    keep = np.zeros(len(df_task), dtype=bool)\n",
    "    labels = df_task.loc[m_train, yname].astype(str).values\n",
    "    idxs   = np.where(m_train)[0]\n",
    "    from collections import defaultdict\n",
    "    per_class = defaultdict(list)\n",
    "    for i, lbl in zip(idxs, labels):\n",
    "        per_class[lbl].append(i)\n",
    "    for lbl, arr in per_class.items():\n",
    "        arr = np.array(arr)\n",
    "        rng.shuffle(arr)\n",
    "        keep[arr[:min(shots_per_class, len(arr))]] = True\n",
    "    return keep\n",
    "\n",
    "def get_mask(df_all, split_name):\n",
    "    return (df_all[\"split\"] == split_name).values\n",
    "\n",
    "def run_linear_probe(X_tr, y_tr, X_va, y_va, X_te, y_te):\n",
    "    pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "        (\"clf\", LogisticRegression(\n",
    "            solver=\"saga\",\n",
    "            max_iter=MAX_ITER_LR,\n",
    "            class_weight=\"balanced\",\n",
    "            random_state=SEED,\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ])\n",
    "    grid = {\"clf__C\": C_GRID}\n",
    "    best = None; best_va = -np.inf\n",
    "    for p in tqdm(list(ParameterGrid(grid)), desc=\"LinearProbe grid\", leave=False):\n",
    "        pipe.set_params(**p)\n",
    "        pipe.fit(X_tr, y_tr)\n",
    "        pred_va = pipe.predict(X_va)\n",
    "        macro_f1 = f1_score(y_va, pred_va, average=\"macro\")\n",
    "        if macro_f1 > best_va:\n",
    "            best_va = macro_f1\n",
    "            best = copy.deepcopy(pipe)\n",
    "    pred_te = best.predict(X_te)\n",
    "    acc = accuracy_score(y_te, pred_te)\n",
    "    f1m = f1_score(y_te, pred_te, average=\"macro\")\n",
    "    return acc, f1m, pred_te, best\n",
    "\n",
    "def run_knn(X_tr, y_tr, X_te, y_te, k=20):\n",
    "    n_fit = int(X_tr.shape[0])\n",
    "    # need at least 1 sample and 2 classes to classify\n",
    "    if n_fit < 1 or len(np.unique(y_tr)) < 2:\n",
    "        return np.nan, np.nan, np.array([], dtype=object), None, 0\n",
    "\n",
    "    k_eff = max(1, min(k, n_fit))\n",
    "    knn = KNeighborsClassifier(n_neighbors=k_eff, metric=\"cosine\", n_jobs=-1)\n",
    "    knn.fit(X_tr, y_tr)\n",
    "\n",
    "    if X_te.shape[0] == 0:\n",
    "        return np.nan, np.nan, np.array([], dtype=object), knn, k_eff\n",
    "\n",
    "    pred_te = knn.predict(X_te)\n",
    "    acc = accuracy_score(y_te, pred_te)\n",
    "    f1m = f1_score(y_te, pred_te, average=\"macro\")\n",
    "    return acc, f1m, pred_te, knn, k_eff\n",
    "\n",
    "def per_class_f1(y_true, y_pred):\n",
    "    rep = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "    out = {k: v[\"f1-score\"] for k, v in rep.items() if k not in (\"accuracy\",\"macro avg\",\"weighted avg\")}\n",
    "    return out\n",
    "\n",
    "# -------------------------\n",
    "# Evaluate ONE embedding set dir\n",
    "# -------------------------\n",
    "def evaluate_embeddings(emb_dir: str, model_tag: str):\n",
    "    feats_path = get_feats_path(model_tag, emb_dir)\n",
    "    index_path = os.path.join(emb_dir, INDEX_FILE)\n",
    "    if not (feats_path and os.path.exists(feats_path) and os.path.exists(index_path)):\n",
    "        print(f\"[WARN] Missing feats or index for {model_tag} at {emb_dir}; \"\n",
    "              f\"looked for {FEATS_PRIMARY}\"\n",
    "              f\"{' or ' + FEATS_ALT_LAST if model_tag=='metabofm' else ''} and {INDEX_FILE}. Skipping.\")\n",
    "        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    # Optional: let the logs show which file was used\n",
    "    if os.path.basename(feats_path) == FEATS_ALT_LAST:\n",
    "        print(f\"[INFO] ({model_tag}) Using LAST embeddings: {feats_path}\")\n",
    "    else:\n",
    "        print(f\"[INFO] ({model_tag}) Using BEST embeddings: {feats_path}\")\n",
    "\n",
    "    emb = np.load(feats_path)\n",
    "    index = pd.read_csv(index_path)      # must include column 'sample_path'\n",
    "    index = index.reset_index().rename(columns={\"index\": \"row_id\"})\n",
    "\n",
    "    # Join metadata to index\n",
    "    df_base = df_meta.copy()\n",
    "    df_emb = df_base.merge(index, on=\"sample_path\", how=\"inner\")\n",
    "    df_emb = df_emb.sort_values(\"row_id\").reset_index(drop=True)\n",
    "\n",
    "    if emb.shape[0] != len(df_emb):\n",
    "        print(f\"[INFO] ({model_tag}) Embedding count != joined rows; aligning to matched rows only.\")\n",
    "        emb = emb[df_emb[\"row_id\"].values, :]\n",
    "    if emb.shape[0] != len(df_emb):\n",
    "        raise RuntimeError(f\"({model_tag}) Embedding count and joined rows mismatch after alignment.\")\n",
    "\n",
    "    # Optional PCA\n",
    "    if PCA_DIM is not None and PCA_DIM > 0 and PCA_DIM < emb.shape[1]:\n",
    "        print(f\"[INFO] ({model_tag}) Reducing dim to {PCA_DIM} via PCA.\")\n",
    "        from sklearn.decomposition import PCA\n",
    "        pca = PCA(n_components=PCA_DIM, random_state=SEED)\n",
    "        emb = pca.fit_transform(emb)\n",
    "\n",
    "    df_emb[\"row_pos\"] = np.arange(len(df_emb), dtype=int)\n",
    "\n",
    "    # ---- Main evaluation (All-train or FEW_SHOT_TRAIN) ----\n",
    "    results = []\n",
    "    pcs_rows = []   # per-class F1 (linear probe) rows\n",
    "    kgrid_rows = [] # k-sweep rows\n",
    "    for yname in tqdm(TASKS, desc=f\"Tasks[{model_tag}]\"):\n",
    "        if yname not in df_emb.columns:\n",
    "            print(f\"[WARN] ({model_tag}) Missing column {yname}; skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n==== [{model_tag}] Task: {yname} ====\")\n",
    "        df_task = filter_valid(df_emb, yname, min_count=MIN_CLASS_COUNT)\n",
    "        if df_task.empty:\n",
    "            print(f\"[WARN] ({model_tag}) Skipping {yname}: no data after filtering.\")\n",
    "            continue\n",
    "        if df_task[\"split\"].isna().any():\n",
    "            df_task = df_task[~df_task[\"split\"].isna()].copy()\n",
    "\n",
    "        row_pos = df_task[\"row_pos\"].values\n",
    "        X = emb[row_pos]\n",
    "        y = df_task[yname].astype(str).values\n",
    "        m_tr_all = get_mask(df_task, \"train\")\n",
    "        m_va     = get_mask(df_task, \"val\")\n",
    "        m_te     = get_mask(df_task, \"test\")\n",
    "\n",
    "        def _ok(mask):\n",
    "            return np.sum(mask) > 0 and (len(np.unique(y[mask])) > 1)\n",
    "\n",
    "        # ---- Few-shot sweep (uses val for C tuning; test for final) ----\n",
    "        for shots in FEW_SHOT_GRID:\n",
    "            m_tr = few_shot_subset(df_task, yname, shots_per_class=shots)\n",
    "            if not (_ok(m_tr) and _ok(m_va) and _ok(m_te)):\n",
    "                print(f\"[WARN] ({model_tag}) {yname} few-shot={shots}: insufficient classes.\")\n",
    "                continue\n",
    "\n",
    "            # Linear probe (C grid on val)\n",
    "            acc_lp, f1_lp, pred_lp, best_lp = run_linear_probe(\n",
    "                X[m_tr], y[m_tr], X[m_va], y[m_va], X[m_te], y[m_te]\n",
    "            )\n",
    "            # k-NN with default K_FOR_KNN\n",
    "            acc_knn, f1_knn, pred_knn, _, k_eff = run_knn(\n",
    "                X[m_tr], y[m_tr], X[m_te], y[m_te], k=K_FOR_KNN\n",
    "            )\n",
    "\n",
    "            results.append({\n",
    "                \"model\": model_tag,\n",
    "                \"task\": yname,\n",
    "                \"shots_per_class\": shots if shots else 0,\n",
    "                \"test_acc_linear\": acc_lp,\n",
    "                \"test_macroF1_linear\": f1_lp,\n",
    "                \"test_acc_knn\": acc_knn,\n",
    "                \"test_macroF1_knn\": f1_knn,\n",
    "                \"k_for_knn\": int(k_eff),\n",
    "                \"pca_dim\": PCA_DIM if PCA_DIM else X.shape[1],\n",
    "                \"n_train\": int(m_tr.sum()),\n",
    "                \"n_val\": int(m_va.sum()),\n",
    "                \"n_test\": int(m_te.sum()),\n",
    "                \"n_classes\": int(len(np.unique(y)))\n",
    "            })\n",
    "\n",
    "            # Per-class F1 (only for All-train to keep size modest)\n",
    "            if shots is None:\n",
    "                pcs = per_class_f1(y[m_te], pred_lp)\n",
    "                for cls, f1v in pcs.items():\n",
    "                    pcs_rows.append({\n",
    "                        \"model\": model_tag,\n",
    "                        \"task\": yname,\n",
    "                        \"label\": cls,\n",
    "                        \"f1\": f1v,\n",
    "                        \"shots_per_class\": 0  # 0 = All-train\n",
    "                    })\n",
    "\n",
    "            # k-sweep on All-train only\n",
    "            if shots is None:\n",
    "                for k in K_GRID:\n",
    "                    acc_k, f1_k, _, _, k_eff = run_knn(X[m_tr], y[m_tr], X[m_te], y[m_te], k=k)\n",
    "                    if np.isnan(f1_k):\n",
    "                        continue\n",
    "                    kgrid_rows.append({\n",
    "                        \"model\": model_tag,\n",
    "                        \"task\": yname,\n",
    "                        \"k\": int(k_eff),\n",
    "                        \"test_macroF1_knn\": f1_k\n",
    "                    })\n",
    "\n",
    "    res_df = pd.DataFrame(results).sort_values([\"task\", \"model\", \"shots_per_class\"])\n",
    "    pcs_df = pd.DataFrame(pcs_rows).sort_values([\"task\", \"model\", \"label\"])\n",
    "    kgrid_df = pd.DataFrame(kgrid_rows).sort_values([\"task\", \"model\", \"k\"])\n",
    "\n",
    "    # --------- SAVE: per-model to its corresponding location ---------\n",
    "    primary_out_dir = resolve_model_outdir(model_tag, emb_dir)\n",
    "    os.makedirs(primary_out_dir, exist_ok=True)\n",
    "\n",
    "    res_path = os.path.join(primary_out_dir, \"downstream_results.csv\")\n",
    "    res_df.to_csv(res_path, index=False)\n",
    "    if len(pcs_df):\n",
    "        pcs_path = os.path.join(primary_out_dir, \"per_class_f1_linear.csv\")\n",
    "        pcs_df.to_csv(pcs_path, index=False)\n",
    "    if len(kgrid_df):\n",
    "        ks_path = os.path.join(primary_out_dir, \"knn_k_sweep.csv\")\n",
    "        kgrid_df.to_csv(ks_path, index=False)\n",
    "\n",
    "    print(f\"[OK] ({model_tag}) Saved per-model results to: {primary_out_dir}\")\n",
    "\n",
    "    # Also mirror into BENCH_OUT/model_tag for centralized copy\n",
    "    mirror_dir = os.path.join(BENCH_OUT, model_tag)\n",
    "    os.makedirs(mirror_dir, exist_ok=True)\n",
    "    res_df.to_csv(os.path.join(mirror_dir, \"downstream_results.csv\"), index=False)\n",
    "    if len(pcs_df):\n",
    "        pcs_df.to_csv(os.path.join(mirror_dir, \"per_class_f1_linear.csv\"), index=False)\n",
    "    if len(kgrid_df):\n",
    "        kgrid_df.to_csv(os.path.join(mirror_dir, \"knn_k_sweep.csv\"), index=False)\n",
    "\n",
    "    return res_df, pcs_df, kgrid_df\n",
    "\n",
    "# -------------------------\n",
    "# Run all baselines + FM and aggregate (with skip guards)\n",
    "# -------------------------\n",
    "all_main, all_pcs, all_ks = [], [], []\n",
    "for tag, emb_dir in EMB_SETS.items():\n",
    "    # 1) Only run if embeddings are present (for baselines)\n",
    "    if ONLY_EVAL_EXISTING and not embeddings_ready(tag, emb_dir):\n",
    "        print(f\"[SKIP] '{tag}': embeddings missing at {emb_dir} \"\n",
    "            f\"(need {FEATS_PRIMARY} or {FEATS_ALT_LAST} for metabofm, plus {INDEX_FILE}); skipping.\")\n",
    "        continue\n",
    "\n",
    "\n",
    "    # 2) Skip if results already exist (unless inputs are newer or you force)\n",
    "    if SKIP_IF_RESULTS_EXIST and results_already_done(tag, emb_dir) and not FORCE_REEVAL:\n",
    "        if inputs_newer_than_results(tag, emb_dir):\n",
    "            print(f\"[RERUN] '{tag}': inputs newer than results; re-evaluating.\")\n",
    "        else:\n",
    "            print(f\"[SKIP] '{tag}': results already exist and are up-to-date.\")\n",
    "            # Load existing and append to aggregates (and ensure mirror exists)\n",
    "            try:\n",
    "                primary_out_dir = resolve_model_outdir(tag, emb_dir)\n",
    "                res_df = pd.read_csv(os.path.join(primary_out_dir, \"downstream_results.csv\"))\n",
    "                all_main.append(res_df)\n",
    "\n",
    "                pcs_path = os.path.join(primary_out_dir, \"per_class_f1_linear.csv\")\n",
    "                if os.path.exists(pcs_path):\n",
    "                    all_pcs.append(pd.read_csv(pcs_path))\n",
    "                ks_path = os.path.join(primary_out_dir, \"knn_k_sweep.csv\")\n",
    "                if os.path.exists(ks_path):\n",
    "                    all_ks.append(pd.read_csv(ks_path))\n",
    "\n",
    "                # Mirror to BENCH_OUT/tag if not already mirrored\n",
    "                mirror_dir = os.path.join(BENCH_OUT, tag)\n",
    "                os.makedirs(mirror_dir, exist_ok=True)\n",
    "                res_df.to_csv(os.path.join(mirror_dir, \"downstream_results.csv\"), index=False)\n",
    "                if os.path.exists(pcs_path):\n",
    "                    pd.read_csv(pcs_path).to_csv(os.path.join(mirror_dir, \"per_class_f1_linear.csv\"), index=False)\n",
    "                if os.path.exists(ks_path):\n",
    "                    pd.read_csv(ks_path).to_csv(os.path.join(mirror_dir, \"knn_k_sweep.csv\"), index=False)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Failed to load existing results for '{tag}': {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"\\n[RUN] Evaluating model '{tag}' from {emb_dir}\")\n",
    "    res_df, pcs_df, kgrid_df = evaluate_embeddings(emb_dir, tag)\n",
    "    if len(res_df):  all_main.append(res_df)\n",
    "    if len(pcs_df):  all_pcs.append(pcs_df)\n",
    "    if len(kgrid_df): all_ks.append(kgrid_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c5d175",
   "metadata": {},
   "source": [
    "### Few-shot Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4ca718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Loaded models:\n",
      " - PCA\n",
      "    main: fm_ssl_run\\pretrained_feats2\\pixels_fast256\\downstream_results.csv\n",
      "    pcs : fm_ssl_run\\pretrained_feats2\\pixels_fast256\\per_class_f1_linear.csv\n",
      "    k   : fm_ssl_run\\pretrained_feats2\\pixels_fast256\\knn_k_sweep.csv\n",
      " - DeiT distilled (Random)\n",
      "    main: fm_ssl_run\\pretrained_feats2\\imagenet_deit_b16_random\\downstream_results.csv\n",
      "    pcs : fm_ssl_run\\pretrained_feats2\\imagenet_deit_b16_random\\per_class_f1_linear.csv\n",
      "    k   : fm_ssl_run\\pretrained_feats2\\imagenet_deit_b16_random\\knn_k_sweep.csv\n",
      " - Dinov2-VIT14 (Random)\n",
      "    main: fm_ssl_run\\pretrained_feats2\\dinov2_vitb14_random\\downstream_results.csv\n",
      "    pcs : fm_ssl_run\\pretrained_feats2\\dinov2_vitb14_random\\per_class_f1_linear.csv\n",
      "    k   : fm_ssl_run\\pretrained_feats2\\dinov2_vitb14_random\\knn_k_sweep.csv\n",
      " - MAE-VIT16 (Random)\n",
      "    main: fm_ssl_run\\pretrained_feats2\\mae_vitb16_random\\downstream_results.csv\n",
      "    pcs : fm_ssl_run\\pretrained_feats2\\mae_vitb16_random\\per_class_f1_linear.csv\n",
      "    k   : fm_ssl_run\\pretrained_feats2\\mae_vitb16_random\\knn_k_sweep.csv\n",
      " - MAE-VIT16 (ImageNet)\n",
      "    main: fm_ssl_run\\pretrained_feats2\\mae_vitb16\\downstream_results.csv\n",
      "    pcs : fm_ssl_run\\pretrained_feats2\\mae_vitb16\\per_class_f1_linear.csv\n",
      "    k   : fm_ssl_run\\pretrained_feats2\\mae_vitb16\\knn_k_sweep.csv\n",
      " - DeiT distilled (ImageNet)\n",
      "    main: fm_ssl_run\\pretrained_feats2\\imagenet_deit_b16\\downstream_results.csv\n",
      "    pcs : fm_ssl_run\\pretrained_feats2\\imagenet_deit_b16\\per_class_f1_linear.csv\n",
      "    k   : fm_ssl_run\\pretrained_feats2\\imagenet_deit_b16\\knn_k_sweep.csv\n",
      " - Dinov2-VIT14 (LVD-142M)\n",
      "    main: fm_ssl_run\\pretrained_feats2\\dinov2_vitb14\\downstream_results.csv\n",
      "    pcs : fm_ssl_run\\pretrained_feats2\\dinov2_vitb14\\per_class_f1_linear.csv\n",
      "    k   : fm_ssl_run\\pretrained_feats2\\dinov2_vitb14\\knn_k_sweep.csv\n",
      "[OK] Few-shot per-task barplots (Linear & k-NN) saved to: fm_ssl_run\\baseline_eval_combined2\\plots\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_68520\\2247670692.py:362: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(y, x_norm) / denom\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_68520\\2247670692.py:362: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(y, x_norm) / denom\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_68520\\2247670692.py:362: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(y, x_norm) / denom\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_68520\\2247670692.py:362: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(y, x_norm) / denom\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_68520\\2247670692.py:362: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(y, x_norm) / denom\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_68520\\2247670692.py:362: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(y, x_norm) / denom\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_68520\\2247670692.py:362: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(y, x_norm) / denom\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_68520\\2247670692.py:362: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(y, x_norm) / denom\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_68520\\2247670692.py:362: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(y, x_norm) / denom\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_68520\\2247670692.py:362: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(y, x_norm) / denom\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_68520\\2247670692.py:362: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(y, x_norm) / denom\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_68520\\2247670692.py:362: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(y, x_norm) / denom\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_68520\\2247670692.py:362: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(y, x_norm) / denom\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_68520\\2247670692.py:362: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(y, x_norm) / denom\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_68520\\2247670692.py:362: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(y, x_norm) / denom\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_68520\\2247670692.py:362: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(y, x_norm) / denom\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_68520\\2247670692.py:362: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(y, x_norm) / denom\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_68520\\2247670692.py:362: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(y, x_norm) / denom\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_68520\\2247670692.py:362: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(y, x_norm) / denom\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_68520\\2247670692.py:362: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(y, x_norm) / denom\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_68520\\2247670692.py:362: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(y, x_norm) / denom\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_68520\\2247670692.py:362: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(y, x_norm) / denom\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_68520\\2247670692.py:362: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(y, x_norm) / denom\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_68520\\2247670692.py:362: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(y, x_norm) / denom\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_68520\\2247670692.py:362: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(y, x_norm) / denom\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_68520\\2247670692.py:362: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(y, x_norm) / denom\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_68520\\2247670692.py:362: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(y, x_norm) / denom\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_68520\\2247670692.py:362: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(y, x_norm) / denom\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_68520\\2247670692.py:362: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(y, x_norm) / denom\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_68520\\2247670692.py:362: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(y, x_norm) / denom\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_68520\\2247670692.py:362: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(y, x_norm) / denom\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_68520\\2247670692.py:362: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(y, x_norm) / denom\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_68520\\2247670692.py:362: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(y, x_norm) / denom\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_68520\\2247670692.py:362: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(y, x_norm) / denom\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_68520\\2247670692.py:362: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(y, x_norm) / denom\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_68520\\2247670692.py:362: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(y, x_norm) / denom\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_68520\\2247670692.py:362: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(y, x_norm) / denom\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_68520\\2247670692.py:362: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(y, x_norm) / denom\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_68520\\2247670692.py:362: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(y, x_norm) / denom\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_68520\\2247670692.py:362: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(y, x_norm) / denom\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_68520\\2247670692.py:362: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(y, x_norm) / denom\n",
      "C:\\Users\\eozturk7\\AppData\\Local\\Temp\\ipykernel_68520\\2247670692.py:362: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(y, x_norm) / denom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Macro-F1 (TEST, Linear, All-train) by task & model ===\n",
      "model             DeiT distilled (ImageNet)  DeiT distilled (Random)  Dinov2-VIT14 (LVD-142M)  Dinov2-VIT14 (Random)  MAE-VIT16 (ImageNet)  MAE-VIT16 (Random)    PCA\n",
      "task                                                                                                                                                                 \n",
      "Condition                             0.579                    0.406                    0.637                  0.186                 0.546               0.433  0.296\n",
      "Organism_Part                         0.692                    0.469                    0.684                  0.256                 0.653               0.436  0.334\n",
      "analyzerType                          0.774                    0.593                    0.799                  0.480                 0.779               0.593  0.467\n",
      "ionisationSource                      0.783                    0.562                    0.725                  0.406                 0.720               0.558  0.436\n",
      "organism                              0.793                    0.718                    0.782                  0.573                 0.818               0.733  0.709\n",
      "polarity                              0.802                    0.650                    0.821                  0.623                 0.791               0.652  0.603\n",
      "\n",
      "[OK] Combined plots + CSVs saved under: fm_ssl_run\\baseline_eval_combined2\\plots\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# DOWNSTREAM EVALUATION ON HELD-OUT SPLITS (multi-baseline)\n",
    "# - Canonicalize/merge label variants\n",
    "# - Evaluate many embedding sets with linear probe & k-NN\n",
    "# - Extras: Few-shot curves, k-NN sensitivity, label-efficiency AUC, win-rate, k-robustness\n",
    "# - Save per-model results + centralized mirrors + combined plots\n",
    "# - Skip guards to avoid recomputing existing baselines\n",
    "# - NEW: Save CSV backing each plot\n",
    "# =====================================================\n",
    "\n",
    "import os, copy, re, glob, json, hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "SEED = 6740\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# -------------------------\n",
    "# Speed / behavior knobs\n",
    "# -------------------------\n",
    "FAST_MODE = True                       # flip False for full evaluation\n",
    "FEW_SHOT_GRID = [1, 5, 10, 25, None]   # None means \"All train\"\n",
    "K_GRID = [1, 5, 10, 20, 50]\n",
    "K_FOR_KNN = 10 if FAST_MODE else 20\n",
    "C_GRID = [0.1, 1.0] if FAST_MODE else [0.01, 0.1, 0.5, 1.0, 2.0, 5.0]\n",
    "MAX_ITER_LR = 800 if FAST_MODE else 2000\n",
    "MIN_CLASS_COUNT = 100 if FAST_MODE else 50\n",
    "PCA_DIM = None                         # e.g., 256 to speed up; None disables\n",
    "METABOFM_VERSION = \"last\"              # \"best\" or \"last\"\n",
    "\n",
    "# -------------------------\n",
    "# Skips & guards\n",
    "# -------------------------\n",
    "ONLY_EVAL_EXISTING = True     # only run models that already have embeddings saved\n",
    "SKIP_IF_RESULTS_EXIST = True  # don't recompute per-model results if CSVs already exist\n",
    "FORCE_REEVAL = False          # set True to ignore SKIP_IF_RESULTS_EXIST\n",
    "\n",
    "FEATS_PRIMARY = \"image_feats.npy\"\n",
    "FEATS_ALT_LAST = \"image_feats_last.npy\"\n",
    "INDEX_FILE = \"index.csv\"\n",
    "REQUIRED_INDEX = INDEX_FILE\n",
    "\n",
    "# -------------------------\n",
    "\n",
    "COMBINED_BENCH_OUT = os.path.join(\"fm_ssl_run\", \"baseline_eval_combined2\")\n",
    "COMBINED_PLOTS_OUT = os.path.join(COMBINED_BENCH_OUT, \"plots\")\n",
    "os.makedirs(COMBINED_PLOTS_OUT, exist_ok=True)\n",
    "\n",
    "# Keep same task list\n",
    "TASKS = [\"organism\", \"polarity\", \"Organism_Part\", \"Condition\", \"analyzerType\", \"ionisationSource\"]\n",
    "\n",
    "# Off-the-shelf baselines to include in combined plots\n",
    "BASELINE_EMB_SETS = {\n",
    "    \"PCA\":                         os.path.join(\"fm_ssl_run\", \"pretrained_feats2\", \"pixels_fast256\"),\n",
    "    \"DeiT distilled (Random)\":    os.path.join(\"fm_ssl_run\", \"pretrained_feats2\", \"imagenet_deit_b16_random\"),\n",
    "    \"Dinov2-VIT14 (Random)\":      os.path.join(\"fm_ssl_run\", \"pretrained_feats2\", \"dinov2_vitb14_random\"),\n",
    "    \"MAE-VIT16 (Random)\":         os.path.join(\"fm_ssl_run\", \"pretrained_feats2\", \"mae_vitb16_random\"),\n",
    "    \"MAE-VIT16 (ImageNet)\":       os.path.join(\"fm_ssl_run\", \"pretrained_feats2\", \"mae_vitb16\"),\n",
    "    \"DeiT distilled (ImageNet)\":  os.path.join(\"fm_ssl_run\", \"pretrained_feats2\", \"imagenet_deit_b16\"),\n",
    "    \"Dinov2-VIT14 (LVD-142M)\":    os.path.join(\"fm_ssl_run\", \"pretrained_feats2\", \"dinov2_vitb14\"),\n",
    "}\n",
    "\n",
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "def save_df_csv(df: pd.DataFrame, path: str):\n",
    "    \"\"\"Safely save dataframe to CSV (and ensure dir).\"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    df.to_csv(path, index=False)\n",
    "\n",
    "def load_first_existing_csv(paths):\n",
    "    for p in paths:\n",
    "        if p and os.path.exists(p):\n",
    "            return pd.read_csv(p), p\n",
    "    return None, None\n",
    "\n",
    "def read_vit_name_from_run(run_dir: str) -> str:\n",
    "    rp = os.path.join(run_dir, \"run_params.json\")\n",
    "    if os.path.exists(rp):\n",
    "        try:\n",
    "            with open(rp, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            if \"cfg\" in data and isinstance(data[\"cfg\"], dict):\n",
    "                vit = data[\"cfg\"].get(\"vit_name\", None)\n",
    "                if vit:\n",
    "                    return str(vit)\n",
    "            vit = data.get(\"vit_name\", None)\n",
    "            if vit:\n",
    "                return str(vit)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed reading vit_name from {rp}: {e}\")\n",
    "    return os.path.basename(os.path.normpath(run_dir))\n",
    "\n",
    "def make_unique_name(base_name: str, existing: set, disambig_hint: str) -> str:\n",
    "    name = base_name\n",
    "    if name not in existing:\n",
    "        return name\n",
    "    short = disambig_hint.replace(\"\\\\\", \"/\").strip(\"/\").split(\"/\")[-1]\n",
    "    suffix = f\" ({short})\"\n",
    "    if name + suffix not in existing:\n",
    "        return name + suffix\n",
    "    tiny = hashlib.md5(disambig_hint.encode(\"utf-8\")).hexdigest()[:6]\n",
    "    alt = f\"{name} [{tiny}]\"\n",
    "    if alt not in existing:\n",
    "        return alt\n",
    "    i = 2\n",
    "    while True:\n",
    "        cand = f\"{alt}-{i}\"\n",
    "        if cand not in existing:\n",
    "            return cand\n",
    "        i += 1\n",
    "\n",
    "def primary_results_dir_for_fm_run(run_dir: str) -> str:\n",
    "    return run_dir\n",
    "\n",
    "def primary_results_dir_for_baseline(baseline_emb_dir: str) -> str:\n",
    "    return baseline_emb_dir\n",
    "\n",
    "# -------------------------\n",
    "# Build the model list (baselines)\n",
    "# -------------------------\n",
    "model_entries = []\n",
    "seen_names = set()\n",
    "\n",
    "# Add baselines\n",
    "for tag, emb_dir in BASELINE_EMB_SETS.items():\n",
    "    disp = make_unique_name(tag, seen_names, disambig_hint=emb_dir)\n",
    "    seen_names.add(disp)\n",
    "    model_entries.append({\n",
    "        \"name\": disp,\n",
    "        \"kind\": \"baseline\",\n",
    "        \"primary_dir\": primary_results_dir_for_baseline(emb_dir),\n",
    "        \"mirror_dir\": os.path.join(COMBINED_BENCH_OUT, disp.replace(os.sep, \"_\")),\n",
    "    })\n",
    "\n",
    "# -------------------------\n",
    "# Load per-model CSVs\n",
    "# -------------------------\n",
    "all_main, all_pcs, all_ks = [], [], []\n",
    "loaded = []\n",
    "\n",
    "for m in model_entries:\n",
    "    name = m[\"name\"]\n",
    "    primary_dir = m[\"primary_dir\"]\n",
    "    mirror_dir  = m[\"mirror_dir\"]\n",
    "\n",
    "    main_paths = [\n",
    "        os.path.join(primary_dir, \"downstream_results.csv\"),\n",
    "        os.path.join(mirror_dir,  \"downstream_results.csv\"),\n",
    "    ]\n",
    "    pcs_paths = [\n",
    "        os.path.join(primary_dir, \"per_class_f1_linear.csv\"),\n",
    "        os.path.join(mirror_dir,  \"per_class_f1_linear.csv\"),\n",
    "    ]\n",
    "    ks_paths = [\n",
    "        os.path.join(primary_dir, \"knn_k_sweep.csv\"),\n",
    "        os.path.join(mirror_dir,  \"knn_k_sweep.csv\"),\n",
    "    ]\n",
    "\n",
    "    df_main, mp = load_first_existing_csv(main_paths)\n",
    "    if df_main is None:\n",
    "        print(f\"[WARN] No downstream_results.csv for '{name}'. Skipping this model.\")\n",
    "        continue\n",
    "\n",
    "    df_pcs, pp = load_first_existing_csv(pcs_paths)\n",
    "    df_ks,  kp = load_first_existing_csv(ks_paths)\n",
    "\n",
    "    required_cols = {\"model\",\"task\",\"shots_per_class\",\"test_macroF1_linear\",\"test_macroF1_knn\"}\n",
    "    missing = required_cols.difference(set(df_main.columns) | {\"model\"})  # model will be added below\n",
    "    if missing:\n",
    "        raise ValueError(f\"[ERR] '{name}' main CSV missing columns: {missing} at {mp}\")\n",
    "\n",
    "    df_main = df_main.copy()\n",
    "    df_main[\"model\"] = name\n",
    "\n",
    "    all_main.append(df_main)\n",
    "    if df_pcs is not None:\n",
    "        df_pcs = df_pcs.copy()\n",
    "        df_pcs[\"model\"] = name\n",
    "        all_pcs.append(df_pcs)\n",
    "    if df_ks is not None:\n",
    "        if \"k\" not in df_ks.columns or \"test_macroF1_knn\" not in df_ks.columns:\n",
    "            print(f\"[WARN] k-sweep CSV for '{name}' missing expected columns; skipping.\")\n",
    "        else:\n",
    "            df_ks = df_ks.copy()\n",
    "            df_ks[\"model\"] = name\n",
    "            all_ks.append(df_ks)\n",
    "\n",
    "    loaded.append((name, mp, pp, kp))\n",
    "\n",
    "if not all_main:\n",
    "    raise SystemExit(\"[ERR] No per-model CSVs could be loaded.\")\n",
    "\n",
    "bench   = pd.concat(all_main, ignore_index=True)\n",
    "pcs_all = pd.concat(all_pcs, ignore_index=True) if all_pcs else pd.DataFrame()\n",
    "ks_all  = pd.concat(all_ks,  ignore_index=True) if all_ks  else pd.DataFrame()\n",
    "\n",
    "# Save the unified CSVs too\n",
    "save_df_csv(bench,   os.path.join(COMBINED_PLOTS_OUT, \"ALL_downstream_results_merged.csv\"))\n",
    "if len(pcs_all):\n",
    "    save_df_csv(pcs_all, os.path.join(COMBINED_PLOTS_OUT, \"ALL_per_class_f1_linear_merged.csv\"))\n",
    "if len(ks_all):\n",
    "    save_df_csv(ks_all,  os.path.join(COMBINED_PLOTS_OUT, \"ALL_knn_k_sweep_merged.csv\"))\n",
    "\n",
    "print(\"[OK] Loaded models:\")\n",
    "for name, mp, pp, kp in loaded:\n",
    "    print(f\" - {name}\\n    main: {mp}\\n    pcs : {pp or '—'}\\n    k   : {kp or '—'}\")\n",
    "\n",
    "# -------------------------\n",
    "# PLOTS (standard + narrative-pivot extras)\n",
    "# -------------------------\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# 1) Macro-F1 by task & model (Linear Probe) — barplot (All-train only)\n",
    "df_bar = bench[bench[\"shots_per_class\"] == 0].copy()\n",
    "if len(df_bar):\n",
    "    # Save raw plot data\n",
    "    save_df_csv(df_bar, os.path.join(COMBINED_PLOTS_OUT, \"PLOT_DATA_bar_macroF1_linear_alltrain_raw.csv\"))\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    order_tasks = [t for t in TASKS if t in df_bar[\"task\"].unique()]\n",
    "    sns.barplot(\n",
    "        data=df_bar, x=\"task\", y=\"test_macroF1_linear\", hue=\"model\",\n",
    "        order=order_tasks, errorbar=\"ci\", dodge=True\n",
    "    )\n",
    "    plt.title(\"Macro-F1 (Linear Probe, TEST) by Task & Model (All-train)\")\n",
    "    plt.ylabel(\"Macro-F1\")\n",
    "    plt.xlabel(\"Task\")\n",
    "    plt.xticks(rotation=30, ha=\"right\")\n",
    "    plt.legend(bbox_to_anchor=(1.02, 1), loc=\"upper left\", title=\"Model\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(COMBINED_PLOTS_OUT, \"bar_macroF1_linear_alltrain.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "# 2) Macro-F1 by task & model (k-NN default K) — barplot (All-train only)\n",
    "if len(df_bar):\n",
    "    # Save raw plot data\n",
    "    save_df_csv(df_bar, os.path.join(COMBINED_PLOTS_OUT, f\"PLOT_DATA_bar_macroF1_knn_k{K_FOR_KNN}_alltrain_raw.csv\"))\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    order_tasks = [t for t in TASKS if t in df_bar[\"task\"].unique()]\n",
    "    sns.barplot(\n",
    "        data=df_bar, x=\"task\", y=\"test_macroF1_knn\", hue=\"model\",\n",
    "        order=order_tasks, errorbar=\"ci\", dodge=True\n",
    "    )\n",
    "    plt.title(f\"Macro-F1 (k-NN@{K_FOR_KNN}, TEST) by Task & Model (All-train)\")\n",
    "    plt.ylabel(\"Macro-F1\")\n",
    "    plt.xlabel(\"Task\")\n",
    "    plt.xticks(rotation=30, ha=\"right\")\n",
    "    plt.legend(bbox_to_anchor=(1.02, 1), loc=\"upper left\", title=\"Model\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(COMBINED_PLOTS_OUT, f\"bar_macroF1_knn_k{K_FOR_KNN}_alltrain.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "# ============================================\n",
    "# Few-shot BARPLOTS per task (shots on x-axis)\n",
    "# ============================================\n",
    "\n",
    "# Metrics to plot: linear probe & k-NN (macro-F1 on TEST)\n",
    "METRICS = [\n",
    "    (\"test_macroF1_linear\", \"Linear Probe\", \"linear\"),\n",
    "    (\"test_macroF1_knn\",    \"k-NN\",         \"knn\"),\n",
    "]\n",
    "\n",
    "df_fs = bench.copy()\n",
    "if len(df_fs):\n",
    "    # Map shots_per_class (0 means All-train)\n",
    "    df_fs[\"shots_lbl\"] = df_fs[\"shots_per_class\"].replace({0: \"All\"}).astype(str)\n",
    "\n",
    "    # Desired order of bars on the x-axis\n",
    "    desired_order = [\"1\", \"5\", \"10\", \"25\", \"All\"]\n",
    "\n",
    "    # Save the global few-shot merged dataframe used for all plots\n",
    "    save_df_csv(df_fs, os.path.join(COMBINED_PLOTS_OUT, \"PLOT_DATA_bar_fewshot_ALL_metrics_RAW.csv\"))\n",
    "\n",
    "    for metric_col, metric_title, metric_key in METRICS:\n",
    "        if metric_col not in df_fs.columns:\n",
    "            print(f\"[WARN] Metric '{metric_col}' missing in bench; skipping all tasks for this metric.\")\n",
    "            continue\n",
    "\n",
    "        for tsk in TASKS:\n",
    "            cur = df_fs[df_fs[\"task\"] == tsk].copy()\n",
    "            if cur.empty:\n",
    "                continue\n",
    "\n",
    "            # Keep only present shot labels but preserve canonical order\n",
    "            present = [lab for lab in desired_order if lab in set(cur[\"shots_lbl\"])]\n",
    "            if not present:\n",
    "                print(f\"[WARN] No few-shot settings present for task '{tsk}'. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            cur[\"shots_lbl\"] = pd.Categorical(cur[\"shots_lbl\"], categories=present, ordered=True)\n",
    "\n",
    "            # Save per-task/metric plot data\n",
    "            out_csv = os.path.join(COMBINED_PLOTS_OUT, f\"PLOT_DATA_bar_fewshot_{metric_key}_{tsk}.csv\")\n",
    "            save_df_csv(cur[[\"model\",\"task\",\"shots_per_class\",\"shots_lbl\",metric_col]], out_csv)\n",
    "\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            sns.barplot(\n",
    "                data=cur,\n",
    "                x=\"shots_lbl\", y=metric_col, hue=\"model\",\n",
    "                order=present, dodge=True, errorbar=\"ci\"\n",
    "            )\n",
    "            plt.title(f\"Few-shot Macro-F1 ({metric_title}) — Task: {tsk}\")\n",
    "            plt.ylabel(\"Macro-F1 (TEST)\")\n",
    "            plt.xlabel(\"Shots per class\")\n",
    "            plt.xticks(rotation=0)\n",
    "            plt.legend(bbox_to_anchor=(1.02, 1), loc=\"upper left\", title=\"Model\")\n",
    "            plt.tight_layout()\n",
    "\n",
    "            out_name = f\"bar_fewshot_{metric_key}_{tsk}.png\"\n",
    "            plt.savefig(os.path.join(COMBINED_PLOTS_OUT, out_name), dpi=200)\n",
    "            plt.close()\n",
    "\n",
    "print(f\"[OK] Few-shot per-task barplots (Linear & k-NN) saved to: {COMBINED_PLOTS_OUT}\")\n",
    "\n",
    "# --- Label-efficiency AUC (Linear) across shots per task ---\n",
    "shot_x_map = {1:1, 5:5, 10:10, 25:25, 0:50}  # 0 (\"All\") -> large x\n",
    "df_auc = []\n",
    "for (model, task), g in df_fs.groupby([\"model\", \"task\"]):\n",
    "    gg = g.copy()\n",
    "    gg[\"x\"] = gg[\"shots_per_class\"].map(shot_x_map)\n",
    "    gg = gg.dropna(subset=[\"x\", \"test_macroF1_linear\"]).sort_values(\"x\")\n",
    "    if gg[\"x\"].nunique() >= 2:\n",
    "        x = gg[\"x\"].values.astype(float)\n",
    "        y = gg[\"test_macroF1_linear\"].values.astype(float)\n",
    "        x_norm = (x - x.min()) / (x.max() - x.min())\n",
    "        # Guard against zero division (shouldn't happen because nunique>=2)\n",
    "        denom = (x_norm.max() - x_norm.min()) if (x_norm.max() - x_norm.min()) > 0 else 1.0\n",
    "        auc = np.trapz(y, x_norm) / denom\n",
    "        df_auc.append({\"model\": model, \"task\": task, \"auc_linear\": auc})\n",
    "df_auc = pd.DataFrame(df_auc)\n",
    "\n",
    "if len(df_auc):\n",
    "    # Save per-task AUC values\n",
    "    save_df_csv(df_auc, os.path.join(COMBINED_PLOTS_OUT, \"PLOT_DATA_label_efficiency_auc_linear_by_task.csv\"))\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    auc_bar = df_auc.groupby(\"model\", as_index=False)[\"auc_linear\"].mean()\n",
    "    # Save aggregated AUC means per model\n",
    "    save_df_csv(auc_bar, os.path.join(COMBINED_PLOTS_OUT, \"PLOT_DATA_label_efficiency_auc_linear_mean_over_tasks.csv\"))\n",
    "\n",
    "    sns.barplot(data=auc_bar, x=\"model\", y=\"auc_linear\")\n",
    "    plt.title(\"Label-Efficiency AUC (Linear Probe) — Mean over tasks\")\n",
    "    plt.ylabel(\"AUC (0–1)\"); plt.xlabel(\"\")\n",
    "    plt.xticks(rotation=20, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(COMBINED_PLOTS_OUT, \"bar_label_efficiency_auc_linear.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "# --- Best-of (Linear vs k-NN) at All-train ---\n",
    "df_alltrain = bench[bench[\"shots_per_class\"] == 0].copy()\n",
    "if len(df_alltrain):\n",
    "    df_alltrain[\"best_macroF1\"] = df_alltrain[[\"test_macroF1_linear\", \"test_macroF1_knn\"]].max(axis=1)\n",
    "    # Save raw + best-of\n",
    "    save_df_csv(df_alltrain, os.path.join(COMBINED_PLOTS_OUT, \"PLOT_DATA_bestof_alltrain_raw.csv\"))\n",
    "    save_df_csv(df_alltrain[[\"model\",\"task\",\"best_macroF1\"]], os.path.join(COMBINED_PLOTS_OUT, \"PLOT_DATA_bestof_alltrain_compact.csv\"))\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    order_tasks = [t for t in TASKS if t in df_alltrain[\"task\"].unique()]\n",
    "    sns.barplot(\n",
    "        data=df_alltrain, x=\"task\", y=\"best_macroF1\", hue=\"model\",\n",
    "        order=order_tasks, errorbar=\"ci\", dodge=True\n",
    "    )\n",
    "    plt.title(\"Best-of (Linear or k-NN) Macro-F1 — TEST, All-train\")\n",
    "    plt.ylabel(\"Macro-F1\"); plt.xlabel(\"Task\")\n",
    "    plt.xticks(rotation=30, ha=\"right\")\n",
    "    plt.legend(bbox_to_anchor=(1.02, 1), loc=\"upper left\", title=\"Model\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(COMBINED_PLOTS_OUT, \"bar_bestof_alltrain.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "# --- Win-rate across tasks (All-train, Linear) ---\n",
    "if len(df_alltrain):\n",
    "    wr_rows = []\n",
    "    for tsk, g in df_alltrain.groupby(\"task\"):\n",
    "        g2 = g.sort_values(\"test_macroF1_linear\", ascending=False)\n",
    "        if not g2.empty:\n",
    "            top_val = g2[\"test_macroF1_linear\"].iloc[0]\n",
    "            winners = g2[g2[\"test_macroF1_linear\"] >= top_val - 1e-9][\"model\"].unique()\n",
    "            for w in winners:\n",
    "                wr_rows.append({\"model\": w, \"win_task\": tsk})\n",
    "    df_wr = pd.DataFrame(wr_rows)\n",
    "\n",
    "    if len(df_wr):\n",
    "        # Save raw win tasks + win counts\n",
    "        save_df_csv(df_wr, os.path.join(COMBINED_PLOTS_OUT, \"PLOT_DATA_winrate_tasks_linear_alltrain.csv\"))\n",
    "        winrate = df_wr.groupby(\"model\")[\"win_task\"].nunique().reset_index()\n",
    "        winrate = winrate.rename(columns={\"win_task\": \"win_count\"})\n",
    "        save_df_csv(winrate, os.path.join(COMBINED_PLOTS_OUT, \"PLOT_DATA_winrate_counts_linear_alltrain.csv\"))\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sns.barplot(data=winrate, x=\"model\", y=\"win_count\")\n",
    "        plt.title(\"Win-rate (#tasks won) — Linear, All-train\")\n",
    "        plt.ylabel(\"#Tasks won\"); plt.xlabel(\"\")\n",
    "        plt.xticks(rotation=20, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(COMBINED_PLOTS_OUT, \"bar_winrate_alltrain.png\"), dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "# --- k-robustness: lower variance across k is better ---\n",
    "if len(ks_all):\n",
    "    var_df = ks_all.groupby([\"model\", \"task\"])[\"test_macroF1_knn\"].std(ddof=0).reset_index()\n",
    "    var_df = var_df.rename(columns={\"test_macroF1_knn\": \"std_over_k\"})\n",
    "    # Save robustness data\n",
    "    save_df_csv(var_df, os.path.join(COMBINED_PLOTS_OUT, \"PLOT_DATA_knn_robustness_std_over_k.csv\"))\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=var_df, x=\"task\", y=\"std_over_k\", hue=\"model\")\n",
    "    plt.title(\"k-NN Robustness — Std of Macro-F1 over k (lower is better)\")\n",
    "    plt.ylabel(\"Std(F1) over k\"); plt.xlabel(\"Task\")\n",
    "    plt.xticks(rotation=30, ha=\"right\")\n",
    "    plt.legend(bbox_to_anchor=(1.02, 1), loc=\"upper left\", title=\"Model\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(COMBINED_PLOTS_OUT, \"bar_knn_robustness_std_over_k.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "# 5) (Optional) Table preview in stdout: Macro-F1 (Linear, All-train)\n",
    "if len(df_bar):\n",
    "    print(\"\\n=== Macro-F1 (TEST, Linear, All-train) by task & model ===\")\n",
    "    pivot = df_bar.pivot_table(index=\"task\", columns=\"model\", values=\"test_macroF1_linear\", aggfunc=\"mean\")\n",
    "    print(pivot.round(3).to_string())\n",
    "    # Save the pivot too\n",
    "    save_df_csv(pivot.reset_index(), os.path.join(COMBINED_PLOTS_OUT, \"TABLE_macroF1_linear_alltrain_pivot.csv\"))\n",
    "\n",
    "print(f\"\\n[OK] Combined plots + CSVs saved under: {COMBINED_PLOTS_OUT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2d3a92",
   "metadata": {},
   "source": [
    "\n",
    "### UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed905790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Models to visualize:\n",
      " - Dinov2-VIT14 (Random): fm_ssl_run\\pretrained_feats2\\dinov2_vitb14_random [image_feats.npy]\n",
      " - Dinov2-VIT14 (LVD-142M): fm_ssl_run\\pretrained_feats2\\dinov2_vitb14 [image_feats.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] UMAP Dinov2-VIT14 (Random): n=3938 → coords computed once\n",
      "[INFO] Dinov2-VIT14 (Random): removing 303 outliers (7.7%) from plot\n",
      "[OK] saved: fm_ssl_run\\baseline_eval_combined2\\umaps\\umap_Dinov2-VIT14 (Random)_organism.png\n",
      "[OK] saved: fm_ssl_run\\baseline_eval_combined2\\umaps\\umap_Dinov2-VIT14 (Random)_polarity.png\n",
      "[OK] saved: fm_ssl_run\\baseline_eval_combined2\\umaps\\umap_Dinov2-VIT14 (Random)_Organism_Part.png\n",
      "[OK] saved: fm_ssl_run\\baseline_eval_combined2\\umaps\\umap_Dinov2-VIT14 (Random)_Condition.png\n",
      "[OK] saved: fm_ssl_run\\baseline_eval_combined2\\umaps\\umap_Dinov2-VIT14 (Random)_analyzerType.png\n",
      "[OK] saved: fm_ssl_run\\baseline_eval_combined2\\umaps\\umap_Dinov2-VIT14 (Random)_ionisationSource.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eozturk7\\AppData\\Local\\miniconda3\\envs\\magic\\lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] UMAP Dinov2-VIT14 (LVD-142M): n=3938 → coords computed once\n",
      "[INFO] Dinov2-VIT14 (LVD-142M): removing 287 outliers (7.3%) from plot\n",
      "[OK] saved: fm_ssl_run\\baseline_eval_combined2\\umaps\\umap_Dinov2-VIT14 (LVD-142M)_organism.png\n",
      "[OK] saved: fm_ssl_run\\baseline_eval_combined2\\umaps\\umap_Dinov2-VIT14 (LVD-142M)_polarity.png\n",
      "[OK] saved: fm_ssl_run\\baseline_eval_combined2\\umaps\\umap_Dinov2-VIT14 (LVD-142M)_Organism_Part.png\n",
      "[OK] saved: fm_ssl_run\\baseline_eval_combined2\\umaps\\umap_Dinov2-VIT14 (LVD-142M)_Condition.png\n",
      "[OK] saved: fm_ssl_run\\baseline_eval_combined2\\umaps\\umap_Dinov2-VIT14 (LVD-142M)_analyzerType.png\n",
      "[OK] saved: fm_ssl_run\\baseline_eval_combined2\\umaps\\umap_Dinov2-VIT14 (LVD-142M)_ionisationSource.png\n",
      "[OK] saved: fm_ssl_run\\baseline_eval_combined2\\umaps\\pie_organism.png\n",
      "[OK] saved: fm_ssl_run\\baseline_eval_combined2\\umaps\\pie_polarity.png\n",
      "[OK] saved: fm_ssl_run\\baseline_eval_combined2\\umaps\\pie_Organism_Part.png\n",
      "[OK] saved: fm_ssl_run\\baseline_eval_combined2\\umaps\\pie_Condition.png\n",
      "[OK] saved: fm_ssl_run\\baseline_eval_combined2\\umaps\\pie_analyzerType.png\n",
      "[OK] saved: fm_ssl_run\\baseline_eval_combined2\\umaps\\pie_ionisationSource.png\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# UMAP visualization + Global Pie Charts (colors match)\n",
    "# - One UMAP per model, recolored by multiple tasks\n",
    "# - ONE pie chart per task (not per model), with counts on slices\n",
    "# - Colors are consistent between UMAPs and pies\n",
    "# - NEW: Robust outlier removal in UMAP PLOTS (MAD/IQR/percentile)\n",
    "#   * Outliers are filtered ONLY for plotting; pies & colors unaffected\n",
    "#   * Saves both raw and filtered UMAP coords for transparency\n",
    "# =====================================================\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import umap\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "FAST_MODE = True\n",
    "MIN_CLASS_COUNT = 100 if FAST_MODE else 50\n",
    "RANDOM_SEED = 6740\n",
    "\n",
    "# Choose which FM embeddings to visualize:\n",
    "#   \"best\" -> embeddings_all/image_feats.npy\n",
    "#   \"last\" -> embeddings_all/image_feats_last.npy\n",
    "FM_EMB_VERSION = \"last\"   # options: \"best\", \"last\"\n",
    "# =========================================================================================================\n",
    "\n",
    "# Off-the-shelf baselines (embedding folders must contain image_feats.npy + index.csv)\n",
    "BASELINE_EMB_SETS = {\n",
    "    # \"PCA\":                             os.path.join(\"fm_ssl_run\", \"pretrained_feats2\", \"pixels_fast256\"),\n",
    "    # \"DeiT distilled (Random)\":         os.path.join(\"fm_ssl_run\", \"pretrained_feats2\", \"imagenet_deit_b16_random\"),\n",
    "    \"Dinov2-VIT14 (Random)\":           os.path.join(\"fm_ssl_run\", \"pretrained_feats2\", \"dinov2_vitb14_random\"),\n",
    "    # \"MAE-VIT16 (Random)\":              os.path.join(\"fm_ssl_run\", \"pretrained_feats2\", \"mae_vitb16_random\"),\n",
    "    # \"MAE-VIT16 (ImageNet)\":            os.path.join(\"fm_ssl_run\", \"pretrained_feats2\", \"mae_vitb16\"),\n",
    "    # \"DeiT distilled (ImageNet)\":       os.path.join(\"fm_ssl_run\", \"pretrained_feats2\", \"imagenet_deit_b16\"),\n",
    "    \"Dinov2-VIT14 (LVD-142M)\":           os.path.join(\"fm_ssl_run\", \"pretrained_feats2\", \"dinov2_vitb14\"),\n",
    "}\n",
    "\n",
    "# Metadata sources\n",
    "IDX_PARQUET = \"metaspace_images_dump/msi_fm_samples3.parquet\"\n",
    "MAN_PARQUET = \"metaspace_images_dump/manifest_expanded.parquet\"\n",
    "\n",
    "# Output (single combined folder for all models)\n",
    "OUT_DIR = os.path.join(\"fm_ssl_run\", \"baseline_eval_combined2\", \"umaps\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# UMAP params (per model)\n",
    "N_NEIGHBORS = 15\n",
    "MIN_DIST = 0.15\n",
    "METRIC = \"cosine\"\n",
    "\n",
    "# Tasks to color by (must exist after canonicalization)\n",
    "TASKS = [\"organism\", \"polarity\", \"Organism_Part\", \"Condition\", \"analyzerType\", \"ionisationSource\"]\n",
    "\n",
    "# ------------- Outlier Removal (for plotting only) -------------\n",
    "OUTLIER_REMOVE = True\n",
    "OUTLIER_METHOD = \"mad\"   # options: \"mad\", \"iqr\", \"percentile\"\n",
    "MAD_K = 3             # higher = fewer removals (robust; good default 6–10)\n",
    "IQR_K = 2.5              # multiplier for Q3 + k*IQR on radial distances\n",
    "DIST_PERCENTILE = 99.7   # keep points within this distance percentile (if method=\"percentile\")\n",
    "# Safety rails: if we accidentally drop too much, we auto-relax once\n",
    "MIN_KEEP_FRACTION = 0.85\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers for FM-run naming & paths\n",
    "# -----------------------------\n",
    "def read_vit_name_from_run(run_dir: str) -> str:\n",
    "    \"\"\"Read cfg.vit_name from run_params.json; fallback to folder name.\"\"\"\n",
    "    rp = os.path.join(run_dir, \"run_params.json\")\n",
    "    if os.path.exists(rp):\n",
    "        try:\n",
    "            with open(rp, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            if \"cfg\" in data and isinstance(data[\"cfg\"], dict):\n",
    "                vit = data[\"cfg\"].get(\"vit_name\", None)\n",
    "                if vit:\n",
    "                    return str(vit)\n",
    "            vit = data.get(\"vit_name\", None)\n",
    "            if vit:\n",
    "                return str(vit)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed reading vit_name from {rp}: {e}\")\n",
    "    return os.path.basename(os.path.normpath(run_dir))\n",
    "\n",
    "def make_unique_name(base_name: str, existing: set, hint: str) -> str:\n",
    "    \"\"\"Ensure display name uniqueness across runs/baselines.\"\"\"\n",
    "    if base_name not in existing:\n",
    "        return base_name\n",
    "    short = hint.replace(\"\\\\\", \"/\").strip(\"/\").split(\"/\")[-1]\n",
    "    cand = f\"{base_name} ({short})\"\n",
    "    if cand not in existing:\n",
    "        return cand\n",
    "    tiny = hashlib.md5(hint.encode(\"utf-8\")).hexdigest()[:6]\n",
    "    return f\"{base_name} [{tiny}]\"\n",
    "\n",
    "def fm_embeddings_dir(run_dir: str) -> str:\n",
    "    \"\"\"Default FM embeddings location within each run.\"\"\"\n",
    "    return os.path.join(run_dir, \"embeddings_all\")\n",
    "\n",
    "def fm_feats_filename() -> str:\n",
    "    return \"image_feats.npy\" if FM_EMB_VERSION.lower() == \"best\" else \"image_feats_last.npy\"\n",
    "\n",
    "def fm_embeddings_ready(emb_dir: str) -> bool:\n",
    "    feats_file = fm_feats_filename()\n",
    "    return (\n",
    "        os.path.exists(os.path.join(emb_dir, feats_file)) and\n",
    "        os.path.exists(os.path.join(emb_dir, \"index.csv\"))\n",
    "    )\n",
    "\n",
    "def baseline_embeddings_ready(emb_dir: str) -> bool:\n",
    "    return (\n",
    "        os.path.exists(os.path.join(emb_dir, \"image_feats.npy\")) and\n",
    "        os.path.exists(os.path.join(emb_dir, \"index.csv\"))\n",
    "    )\n",
    "\n",
    "# -----------------------------\n",
    "# Canonicalization (matches eval script)\n",
    "# -----------------------------\n",
    "def _clean(s):\n",
    "    if pd.isna(s): return None\n",
    "    s = str(s).strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def canonicalize_labels(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1) Polarity\n",
    "    pol_map = {\"pos\":\"Positive\",\"positive\":\"Positive\",\"+\":\"Positive\",\n",
    "               \"neg\":\"Negative\",\"negative\":\"Negative\",\"-\":\"Negative\"}\n",
    "    def canon_polarity(s):\n",
    "        if s is None: return None\n",
    "        t = _clean(s).lower()\n",
    "        t2 = pol_map.get(t, t)\n",
    "        if t2 in (\"positive\",\"negative\"):\n",
    "            return t2.capitalize()\n",
    "        if \"pos\" in t: return \"Positive\"\n",
    "        if \"neg\" in t: return \"Negative\"\n",
    "        return _clean(s)\n",
    "    if \"polarity\" in df.columns:\n",
    "        df[\"polarity\"] = df[\"polarity\"].map(canon_polarity)\n",
    "\n",
    "    # 2) Ionisation Source\n",
    "    def canon_ion_src(s):\n",
    "        if s is None: return None\n",
    "        t_raw = _clean(s)\n",
    "        t = t_raw.upper().replace(\"-\", \"\").replace(\"_\",\"\")\n",
    "        if \"APSMALDI\" in t: return \"AP-SMALDI\"\n",
    "        if \"IRMALDESI\" in t or \"IRMALDI\" in t: return \"IR-MALDESI\"\n",
    "        if \"APMALDI\" in t: return \"AP-MALDI\"\n",
    "        if \"DESIMSI\" in t: return \"DESI\"\n",
    "        if \"DESI\" in t: return \"DESI\"\n",
    "        if \"MALDI\" in t: return \"MALDI\"\n",
    "        return t_raw\n",
    "    if \"ionisationSource\" in df.columns:\n",
    "        df[\"ionisationSource\"] = df[\"ionisationSource\"].map(canon_ion_src)\n",
    "\n",
    "    # 3) Analyzer Type\n",
    "    def canon_analyzer(s):\n",
    "        if s is None: return None\n",
    "        t = _clean(s); tl = t.lower()\n",
    "        if \"timstof\" in tl and \"flex\" in tl: return \"timsTOF Flex\"\n",
    "        if \"fticr\" in tl:\n",
    "            if \"12t\" in tl: return \"12T FTICR\"\n",
    "            if \"7t\" in tl and \"scimax\" in tl: return \"FTICR scimaX 7T\"\n",
    "            return \"FTICR\"\n",
    "        if \"orbitrap\" in tl or \"q-exactive\" in tl: return \"Orbitrap\"\n",
    "        if \"tof\" in tl and \"reflector\" in tl: return \"TOF reflector\"\n",
    "        if tl.strip() == \"qtof\": return \"qTOF\"\n",
    "        return t\n",
    "    if \"analyzerType\" in df.columns:\n",
    "        df[\"analyzerType\"] = df[\"analyzerType\"].map(canon_analyzer)\n",
    "\n",
    "    # 4) Organism\n",
    "    def canon_organism(s):\n",
    "        if s is None: return None\n",
    "        t = _clean(s); tl = t.lower()\n",
    "        if \"|\" in t or \",\" in t:\n",
    "            if (\"human\" in tl or \"homo sapiens\" in tl) and (\"mouse\" in tl or \"mus musculus\" in tl):\n",
    "                return \"Mixed\"\n",
    "        if \"homo sapiens\" in tl or tl.strip() in {\"human\",\"h. sapiens\",\"homo\"}:\n",
    "            return \"Homo sapiens\"\n",
    "        if \"mus musculus\" in tl or tl.strip() in {\"mouse\",\"m. musculus\"}:\n",
    "            return \"Mus musculus\"\n",
    "        return t\n",
    "    if \"organism\" in df.columns:\n",
    "        df[\"organism\"] = df[\"organism\"].map(canon_organism)\n",
    "\n",
    "    # 5) Organism_Part\n",
    "    def canon_part(s):\n",
    "        if s is None: return None\n",
    "        t = _clean(s); tl = t.lower()\n",
    "        if \"kidney\" in tl: return \"Kidney\"\n",
    "        if \"brain\"  in tl: return \"Brain\"\n",
    "        if \"liver\"  in tl: return \"Liver\"\n",
    "        if \"lung\"   in tl: return \"Lung\"\n",
    "        if \"breast\" in tl: return \"Breast\"\n",
    "        if \"skin\"   in tl: return \"Skin\"\n",
    "        if \"heart\"  in tl or \"cardiac\" in tl: return \"Heart\"\n",
    "        return t\n",
    "    if \"Organism_Part\" in df.columns:\n",
    "        df[\"Organism_Part\"] = df[\"Organism_Part\"].map(canon_part)\n",
    "\n",
    "    # 6) Condition\n",
    "    def canon_condition(s):\n",
    "        if s is None: return None\n",
    "        t = _clean(s); tl = t.lower()\n",
    "        if tl in {\"n/a\",\"na\",\"none\",\"not available\",\"\"}: return \"NA\"\n",
    "        if tl in {\"biopsy\",\"biopsies\"}: return \"Biopsy\"\n",
    "        if \"fresh frozen\" in tl or \"frozen\" in tl: return \"Frozen\"\n",
    "        if \"tumor\" in tl or \"tumour\" in tl: return \"Tumor\"\n",
    "        if \"cancer\" in tl: return \"Cancer\"\n",
    "        if \"wildtype\" in tl or tl == \"wt\": return \"Wildtype\"\n",
    "        if \"healthy\" in tl or \"control\" in tl: return \"Healthy\"\n",
    "        if \"diseased\" in tl or \"disease\" in tl: return \"Diseased\"\n",
    "        return t\n",
    "    if \"Condition\" in df.columns:\n",
    "        df[\"Condition\"] = df[\"Condition\"].map(canon_condition)\n",
    "\n",
    "    return df\n",
    "\n",
    "# -----------------------------\n",
    "# Load metadata (+ canonicalize)\n",
    "# -----------------------------\n",
    "idx = pd.read_parquet(IDX_PARQUET)\n",
    "man = pd.read_parquet(MAN_PARQUET)\n",
    "meta = idx.merge(man, on=\"dataset_id\", how=\"left\", suffixes=(\"\", \"_man\"))\n",
    "meta = meta.drop_duplicates(\"sample_path\", keep=\"first\").reset_index(drop=True)\n",
    "meta = canonicalize_labels(meta)\n",
    "\n",
    "# -----------------------------\n",
    "# Build model list (FM runs + baselines)\n",
    "# -----------------------------\n",
    "MODELS = []  # {\"tag\": display_name, \"emb_dir\": path, \"feats_file\": fname}\n",
    "seen_names = set()\n",
    "\n",
    "# Add baselines\n",
    "for tag, emb_dir in BASELINE_EMB_SETS.items():\n",
    "    name_disp = make_unique_name(tag, seen_names, hint=emb_dir)\n",
    "    seen_names.add(name_disp)\n",
    "    if not baseline_embeddings_ready(emb_dir):\n",
    "        print(f\"[WARN] Missing embeddings for baseline '{name_disp}' at {emb_dir} \"\n",
    "              f\"(need image_feats.npy + index.csv); skipping.\")\n",
    "        continue\n",
    "    MODELS.append({\"tag\": name_disp, \"emb_dir\": emb_dir, \"feats_file\": \"image_feats.npy\"})\n",
    "\n",
    "if not MODELS:\n",
    "    raise SystemExit(\"[ERR] No models to visualize.\")\n",
    "\n",
    "print(\"[INFO] Models to visualize:\")\n",
    "for m in MODELS:\n",
    "    print(f\" - {m['tag']}: {m['emb_dir']} [{m['feats_file']}]\")\n",
    "\n",
    "# -----------------------------\n",
    "# Outlier detection helpers\n",
    "# -----------------------------\n",
    "def _mad(x):\n",
    "    med = np.median(x)\n",
    "    return np.median(np.abs(x - med))\n",
    "\n",
    "def outlier_mask_umap(umap_xy: np.ndarray,\n",
    "                      method: str = OUTLIER_METHOD,\n",
    "                      mad_k: float = MAD_K,\n",
    "                      iqr_k: float = IQR_K,\n",
    "                      dist_pct: float = DIST_PERCENTILE) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return boolean mask of points to KEEP based on robust radial distance\n",
    "    from the median center of the UMAP cloud.\n",
    "    \"\"\"\n",
    "    if umap_xy.ndim != 2 or umap_xy.shape[1] != 2:\n",
    "        return np.ones(len(umap_xy), dtype=bool)\n",
    "\n",
    "    # robust center\n",
    "    cx, cy = np.median(umap_xy[:, 0]), np.median(umap_xy[:, 1])\n",
    "    r = np.sqrt((umap_xy[:, 0] - cx) ** 2 + (umap_xy[:, 1] - cy) ** 2)\n",
    "\n",
    "    if method == \"mad\":\n",
    "        mad = _mad(r)\n",
    "        if mad == 0:\n",
    "            # fallback to percentile if degenerate\n",
    "            thr = np.percentile(r, dist_pct)\n",
    "            keep = r <= thr\n",
    "        else:\n",
    "            thr = np.median(r) + mad_k * mad\n",
    "            keep = r <= thr\n",
    "\n",
    "    elif method == \"iqr\":\n",
    "        q1, q3 = np.percentile(r, [25, 75])\n",
    "        iqr = q3 - q1\n",
    "        thr = q3 + iqr_k * iqr\n",
    "        keep = r <= thr\n",
    "\n",
    "    elif method == \"percentile\":\n",
    "        thr = np.percentile(r, dist_pct)\n",
    "        keep = r <= thr\n",
    "\n",
    "    else:\n",
    "        # unknown method -> keep all\n",
    "        return np.ones(len(umap_xy), dtype=bool)\n",
    "\n",
    "    # Safety rail: if we dropped too many, relax threshold once\n",
    "    frac = keep.mean()\n",
    "    if frac < MIN_KEEP_FRACTION:\n",
    "        print(f\"[WARN] Outlier removal kept only {frac:.1%}; relaxing...\")\n",
    "        # Relax by blending toward 100th percentile\n",
    "        thr_relaxed = np.percentile(r, min(99.95, max(dist_pct, 99.0)))\n",
    "        keep = r <= thr_relaxed\n",
    "    return keep\n",
    "\n",
    "# -----------------------------\n",
    "# UMAP helper (fit/transform)\n",
    "# -----------------------------\n",
    "def run_umap_once(X, n_neighbors=30, min_dist=0.15, seed=6740, metric=\"cosine\"):\n",
    "    reducer = umap.UMAP(\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        random_state=seed,\n",
    "        metric=metric\n",
    "    )\n",
    "    return reducer.fit_transform(X)\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Build per-task kept-classes + color maps (global, used by UMAPs & pies)\n",
    "# -----------------------------\n",
    "def build_task_color_maps(meta_df, tasks, min_class_count=50):\n",
    "    kept_by_task = {}\n",
    "    cmap_by_task = {}\n",
    "    for task in tasks:\n",
    "        if task not in meta_df.columns:\n",
    "            continue\n",
    "        labels_full = meta_df[task].astype(\"string\").fillna(\"NA\")\n",
    "        vc = labels_full.value_counts()\n",
    "        kept = sorted(vc[vc >= min_class_count].index.tolist())\n",
    "        palette = (\n",
    "            sns.color_palette(\"tab10\", n_colors=max(len(kept), 1))\n",
    "            if len(kept) <= 10\n",
    "            else sns.color_palette(\"husl\", max(len(kept), 1))\n",
    "        )\n",
    "        color_map = {lab: palette[i] for i, lab in enumerate(kept)}\n",
    "        kept_by_task[task] = kept\n",
    "        cmap_by_task[task] = color_map\n",
    "    return kept_by_task, cmap_by_task\n",
    "\n",
    "# Build once (so all models/plots use identical task colors)\n",
    "KEPT_BY_TASK, CMAP_BY_TASK = build_task_color_maps(meta, TASKS, MIN_CLASS_COUNT)\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Global pies (one per task), using same colors as UMAPs\n",
    "# -----------------------------\n",
    "def save_global_pie_for_task(meta_df, task, kept_by_task, cmap_by_task, out_dir):\n",
    "    if task not in meta_df.columns:\n",
    "        print(f\"[WARN] Global pie: task '{task}' not in metadata; skipping.\")\n",
    "        return\n",
    "\n",
    "    labels = meta_df[task].astype(\"string\").fillna(\"NA\")\n",
    "    vc_all = labels.value_counts()\n",
    "    kept = kept_by_task.get(task, [])\n",
    "    c_map = cmap_by_task.get(task, {})\n",
    "\n",
    "    # Only plot kept classes\n",
    "    sizes, colors, names = [], [], []\n",
    "    total = 0\n",
    "    for lab in kept:\n",
    "        cnt = int(vc_all.get(lab, 0))\n",
    "        if cnt > 0:\n",
    "            sizes.append(cnt)\n",
    "            colors.append(c_map[lab])\n",
    "            names.append(str(lab))\n",
    "            total += cnt\n",
    "\n",
    "    if total == 0:\n",
    "        print(f\"[WARN] Global pie: '{task}' has no kept samples; skipping.\")\n",
    "        return\n",
    "\n",
    "    def _count_autopct(pct):\n",
    "        absolute = int(round(pct * total / 100.0))\n",
    "        return f\"{absolute}\"\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6.4, 6.4))\n",
    "    wedges, texts, autotexts = ax.pie(\n",
    "        sizes,\n",
    "        colors=colors,\n",
    "        startangle=90,\n",
    "        autopct=_count_autopct,\n",
    "        pctdistance=0.72,\n",
    "        textprops={\"fontsize\": 9},\n",
    "        wedgeprops=dict(linewidth=0.5, edgecolor=\"white\"),\n",
    "    )\n",
    "    ax.axis(\"equal\")\n",
    "\n",
    "    ax.legend(\n",
    "        wedges, names,\n",
    "        title=\"Classes\",\n",
    "        loc=\"center left\",\n",
    "        bbox_to_anchor=(1.02, 0.5),\n",
    "        frameon=False,\n",
    "        fontsize=8\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    out_path = os.path.join(out_dir, f\"pie_{task}.png\")\n",
    "    plt.savefig(out_path, dpi=250, bbox_inches=\"tight\", pad_inches=0.15)\n",
    "    plt.close()\n",
    "    print(f\"[OK] saved: {out_path}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3) UMAP plotting using precomputed task color maps\n",
    "#     (applies outlier removal to the scatter only)\n",
    "# -----------------------------\n",
    "def plot_same_coords_color_by_tasks(umap_2d, df_join, model_tag, out_dir,\n",
    "                                    tasks, min_class_count=50,\n",
    "                                    outlier_remove=True):\n",
    "    assert len(df_join) == len(umap_2d), \"UMAP coords and df_join length mismatch\"\n",
    "\n",
    "    # Save RAW coords for reuse\n",
    "    raw_coords_out = os.path.join(out_dir, f\"umap_coords_{model_tag}_raw.csv\")\n",
    "    pd.DataFrame({\n",
    "        \"sample_path\": df_join[\"sample_path\"].values,\n",
    "        \"umap_x\": umap_2d[:,0],\n",
    "        \"umap_y\": umap_2d[:,1],\n",
    "    }).to_csv(raw_coords_out, index=False)\n",
    "\n",
    "    # Compute outlier mask (for plotting)\n",
    "    if outlier_remove:\n",
    "        keep_mask = outlier_mask_umap(umap_2d, method=OUTLIER_METHOD,\n",
    "                                      mad_k=MAD_K, iqr_k=IQR_K, dist_pct=DIST_PERCENTILE)\n",
    "    else:\n",
    "        keep_mask = np.ones(len(umap_2d), dtype=bool)\n",
    "\n",
    "    kept_frac = keep_mask.mean()\n",
    "    removed = (~keep_mask).sum()\n",
    "    if removed > 0:\n",
    "        print(f\"[INFO] {model_tag}: removing {removed} outliers ({(1-kept_frac):.1%}) from plot\")\n",
    "\n",
    "    # Save FILTERED coords (for transparency)\n",
    "    filt_coords_out = os.path.join(out_dir, f\"umap_coords_{model_tag}_filtered.csv\")\n",
    "    pd.DataFrame({\n",
    "        \"sample_path\": df_join.loc[keep_mask, \"sample_path\"].values,\n",
    "        \"umap_x\": umap_2d[keep_mask, 0],\n",
    "        \"umap_y\": umap_2d[keep_mask, 1],\n",
    "    }).to_csv(filt_coords_out, index=False)\n",
    "\n",
    "    # Use filtered arrays for plotting only\n",
    "    U = umap_2d[keep_mask]\n",
    "    DF = df_join.loc[keep_mask].reset_index(drop=True)\n",
    "\n",
    "    for task in tasks:\n",
    "        if task not in DF.columns:\n",
    "            print(f\"[WARN] {model_tag}: task '{task}' not in metadata; skipping.\")\n",
    "            continue\n",
    "\n",
    "        labels = DF[task].astype(\"string\").fillna(\"NA\")\n",
    "        kept = KEPT_BY_TASK.get(task, [])\n",
    "        color_map = CMAP_BY_TASK.get(task, {})\n",
    "\n",
    "        is_keep = labels.isin(kept).values\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(7.8, 6.6))\n",
    "\n",
    "        # gray background for non-kept classes (still filtered by outliers)\n",
    "        m_bg = ~is_keep\n",
    "        if m_bg.any():\n",
    "            ax.scatter(\n",
    "                U[m_bg, 0], U[m_bg, 1],\n",
    "                s=6, alpha=0.25, linewidths=0, c=\"#cfcfcf\"\n",
    "            )\n",
    "\n",
    "        # kept classes with fixed colors\n",
    "        for lab in kept:\n",
    "            sel = (labels.values == lab)\n",
    "            if not sel.any():\n",
    "                continue\n",
    "            ax.scatter(\n",
    "                U[sel, 0], U[sel, 1],\n",
    "                s=6, alpha=0.8, linewidths=0,\n",
    "                color=color_map[lab], label=str(lab)\n",
    "            )\n",
    "\n",
    "        ax.set_title(f\"UMAP — {model_tag} [{task}] (outliers removed)\")\n",
    "        ax.set_xlabel(\"UMAP-1\"); ax.set_ylabel(\"UMAP-2\")\n",
    "\n",
    "        if 0 < len(kept) <= 25:\n",
    "            ax.legend(markerscale=3, bbox_to_anchor=(1.02, 1),\n",
    "                      loc=\"upper left\", fontsize=8, frameon=False, borderaxespad=0.0)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        out_path = os.path.join(out_dir, f\"umap_{model_tag}_{task}.png\")\n",
    "        plt.savefig(out_path, dpi=250, bbox_inches=\"tight\", pad_inches=0.15)\n",
    "        plt.close()\n",
    "        print(f\"[OK] saved: {out_path}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Main: loop models (UMAP once per model) + global pies once per task\n",
    "# -----------------------------\n",
    "for m in MODELS:\n",
    "    tag = m[\"tag\"]\n",
    "    emb_dir = m[\"emb_dir\"]\n",
    "    feats_file = m[\"feats_file\"]\n",
    "\n",
    "    feats_path = os.path.join(emb_dir, feats_file)\n",
    "    index_path = os.path.join(emb_dir, \"index.csv\")\n",
    "    if not (os.path.exists(feats_path) and os.path.exists(index_path)):\n",
    "        print(f\"[WARN] Missing artifacts for {tag} at {emb_dir} \"\n",
    "              f\"(need {feats_file} + index.csv)\")\n",
    "        continue\n",
    "\n",
    "    emb = np.load(feats_path)\n",
    "    index = pd.read_csv(index_path)  # must have 'sample_path'\n",
    "\n",
    "    # Join & align\n",
    "    df_join = meta.merge(index, on=\"sample_path\", how=\"inner\").reset_index(drop=True)\n",
    "    if \"index\" in df_join.columns:\n",
    "        df_join = df_join.drop(columns=[\"index\"])\n",
    "\n",
    "    # Align lengths robustly (index.csv order is assumed to match emb row order)\n",
    "    if emb.shape[0] != len(df_join):\n",
    "        if emb.shape[0] > len(df_join):\n",
    "            print(f\"[WARN] {tag}: more embeddings ({emb.shape[0]}) than metadata rows ({len(df_join)}); trimming embeddings.\")\n",
    "            emb = emb[:len(df_join)]\n",
    "        else:\n",
    "            print(f\"[WARN] {tag}: fewer embeddings ({emb.shape[0]}) than metadata rows ({len(df_join)}); truncating metadata.\")\n",
    "            df_join = df_join.iloc[:emb.shape[0]].reset_index(drop=True)\n",
    "\n",
    "    # Optional FAST subsampling for UMAP fit (fit on subset, transform all)\n",
    "    SUB_FIT = None\n",
    "    if FAST_MODE and emb.shape[0] > 120_000:\n",
    "        rng = np.random.default_rng(RANDOM_SEED)\n",
    "        sub_idx = rng.choice(emb.shape[0], size=120_000, replace=False)\n",
    "        SUB_FIT = sub_idx\n",
    "\n",
    "    reducer = umap.UMAP(\n",
    "        n_neighbors=N_NEIGHBORS, min_dist=MIN_DIST,\n",
    "        random_state=RANDOM_SEED, metric=METRIC\n",
    "    )\n",
    "    if SUB_FIT is None:\n",
    "        umap_2d = reducer.fit_transform(emb)\n",
    "    else:\n",
    "        reducer.fit(emb[SUB_FIT])\n",
    "        umap_2d = reducer.transform(emb)\n",
    "\n",
    "    print(f\"[INFO] UMAP {tag}: n={emb.shape[0]} → coords computed once\")\n",
    "\n",
    "    # Plot SAME coords, recolor by each task (colors fixed globally)\n",
    "    plot_same_coords_color_by_tasks(\n",
    "        umap_2d=umap_2d,\n",
    "        df_join=df_join,\n",
    "        model_tag=tag,\n",
    "        out_dir=OUT_DIR,\n",
    "        tasks=TASKS,\n",
    "        min_class_count=MIN_CLASS_COUNT,\n",
    "        outlier_remove=OUTLIER_REMOVE\n",
    "    )\n",
    "\n",
    "# -------- After UMAPs: ONE pie per task (colors match UMAPs) --------\n",
    "for task in TASKS:\n",
    "    save_global_pie_for_task(meta, task, KEPT_BY_TASK, CMAP_BY_TASK, OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65da4b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
