{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bbe5b7f",
   "metadata": {},
   "source": [
    "### m/z importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e02ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] duplicate sample_path rows; keeping first.\n",
      "[OK] Loaded cached X_mz: (3928, 50026), axis: (50026,)\n",
      "[ALIGN] |X paths|=3928 |emb paths|=3928 |meta paths|=3928 |common|=3928\n",
      "[ALIGN] Final shapes — X_mz: (3928, 50026), Z: (3928, 768), meta rows: 3928\n",
      "\n",
      "[DONE] Outputs in: fm_ssl_run\\baseline_eval_combined2\\importance_mz_from_embeddings\\dinov2_vitb14\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# m/z IMPORTANCE from model embeddings (surrogate via spectra)\n",
    "# - Loads precomputed embeddings (image_feats.npy + index.csv)\n",
    "# - Builds binned spectra X_mz from .npz (mz + patch)\n",
    "# - Learns Z ≈ X_mz @ B (ridge), and class ← Z (L1-OvR)\n",
    "# - Attributes class→m/z by: score_mz = B @ w_class\n",
    "# - Outputs: fm_ssl_run/baseline_eval_combined2/importance_mz_from_embeddings/<model_tag>\n",
    "# =====================================================\n",
    "\n",
    "import os, re, math, glob, warnings\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "SEED = 6740\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# --- Data sources ---\n",
    "SPLIT_CSV   = os.path.join(\"splits_by_dataset_id.csv\")\n",
    "IDX_PARQUET = r\"metaspace_images_dump/msi_fm_samples3.parquet\"\n",
    "MAN_PARQUET = r\"metaspace_images_dump/manifest_expanded.parquet\"\n",
    "\n",
    "# --- Embedding model directory (precomputed) ---\n",
    "MODEL_TAG   = \"dinov2_vitb14\"\n",
    "MODEL_DIR   = os.path.join(\"fm_ssl_run\", \"pretrained_feats2\", MODEL_TAG)  # << your path\n",
    "FEATS_NPY   = os.path.join(MODEL_DIR, \"image_feats.npy\")\n",
    "INDEX_CSV   = os.path.join(MODEL_DIR, \"index.csv\")  # must include 'sample_path'\n",
    "\n",
    "# --- Where the raw .npz live (roots for resolver) ---\n",
    "DATA_ROOTS = [\n",
    "    r\"Y:\\coskun-lab\\Efe\\MSI Foundation Model\",\n",
    "    r\"Y:\\coskun-lab\\Efe\\MSI Foundation Model\\metaspace_images_dump\\msi_fm_samples3\",\n",
    "    os.getcwd(),\n",
    "]\n",
    "\n",
    "# --- Output dirs ---\n",
    "OUT_ROOT = os.path.join(\"fm_ssl_run\", \"baseline_eval_combined2\", \"importance_mz_from_embeddings\", MODEL_TAG)\n",
    "CACHE_DIR = os.path.join(OUT_ROOT, \"_cache_mz\")\n",
    "os.makedirs(OUT_ROOT, exist_ok=True)\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# --- Tasks & filtering ---\n",
    "TASKS = [\"organism\", \"polarity\", \"Organism_Part\", \"Condition\", \"analyzerType\", \"ionisationSource\"]\n",
    "EXCLUDE_LABELS = {\"Condition\": {\"NA\"}}\n",
    "MIN_CLASS_COUNT = 100\n",
    "\n",
    "# --- Binning & preprocessing ---\n",
    "BIN_DA  = 0.05   # Δ m/z (Da)\n",
    "BIN_PPM = None   # (not used)\n",
    "TIC_NORM    = False\n",
    "APPLY_LOG1P = False\n",
    "\n",
    "# --- Surrogate / attribution settings ---\n",
    "RIDGE_ALPHA = 5.0       # for Z ≈ X_mz @ B\n",
    "LR_C        = 0.5       # for class ← Z (L1 OvR)\n",
    "LR_MAX_ITER = 2000\n",
    "\n",
    "# --- Visualization ---\n",
    "TOP_MZ_PER_CLASS = 10\n",
    "MAX_MZ_IN_PANEL  = 120\n",
    "\n",
    "# =====================================================\n",
    "# Helpers\n",
    "# =====================================================\n",
    "def safe_name(s: str) -> str:\n",
    "    return re.sub(r\"[^A-Za-z0-9._-]+\", \"_\", str(s)).strip(\"_\")\n",
    "\n",
    "def _clean(s):\n",
    "    if pd.isna(s): return None\n",
    "    s = str(s).strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def canonicalize_labels(df):\n",
    "    df = df.copy()\n",
    "    pol_map = {\"pos\":\"Positive\",\"positive\":\"Positive\",\"+\":\"Positive\",\n",
    "               \"neg\":\"Negative\",\"negative\":\"Negative\",\"-\":\"Negative\"}\n",
    "    def canon_polarity(s):\n",
    "        if s is None: return None\n",
    "        t = _clean(s).lower()\n",
    "        t2 = pol_map.get(t, t)\n",
    "        if t2 in (\"positive\",\"negative\"):\n",
    "            return t2.capitalize()\n",
    "        if \"pos\" in t: return \"Positive\"\n",
    "        if \"neg\" in t: return \"Negative\"\n",
    "        return _clean(s)\n",
    "    if \"polarity\" in df.columns:\n",
    "        df[\"polarity\"] = df[\"polarity\"].map(canon_polarity)\n",
    "\n",
    "    def canon_ion_src(s):\n",
    "        if s is None: return None\n",
    "        t_raw = _clean(s)\n",
    "        t = t_raw.upper().replace(\"-\", \"\").replace(\"_\",\"\")\n",
    "        if \"APSMALDI\" in t: return \"AP-SMALDI\"\n",
    "        if \"IRMALDESI\" in t or \"IRMALDI\" in t: return \"IR-MALDESI\"\n",
    "        if \"APMALDI\" in t: return \"AP-MALDI\"\n",
    "        if \"DESIMSI\" in t: return \"DESI\"\n",
    "        if \"DESI\" in t: return \"DESI\"\n",
    "        if \"MALDI\" in t: return \"MALDI\"\n",
    "        return t_raw\n",
    "    if \"ionisationSource\" in df.columns:\n",
    "        df[\"ionisationSource\"] = df[\"ionisationSource\"].map(canon_ion_src)\n",
    "\n",
    "    def canon_analyzer(s):\n",
    "        if s is None: return None\n",
    "        t = _clean(s); tl = t.lower()\n",
    "        if \"timstof\" in tl and \"flex\" in tl: return \"timsTOF Flex\"\n",
    "        if \"fticr\" in tl:\n",
    "            if \"12t\" in tl: return \"12T FTICR\"\n",
    "            if \"7t\" in tl and \"scimax\" in tl: return \"FTICR scimaX 7T\"\n",
    "            return \"FTICR\"\n",
    "        if \"orbitrap\" in tl or \"q-exactive\" in tl: return \"Orbitrap\"\n",
    "        if \"tof\" in tl and \"reflector\" in tl: return \"TOF reflector\"\n",
    "        if tl.strip() == \"qtof\": return \"qTOF\"\n",
    "        return t\n",
    "    if \"analyzerType\" in df.columns:\n",
    "        df[\"analyzerType\"] = df[\"analyzerType\"].map(canon_analyzer)\n",
    "\n",
    "    def canon_organism(s):\n",
    "        if s is None: return None\n",
    "        t = _clean(s); tl = t.lower()\n",
    "        if \"|\" in t or \",\" in t:\n",
    "            if (\"human\" in tl or \"homo sapiens\" in tl) and (\"mouse\" in tl or \"mus musculus\" in tl):\n",
    "                return \"Mixed\"\n",
    "        if \"homo sapiens\" in tl or tl.strip() in {\"human\",\"h. sapiens\",\"homo\"}:\n",
    "            return \"Homo sapiens\"\n",
    "        if \"mus musculus\" in tl or tl.strip() in {\"mouse\",\"m. musculus\"}:\n",
    "            return \"Mus musculus\"\n",
    "        return t\n",
    "    if \"organism\" in df.columns:\n",
    "        df[\"organism\"] = df[\"organism\"].map(canon_organism)\n",
    "\n",
    "    def canon_part(s):\n",
    "        if s is None: return None\n",
    "        t = _clean(s); tl = t.lower()\n",
    "        if \"kidney\" in tl: return \"Kidney\"\n",
    "        if \"brain\"  in tl: return \"Brain\"\n",
    "        if \"liver\"  in tl: return \"Liver\"\n",
    "        if \"lung\"   in tl: return \"Lung\"\n",
    "        if \"breast\" in tl: return \"Breast\"\n",
    "        if \"skin\"   in tl: return \"Skin\"\n",
    "        if \"heart\"  in tl or \"cardiac\" in tl: return \"Heart\"\n",
    "        return t\n",
    "    if \"Organism_Part\" in df.columns:\n",
    "        df[\"Organism_Part\"] = df[\"Organism_Part\"].map(canon_part)\n",
    "\n",
    "    def canon_condition(s):\n",
    "        if s is None: return None\n",
    "        t = _clean(s); tl = t.lower()\n",
    "        if tl in {\"n/a\",\"na\",\"none\",\"not available\",\"\"}: return \"NA\"\n",
    "        if tl in {\"biopsy\",\"biopsies\"}: return \"Biopsy\"\n",
    "        if \"fresh frozen\" in tl or \"frozen\" in tl: return \"Frozen\"\n",
    "        if \"tumor\" in tl or \"tumour\" in tl: return \"Tumor\"\n",
    "        if \"cancer\" in tl: return \"Cancer\"\n",
    "        if \"wildtype\" in tl or tl == \"wt\": return \"Wildtype\"\n",
    "        if \"healthy\" in tl or \"control\" in tl: return \"Healthy\"\n",
    "        if \"diseased\" in tl or \"disease\" in tl: return \"Diseased\"\n",
    "        return t\n",
    "    if \"Condition\" in df.columns:\n",
    "        df[\"Condition\"] = df[\"Condition\"].map(canon_condition)\n",
    "    return df\n",
    "\n",
    "def filter_valid(df_task, yname, min_count=5):\n",
    "    x = df_task.dropna(subset=[yname]).copy()\n",
    "    if yname in EXCLUDE_LABELS:\n",
    "        x = x[~x[yname].isin(EXCLUDE_LABELS[yname])]\n",
    "    x = x[x[yname].astype(str).str.len() > 0]\n",
    "    vc = x[yname].value_counts()\n",
    "    keep = vc[vc >= min_count].index\n",
    "    x = x[x[yname].isin(keep)].copy()\n",
    "    return x\n",
    "\n",
    "def fit_l1_ovr_on_embeddings(Z: np.ndarray, y: np.ndarray, classes: List[str]) -> Dict[str, np.ndarray]:\n",
    "    out = {}\n",
    "    for cls in classes:\n",
    "        y_bin = (y == cls).astype(int)\n",
    "        pipe = Pipeline([\n",
    "            (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "            (\"clf\", LogisticRegression(\n",
    "                penalty=\"l1\", solver=\"saga\", C=LR_C, max_iter=LR_MAX_ITER,\n",
    "                class_weight=\"balanced\", n_jobs=-1, random_state=SEED\n",
    "            ))\n",
    "        ])\n",
    "        pipe.fit(Z, y_bin)\n",
    "        scaler: StandardScaler = pipe.named_steps[\"scaler\"]\n",
    "        w_std = pipe.named_steps[\"clf\"].coef_.ravel()\n",
    "        out[cls] = w_std / (scaler.scale_ + 1e-12)  # back to embedding scale\n",
    "    return out  # dict[class] -> (D,)\n",
    "\n",
    "def ridge_embeddings_from_mz(X_mz: np.ndarray, Z: np.ndarray, alpha: float) -> np.ndarray:\n",
    "    \"\"\"Fit Z ≈ X_mz @ B with Ridge. Returns B of shape (M_mz, D_emb).\"\"\"\n",
    "    model = Ridge(alpha=alpha, fit_intercept=True, random_state=SEED)\n",
    "    model.fit(X_mz, Z)                    # multioutput\n",
    "    B = model.coef_.T                     # shape (n_targets, n_features) -> (M, D).T => (M, D)\n",
    "    return B\n",
    "\n",
    "def select_union_top(feats_by_class: Dict[str, np.ndarray], top_k: int, max_total: int) -> np.ndarray:\n",
    "    idx_sets = []\n",
    "    for _, vec in feats_by_class.items():\n",
    "        order = np.argsort(-np.abs(vec))\n",
    "        idx_sets.append(set(order[:top_k].tolist()))\n",
    "    union = list(set().union(*idx_sets))\n",
    "    if len(union) > max_total:\n",
    "        mat = np.stack([feats_by_class[c] for c in feats_by_class], axis=0)\n",
    "        agg = np.mean(np.abs(mat), axis=0)\n",
    "        order = np.argsort(-agg[union])\n",
    "        union = [union[i] for i in order[:max_total]]\n",
    "    return np.array(sorted(union))\n",
    "\n",
    "def format_mz(mz_vals: np.ndarray) -> List[str]:\n",
    "    return [f\"{m:.2f}\" for m in mz_vals]\n",
    "\n",
    "def plot_heatmap(df_vals: pd.DataFrame, title: str, out_png: str):\n",
    "    plt.figure(figsize=(min(28, 2 + 0.45*len(df_vals.columns)), 0.6 + 0.55*len(df_vals)))\n",
    "    ax = sns.heatmap(df_vals, cmap=\"vlag\", center=0.0, linewidths=0.3, linecolor='white')\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.set_xlabel(\"m/z\")\n",
    "    ax.set_ylabel(\"Class\")\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), rotation=0)  # horizontal Y labels\n",
    "    # ax.set_xticklabels(ax.get_xticklabels(), rotation=90)  # optional if many columns\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "# ---------- robust resolver + loader for npz ----------\n",
    "LIKELY_SUBFOLDERS = [os.path.join(\"metaspace_images_dump\", \"msi_fm_samples3\"), \"\"]\n",
    "def resolve_npz_path(sp: str) -> Optional[str]:\n",
    "    sp_norm = os.path.normpath(sp)\n",
    "    base = os.path.basename(sp_norm)\n",
    "    base_noext, ext = os.path.splitext(base)\n",
    "    if os.path.isabs(sp_norm) and os.path.exists(sp_norm):\n",
    "        return sp_norm\n",
    "    for root in DATA_ROOTS:\n",
    "        cand = os.path.normpath(os.path.join(root, sp_norm))\n",
    "        if os.path.exists(cand): return cand\n",
    "    for root in DATA_ROOTS:\n",
    "        for sub in LIKELY_SUBFOLDERS:\n",
    "            cand2 = os.path.normpath(os.path.join(root, sub, base))\n",
    "            if os.path.exists(cand2): return cand2\n",
    "            if ext == \"\" and os.path.exists(cand2 + \".npz\"): return cand2 + \".npz\"\n",
    "    for root in DATA_ROOTS:\n",
    "        pat = os.path.join(os.path.normpath(root), \"**\", base)\n",
    "        hits = glob.glob(pat, recursive=True)\n",
    "        if not hits and ext == \"\": hits = glob.glob(pat + \".npz\", recursive=True)\n",
    "        if hits: return os.path.normpath(hits[0])\n",
    "    return None\n",
    "\n",
    "# include 'patch' for your tiles + other common keys\n",
    "MZ_KEYS = [\"mz\", \"mzs\", \"mz_axis\", \"mass\", \"mass_axis\", \"MZ\", \"MZ_axis\"]\n",
    "POSSIBLE_IMAGE_KEYS = [\"patch\",\"tile\",\"tiles\",\"img\",\"image\",\"images\",\"arr\",\"X\",\"cube\",\"data\",\n",
    "                       \"spec\",\"spectra\",\"intensity\",\"intensities\"]\n",
    "\n",
    "def load_npz_spectrum(npz_path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    with np.load(npz_path, allow_pickle=True) as npz:\n",
    "        mz_key = next((k for k in MZ_KEYS if k in npz.files), None)\n",
    "        if mz_key is None:\n",
    "            raise KeyError(f\"No m/z key in {npz_path}; keys={list(npz.files)}\")\n",
    "        mz = np.array(npz[mz_key], dtype=float).ravel()\n",
    "        img_key = next((k for k in POSSIBLE_IMAGE_KEYS if k in npz.files), None)\n",
    "        if img_key is None:\n",
    "            raise KeyError(f\"No intensity key in {npz_path}; keys={list(npz.files)}\")\n",
    "        arr = npz[img_key]\n",
    "        if arr.ndim == 1:\n",
    "            if arr.size != mz.size: raise ValueError(f\"1D {arr.size} != len(mz) {mz.size}\")\n",
    "            spec = arr.astype(np.float64)\n",
    "        elif arr.ndim == 2:\n",
    "            if arr.shape[1] == mz.size: spec = arr.mean(axis=0)\n",
    "            elif arr.shape[0] == mz.size: spec = arr.mean(axis=1)\n",
    "            else: raise ValueError(f\"2D {arr.shape} vs mz {mz.size}\")\n",
    "        elif arr.ndim == 3:\n",
    "            if arr.shape[-1] == mz.size: spec = arr.reshape(-1, arr.shape[-1]).mean(axis=0)\n",
    "            elif arr.shape[0] == mz.size: spec = arr.reshape(arr.shape[0], -1).mean(axis=1)\n",
    "            else: raise ValueError(f\"3D {arr.shape} vs mz {mz.size}\")\n",
    "        elif arr.ndim == 4:\n",
    "            if arr.shape[-1] == mz.size: spec = arr.reshape(-1, arr.shape[-1]).mean(axis=0)\n",
    "            elif arr.shape[1] == mz.size: spec = arr.reshape(arr.shape[0], arr.shape[1], -1).mean(axis=(0,2))\n",
    "            else: raise ValueError(f\"4D {arr.shape} vs mz {mz.size}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported ndim={arr.ndim}\")\n",
    "        return mz, spec\n",
    "\n",
    "def build_global_bins(min_mz: float, max_mz: float, bin_da: Optional[float], ppm: Optional[float]) -> np.ndarray:\n",
    "    if ppm is not None: raise NotImplementedError(\"PPM binning not implemented; use BIN_DA.\")\n",
    "    n_bins = int(math.ceil((max_mz - min_mz) / bin_da)) + 1\n",
    "    edges = min_mz + np.arange(n_bins + 1) * bin_da\n",
    "    centers = (edges[:-1] + edges[1:]) / 2.0\n",
    "    return centers\n",
    "\n",
    "def bin_spectrum(mz: np.ndarray, spec: np.ndarray, centers: np.ndarray, bin_da: float) -> np.ndarray:\n",
    "    min_edge = centers[0] - bin_da/2\n",
    "    idx = np.floor((mz - min_edge) / bin_da).astype(int)\n",
    "    valid = (idx >= 0) & (idx < centers.size)\n",
    "    b = np.zeros(centers.size, dtype=np.float64)\n",
    "    np.add.at(b, idx[valid], spec[valid])\n",
    "    return b\n",
    "\n",
    "# =====================================================\n",
    "# Load metadata + splits\n",
    "# =====================================================\n",
    "idx_df = pd.read_parquet(IDX_PARQUET)          # contains sample_path + dataset_id\n",
    "man_df = pd.read_parquet(MAN_PARQUET)          # dataset-level metadata\n",
    "need_cols = [\"dataset_id\",\"sample_path\",\"organism\",\"polarity\",\"Organism_Part\",\"Condition\",\"analyzerType\",\"ionisationSource\"]\n",
    "man_sub = man_df[[c for c in need_cols if c in man_df.columns]].drop_duplicates(\"dataset_id\")\n",
    "df_meta = idx_df.merge(man_sub, on=\"dataset_id\", how=\"left\", suffixes=(\"\", \"_man\"))\n",
    "df_meta = df_meta.loc[:, ~df_meta.columns.duplicated()].copy().reset_index(drop=True)\n",
    "splits = pd.read_csv(SPLIT_CSV)\n",
    "df_meta = df_meta.merge(splits, on=\"dataset_id\", how=\"left\")\n",
    "if df_meta.duplicated(\"sample_path\").sum():\n",
    "    print(\"[WARN] duplicate sample_path rows; keeping first.\")\n",
    "    df_meta = df_meta.drop_duplicates(\"sample_path\", keep=\"first\").reset_index(drop=True)\n",
    "df_meta = canonicalize_labels(df_meta)\n",
    "\n",
    "# =====================================================\n",
    "# Build/load m/z-binned spectra cache\n",
    "# =====================================================\n",
    "CACHE_AXIS = os.path.join(CACHE_DIR, f\"mz_axis_da_{str(BIN_DA).replace('.','p')}.npy\")\n",
    "CACHE_X    = os.path.join(CACHE_DIR, f\"X_mz_da_{str(BIN_DA).replace('.','p')}.npy\")\n",
    "CACHE_IDX  = os.path.join(CACHE_DIR, f\"index_used_da_{str(BIN_DA).replace('.','p')}.csv\")\n",
    "need_rebuild = not (os.path.exists(CACHE_AXIS) and os.path.exists(CACHE_X) and os.path.exists(CACHE_IDX))\n",
    "\n",
    "if need_rebuild:\n",
    "    all_min, all_max = np.inf, -np.inf\n",
    "    ok_paths, used_rows = [], []\n",
    "    readable = 0\n",
    "    resolved_ct = exists_ct = open_ok_ct = 0\n",
    "\n",
    "    for sp in tqdm(df_meta[\"sample_path\"].tolist(), desc=\"Scan mz ranges\"):\n",
    "        npz_path = resolve_npz_path(sp)\n",
    "        if npz_path is None: continue\n",
    "        resolved_ct += 1\n",
    "        if os.path.exists(npz_path): exists_ct += 1\n",
    "        try:\n",
    "            mz, _ = load_npz_spectrum(npz_path)\n",
    "            open_ok_ct += 1\n",
    "            if mz.size == 0: continue\n",
    "            all_min = min(all_min, float(np.min(mz)))\n",
    "            all_max = max(all_max, float(np.max(mz)))\n",
    "            ok_paths.append(npz_path)\n",
    "            readable += 1\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    print(f\"[INFO] mz-scan: resolved={resolved_ct}, exists={exists_ct}, opened_ok={open_ok_ct}, readable={readable}\")\n",
    "    if readable == 0:\n",
    "        raise RuntimeError(\"No readable .npz for spectra binning.\")\n",
    "\n",
    "    centers = build_global_bins(all_min, all_max, BIN_DA, BIN_PPM)\n",
    "\n",
    "    X = np.zeros((len(ok_paths), centers.size), dtype=np.float64)\n",
    "    for i, npz_path in enumerate(tqdm(ok_paths, desc=\"Bin spectra\")):\n",
    "        try:\n",
    "            mz, spec = load_npz_spectrum(npz_path)\n",
    "            if TIC_NORM: spec = spec / (spec.sum() + 1e-12)\n",
    "            if APPLY_LOG1P: spec = np.log1p(spec)\n",
    "            X[i] = bin_spectrum(mz, spec, centers, BIN_DA)\n",
    "            used_rows.append(npz_path)\n",
    "        except Exception:\n",
    "            X[i] = np.nan\n",
    "\n",
    "    mask = ~np.isnan(X).any(axis=1)\n",
    "    X = X[mask]\n",
    "    used_paths = [os.path.normpath(p) for p, m in zip(ok_paths, mask) if m]\n",
    "\n",
    "    np.save(CACHE_AXIS, centers); np.save(CACHE_X, X)\n",
    "    pd.DataFrame({\"sample_path\": used_paths}).to_csv(CACHE_IDX, index=False)\n",
    "    print(f\"[OK] Cached X_mz: {X.shape}, axis: {centers.shape}\")\n",
    "else:\n",
    "    centers = np.load(CACHE_AXIS)\n",
    "    X = np.load(CACHE_X)\n",
    "    used_paths = [os.path.normpath(p) for p in pd.read_csv(CACHE_IDX)[\"sample_path\"].tolist()]\n",
    "    print(f\"[OK] Loaded cached X_mz: {X.shape}, axis: {centers.shape}\")\n",
    "\n",
    "# =====================================================\n",
    "# Load embeddings + index\n",
    "# =====================================================\n",
    "if not (os.path.exists(FEATS_NPY) and os.path.exists(INDEX_CSV)):\n",
    "    raise FileNotFoundError(f\"Expected {FEATS_NPY} and {INDEX_CSV}\")\n",
    "\n",
    "Z = np.load(FEATS_NPY)               # (N_emb, D)\n",
    "idx_emb = pd.read_csv(INDEX_CSV)     # must include 'sample_path'\n",
    "if \"sample_path\" not in idx_emb.columns:\n",
    "    raise KeyError(f\"'sample_path' missing in {INDEX_CSV}\")\n",
    "\n",
    "# =====================================================\n",
    "# Strict 3-way alignment: X_mz ↔ Z ↔ metadata\n",
    "# =====================================================\n",
    "# 1) Resolve paths everywhere\n",
    "df_meta[\"resolved_path\"] = df_meta[\"sample_path\"].apply(resolve_npz_path)\n",
    "idx_emb[\"resolved_path\"] = idx_emb[\"sample_path\"].apply(resolve_npz_path)\n",
    "\n",
    "# 2) Build row maps for each source\n",
    "x_row   = {p: i for i, p in enumerate(used_paths)}                         # path -> row in X\n",
    "emb_row = {p: i for i, p in enumerate(idx_emb[\"resolved_path\"].tolist())}  # path -> row in Z\n",
    "\n",
    "# 3) Compute common set across all three (canonical order = X order)\n",
    "meta_paths = set(df_meta[\"resolved_path\"].dropna().tolist())\n",
    "emb_paths  = set(k for k in emb_row.keys() if k is not None)\n",
    "x_paths    = set(x_row.keys())\n",
    "common = [p for p in used_paths if p in emb_paths and p in meta_paths]\n",
    "if len(common) == 0:\n",
    "    raise RuntimeError(\"No overlap between X_mz (npz), embeddings, and metadata.\")\n",
    "\n",
    "print(f\"[ALIGN] |X paths|={len(x_paths)} |emb paths|={len(emb_paths)} |meta paths|={len(meta_paths)} |common|={len(common)}\")\n",
    "\n",
    "# 4) Subset & reorder all to common canonical order\n",
    "X = X[[x_row[p] for p in common]]\n",
    "Z = Z[[emb_row[p] for p in common]]\n",
    "\n",
    "df_used = df_meta[df_meta[\"resolved_path\"].isin(common)].copy()\n",
    "order_map = {p:i for i,p in enumerate(common)}\n",
    "df_used[\"__ord\"] = df_used[\"resolved_path\"].map(order_map)\n",
    "df_used = df_used.sort_values(\"__ord\").drop(columns=\"__ord\").reset_index(drop=True)\n",
    "df_used[\"x_row\"] = np.arange(len(df_used), dtype=int)  # stable row index for slicing\n",
    "\n",
    "print(f\"[ALIGN] Final shapes — X_mz: {X.shape}, Z: {Z.shape}, meta rows: {len(df_used)}\")\n",
    "\n",
    "# =====================================================\n",
    "# Learn surrogate & attribute class→m/z\n",
    "# =====================================================\n",
    "PLOTS_DIR = os.path.join(OUT_ROOT, \"plots\"); os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "CSV_DIR   = os.path.join(OUT_ROOT, \"csv\");   os.makedirs(CSV_DIR, exist_ok=True)\n",
    "\n",
    "# Learn Z ≈ X_mz @ B (use all rows that survived intersection)\n",
    "B = ridge_embeddings_from_mz(X, Z, alpha=RIDGE_ALPHA)  # (M_mz, D_emb)\n",
    "np.save(os.path.join(CSV_DIR, \"B_mz_to_embedding.npy\"), B)\n",
    "\n",
    "for task in TASKS:\n",
    "    if task not in df_used.columns:\n",
    "        print(f\"[SKIP] {task}: missing in metadata.\")\n",
    "        continue\n",
    "\n",
    "    df_task = filter_valid(df_used, task, min_count=MIN_CLASS_COUNT)\n",
    "    if df_task.empty:\n",
    "        print(f\"[SKIP] {task}: empty after filtering.\")\n",
    "        continue\n",
    "\n",
    "    # Use TRAIN+VAL for classifier on embeddings\n",
    "    m_tr = (df_task[\"split\"] == \"train\").values\n",
    "    m_va = (df_task[\"split\"] == \"val\").values\n",
    "    fit_mask = m_tr | m_va\n",
    "    if fit_mask.sum() < 2 or len(np.unique(df_task.loc[fit_mask, task])) < 2:\n",
    "        print(f\"[SKIP] {task}: insufficient TRAIN+VAL.\")\n",
    "        continue\n",
    "\n",
    "    rows = df_task[\"x_row\"].values\n",
    "    Z_task = Z[rows]\n",
    "    y_task = df_task[task].astype(str).values\n",
    "    classes = sorted(np.unique(y_task).tolist())\n",
    "\n",
    "    # Fit class ← Z (L1 OvR) on TRAIN+VAL subset\n",
    "    Z_fit = Z_task[fit_mask]; y_fit = y_task[fit_mask]\n",
    "    w_by_class = fit_l1_ovr_on_embeddings(Z_fit, y_fit, classes)  # dict[class] -> (D,)\n",
    "\n",
    "    # Attribute to m/z: score_mz[class] = B @ w_class\n",
    "    scores_mz = {cls: B @ w_by_class[cls] for cls in classes}  # dict[class] -> (M,)\n",
    "\n",
    "    # Select columns by top |score| across classes\n",
    "    sel_idx = select_union_top(scores_mz, TOP_MZ_PER_CLASS, MAX_MZ_IN_PANEL)\n",
    "    sel_labels = format_mz(centers[sel_idx])\n",
    "\n",
    "    # Panel (signed) heatmap dataframe\n",
    "    df_panel = pd.DataFrame(\n",
    "        np.vstack([scores_mz[c][sel_idx] for c in classes]),\n",
    "        index=classes, columns=sel_labels\n",
    "    )\n",
    "\n",
    "    # Save CSVs\n",
    "    df_panel.to_csv(os.path.join(CSV_DIR, f\"{safe_name(task)}__class_to_mz_panel.csv\"))\n",
    "    full_labels = format_mz(centers)\n",
    "    df_full = pd.DataFrame({c: scores_mz[c] for c in classes}, index=full_labels)\n",
    "    df_full.to_csv(os.path.join(CSV_DIR, f\"{safe_name(task)}__class_to_mz_full.csv\"))\n",
    "\n",
    "    # Selected m/z map (exact centers)\n",
    "    pd.DataFrame({\"col\": list(df_panel.columns), \"mz\": centers[sel_idx]}).to_csv(\n",
    "        os.path.join(CSV_DIR, f\"{safe_name(task)}__selected_mz_columns.csv\"), index=False\n",
    "    )\n",
    "\n",
    "    # Plot heatmap (horizontal class labels)\n",
    "    plot_heatmap(\n",
    "        df_panel,\n",
    "        title=f\"{task} — m/z attribution via {MODEL_TAG} (B @ w_class)\",\n",
    "        out_png=os.path.join(PLOTS_DIR, f\"{safe_name(task)}__mz_attr_heatmap.png\"),\n",
    "    )\n",
    "\n",
    "print(f\"\\n[DONE] Outputs in: {OUT_ROOT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc975c98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
